This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: */**, README.md
- Files matching these patterns are excluded: web/d3.v7.min.js, /*.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
config/
  default.yml
docs/
  API.md
  ARCHITECTURE.md
  codebased_improvements_implemented.md
  FAQ.md
  INSTALL.md
  TROUBLESHOOTING.md
examples/
  queries.md
  query_templates.json
src/
  codebased/
    api/
      __init__.py
      endpoints.py
      main.py
      models.py
    database/
      __init__.py
      schema.py
      service.py
    parsers/
      __init__.py
      angular.py
      base.py
      css.py
      extractor.py
      file_types.py
      html.py
      incremental.py
      javascript.py
      nodejs.py
      python.py
      registry.py
      treesitter_setup.py
      typescript.py
    __init__.py
    cli.py
    config.py
tests/
  __init__.py
  test_api.py
  test_database.py
  test_extractor_registry.py
  test_file_types.py
  test_parser.py
web/
  app.js
  graph.js
  index.html
  performance.js
  style.css
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="config/default.yml">
project_root: ".."
log_level: "INFO"
parsing:
  file_extensions:
    - ".py"
    - ".js"
    - ".ts"
    - ".jsx"
    - ".tsx"
  exclude_patterns:
    - "__pycache__"
    - "*.pyc"
    - ".git"
    - "node_modules"
    - ".env"
    - "venv"
    - "env"
    - "build"
    - "dist"
    - ".codebased"
  include_docstrings: true
  max_file_size: 1048576
  follow_symlinks: false
database:
  path: ".codebased/data/graph.kuzu"
  query_timeout: 30
  batch_size: 1000
  auto_backup: true
  backup_retention_days: 7
api:
  host: "127.0.0.1"
  port: 8000
  debug: false
  reload: false
  cors_origins:
    - "*"
  max_query_time: 30
  enable_docs: true
web:
  static_path: ".codebased/web"
  template_path: ".codebased/web/templates"
  max_nodes: 1000
  max_edges: 5000
  default_layout: "force"
</file>

<file path="docs/API.md">
# CodeBased API Documentation

This document describes the REST API endpoints provided by CodeBased.

## Base URL

Default: `http://localhost:8000/api`

## Authentication

CodeBased does not require authentication for local development. All endpoints are publicly accessible.

## Endpoints

### Graph Data

#### GET /api/graph

Get graph visualization data with optional filters.

**Parameters:**
- `node_types` (string, optional): Comma-separated node types to include
- `max_nodes` (integer, optional): Maximum nodes to return (default: 5000)
- `max_edges` (integer, optional): Maximum edges to return (default: 10000)
- `file_filter` (string, optional): File path filter pattern

**Response:**
```json
{
  "nodes": [
    {
      "id": "file_main.py_func_hello",
      "name": "hello",
      "type": "Function",
      "file_path": "main.py",
      "line_start": 5,
      "line_end": 8,
      "metadata": {}
    }
  ],
  "edges": [
    {
      "source": "file_main.py_func_main",
      "target": "file_main.py_func_hello",
      "relationship_type": "CALLS",
      "metadata": {}
    }
  ],
  "metadata": {
    "total_nodes": 150,
    "total_edges": 200,
    "node_types": ["File", "Function", "Class"],
    "filters": {}
  }
}
```

### Query Execution

#### POST /api/query

Execute Cypher queries against the graph database.

**Request Body:**
```json
{
  "query": "MATCH (n:Function) RETURN n.name LIMIT 10",
  "parameters": {}
}
```

**Response:**
```json
{
  "data": [
    {"n.name": "main"},
    {"n.name": "hello"},
    {"n.name": "process"}
  ],
  "execution_time": 0.05,
  "record_count": 3
}
```

**Security Notes:**
- Only read operations (MATCH, RETURN) are allowed
- Write operations (CREATE, DELETE, SET) are blocked
- Queries have a 30-second timeout limit

### Graph Updates

#### POST /api/update

Trigger graph update (incremental or full).

**Request Body (optional):**
```json
{
  "force_full": false,
  "directory_path": null
}
```

**Response:**
```json
{
  "success": true,
  "message": "Update completed successfully",
  "statistics": {
    "files_processed": 25,
    "nodes_created": 150,
    "edges_created": 200,
    "files_changed": 3,
    "execution_time": 2.5
  },
  "errors": []
}
```

### Query Templates

#### GET /api/templates

Get pre-built query templates for common analysis tasks.

**Response:**
```json
{
  "templates": [
    {
      "id": "find_callers",
      "name": "Find Function Callers",
      "description": "Find all functions that call a specific function",
      "query": "MATCH (caller:Function)-[:CALLS]->(target:Function {name: $function_name}) RETURN caller.name, caller.file_path",
      "parameters": ["function_name"],
      "example_params": {"function_name": "example_function"}
    }
  ]
}
```

### System Status

#### GET /api/status

Get system status and statistics.

**Response:**
```json
{
  "database_stats": {
    "total_nodes": 1500,
    "total_edges": 2000,
    "node_types": {"Function": 800, "Class": 200, "File": 50},
    "database_size_mb": 5.2
  },
  "database_health": {
    "status": "healthy",
    "last_update": "2024-01-15T10:30:00Z"
  },
  "update_status": {
    "is_updating": false,
    "last_update_time": "2024-01-15T10:25:00Z",
    "next_scheduled_update": null
  },
  "configuration": {
    "project_root": "/path/to/project",
    "database_path": ".codebased/data/graph.kuzu",
    "max_nodes": 5000,
    "max_edges": 10000
  }
}
```

### Project Tree

#### GET /api/tree

Get project directory structure as a tree.

**Parameters:**
- `path` (string, optional): Root path relative to project root (default: ".")

**Response:**
```json
{
  "tree": {
    "name": "project",
    "path": ".",
    "type": "directory",
    "children": [
      {
        "name": "main.py",
        "path": "main.py",
        "type": "file",
        "size": 1024,
        "modified_time": 1705317000,
        "children": []
      }
    ]
  },
  "root_path": "."
}
```

## Error Responses

All endpoints return standard HTTP status codes:

- `200` - Success
- `400` - Bad Request (invalid parameters)
- `403` - Forbidden (unauthorized operation)
- `404` - Not Found
- `500` - Internal Server Error

Error response format:
```json
{
  "detail": "Error message description"
}
```

## Rate Limiting

CodeBased does not implement rate limiting by default. For production deployments, consider using a reverse proxy with rate limiting.

## OpenAPI Documentation

Interactive API documentation is available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`
- OpenAPI JSON: `http://localhost:8000/openapi.json`

## Examples

### Python Client Example

```python
import requests
import json

base_url = "http://localhost:8000/api"

# Get graph data
response = requests.get(f"{base_url}/graph", params={
    "node_types": "Function,Class",
    "max_nodes": 1000
})
graph_data = response.json()

# Execute query
query_data = {
    "query": "MATCH (f:Function) WHERE f.complexity > $min_complexity RETURN f.name, f.complexity",
    "parameters": {"min_complexity": 10}
}
response = requests.post(f"{base_url}/query", json=query_data)
results = response.json()

# Trigger update
response = requests.post(f"{base_url}/update", json={"force_full": False})
update_result = response.json()
```

### JavaScript Client Example

```javascript
const API_BASE = 'http://localhost:8000/api';

// Get graph data
async function getGraphData(filters = {}) {
  const params = new URLSearchParams(filters);
  const response = await fetch(`${API_BASE}/graph?${params}`);
  return await response.json();
}

// Execute query
async function executeQuery(query, parameters = {}) {
  const response = await fetch(`${API_BASE}/query`, {
    method: 'POST',
    headers: {'Content-Type': 'application/json'},
    body: JSON.stringify({query, parameters})
  });
  return await response.json();
}

// Usage
const graphData = await getGraphData({node_types: 'Function,Class'});
const queryResults = await executeQuery(
  'MATCH (f:Function) RETURN f.name LIMIT 10'
);
```
</file>

<file path="docs/ARCHITECTURE.md">
# CodeBased Architecture

This document describes the technical architecture of CodeBased, explaining how the components work together to provide code graph visualization and analysis.

## Overview

CodeBased follows a multi-layered architecture with clear separation of concerns:

```
┌─────────────────────────────────────────────┐
│                Web Frontend                 │
│           (D3.js Visualization)            │
└─────────────────────────────────────────────┘
                       │ HTTP/REST
┌─────────────────────────────────────────────┐
│               FastAPI Server                │
│              (REST Endpoints)               │
└─────────────────────────────────────────────┘
                       │ Python API
┌─────────────────────────────────────────────┐
│              Database Service               │
│               (Kuzu Interface)              │
└─────────────────────────────────────────────┘
                       │ Cypher
┌─────────────────────────────────────────────┐
│                Kuzu Database                │
│             (Graph Storage)                 │
└─────────────────────────────────────────────┘
            ┌──────────────────────┐
            │   Incremental Parser │
            │    (Python AST)      │
            └──────────────────────┘
```

## Core Components

### 1. Database Layer (Kuzu)

**Purpose**: Store and query the code graph
**Technology**: Kuzu embedded graph database
**Location**: `src/codebased/database/`

```python
# Database Service Interface
class DatabaseService:
    def __init__(self, db_path: str)
    def execute_query(self, query: str, params: dict) -> List[Dict]
    def create_schema(self) -> None
    def health_check(self) -> Dict
    def get_stats(self) -> Dict
```

**Schema Design**:
```cypher
# Node Types
CREATE NODE TABLE File(id STRING, path STRING, hash STRING, size INT64, PRIMARY KEY(id));
CREATE NODE TABLE Module(id STRING, name STRING, docstring STRING, PRIMARY KEY(id));
CREATE NODE TABLE Class(id STRING, name STRING, docstring STRING, line_start INT64, line_end INT64, PRIMARY KEY(id));
CREATE NODE TABLE Function(id STRING, name STRING, docstring STRING, line_start INT64, line_end INT64, complexity INT64, PRIMARY KEY(id));
CREATE NODE TABLE Import(id STRING, module STRING, alias STRING, is_from_import BOOLEAN, PRIMARY KEY(id));

# Relationship Types
CREATE REL TABLE CONTAINS(FROM File TO Module, FROM File TO Class, FROM File TO Function, FROM File TO Import);
CREATE REL TABLE DEFINES(FROM Module TO Class, FROM Class TO Function);
CREATE REL TABLE CALLS(FROM Function TO Function);
CREATE REL TABLE IMPORTS(FROM Import TO File);
CREATE REL TABLE INHERITS(FROM Class TO Class);
```

**Optimizations**:
- Indexed node IDs for fast lookups
- Columnar storage for analytical queries
- Transaction support for atomic updates
- Query plan optimization

### 2. Parser Layer

**Purpose**: Extract code entities and relationships from source files
**Technology**: Python AST module
**Location**: `src/codebased/parsers/`

#### Two-Pass Parsing System

**Pass 1: Symbol Extraction**
```python
class SymbolExtractor(ast.NodeVisitor):
    def visit_ClassDef(self, node) -> None
    def visit_FunctionDef(self, node) -> None
    def visit_Import(self, node) -> None
    def visit_ImportFrom(self, node) -> None
```

**Pass 2: Relationship Resolution**
```python
class RelationshipExtractor(ast.NodeVisitor):
    def visit_Call(self, node) -> None  # Function calls
    def visit_Attribute(self, node) -> None  # Method calls
    def visit_Name(self, node) -> None  # Variable references
```

#### Incremental Updates
```python
class IncrementalUpdater:
    def __init__(self, config, db_service)
    def update_graph(self, path: Optional[str] = None) -> Dict
    def _get_file_hash(self, file_path: Path) -> str
    def _is_file_changed(self, file_path: Path, stored_hash: str) -> bool
    def _cleanup_removed_files(self, current_files: Set[str]) -> None
```

**File Change Detection**:
1. Calculate SHA-256 hash of file content
2. Compare with stored hash in database
3. Parse only changed files
4. Update relationships for affected files
5. Clean up deleted file references

### 3. API Layer (FastAPI)

**Purpose**: Provide REST endpoints for frontend and external tools
**Technology**: FastAPI with Pydantic models
**Location**: `src/codebased/api/`

```python
# Core endpoints
@router.get("/graph", response_model=GraphResponse)
@router.post("/query", response_model=QueryResponse)
@router.post("/update", response_model=UpdateResponse)
@router.get("/templates", response_model=TemplatesResponse)
@router.get("/status", response_model=StatusResponse)
```

**Request/Response Models**:
```python
class GraphNode(BaseModel):
    id: str
    name: str
    type: str
    file_path: Optional[str]
    line_start: Optional[int]
    line_end: Optional[int]
    metadata: Dict[str, Any]

class GraphEdge(BaseModel):
    source: str
    target: str
    relationship_type: str
    metadata: Dict[str, Any]

class QueryRequest(BaseModel):
    query: str
    parameters: Dict[str, Any] = {}
```

**Security Measures**:
- Query validation (read-only operations)
- Parameter sanitization
- Timeout protection (30s default)
- CORS configuration
- Error message sanitization

### 4. Frontend (D3.js Visualization)

**Purpose**: Interactive graph visualization and user interface
**Technology**: Vanilla JavaScript + D3.js v7
**Location**: `web/`

#### Main Components

**Application Controller** (`app.js`):
```javascript
class CodeBasedApp {
    constructor()
    init()
    loadGraphData()
    updateGraph()
    applyFilters()
    handleNodeSelected(node)
}
```

**Graph Visualization** (`graph.js`):
```javascript
class GraphVisualizer {
    constructor(container)
    updateData(graphData)
    setupForceSimulation()
    renderNodes()
    renderEdges()
    centerGraph()
    resetZoom()
}
```

**Performance Manager** (`performance.js`):
```javascript
class PerformanceManager {
    constructor(graphVisualizer)
    optimizeForNodeCount(nodeCount)
    enableWebGL()
    implementViewportCulling()
    setupLevelOfDetail()
}
```

#### Rendering Pipeline

1. **Data Loading**: Fetch graph data from API
2. **Force Simulation**: D3.js force-directed layout
3. **Viewport Culling**: Only render visible nodes/edges
4. **Level-of-Detail**: Simplify distant elements
5. **WebGL Fallback**: Hardware acceleration for large graphs

#### Performance Optimizations

- **Lazy Loading**: Load graph incrementally
- **Debounced Updates**: Batch filter operations
- **Memory Management**: Clean up unused DOM elements
- **Efficient Redraws**: Minimize DOM manipulations

### 5. CLI Interface

**Purpose**: Command-line interface for automation and scripting
**Technology**: Click framework
**Location**: `src/codebased/cli/`

```python
@click.group()
def cli():
    """CodeBased command-line interface."""

@cli.command()
def init():
    """Initialize CodeBased in current directory."""

@cli.command()
@click.option('--full', is_flag=True)
def update(full):
    """Update the code graph."""

@cli.command()
@click.option('--host', default='localhost')
@click.option('--port', default=8000)
def serve(host, port):
    """Start the web server."""
```

### 6. Configuration Management

**Purpose**: Centralized configuration with validation
**Technology**: Pydantic dataclasses + YAML
**Location**: `src/codebased/config.py`

```python
@dataclass
class DatabaseConfig:
    path: str = ".codebased/data/graph.kuzu"
    backup_enabled: bool = True

@dataclass
class ParsingConfig:
    include_patterns: List[str] = field(default_factory=lambda: ["**/*.py"])
    exclude_patterns: List[str] = field(default_factory=lambda: ["venv/**"])
    max_file_size_mb: int = 10

@dataclass
class CodeBasedConfig:
    project_root: str
    database: DatabaseConfig = field(default_factory=DatabaseConfig)
    parsing: ParsingConfig = field(default_factory=ParsingConfig)
    api: APIConfig = field(default_factory=APIConfig)
    web: WebConfig = field(default_factory=WebConfig)
```

## Data Flow

### 1. Initial Graph Generation

```mermaid
graph TD
    A[Source Files] --> B[File Discovery]
    B --> C[Pass 1: Symbol Extraction]
    C --> D[Store Entities in DB]
    D --> E[Pass 2: Relationship Extraction]
    E --> F[Store Relationships in DB]
    F --> G[Graph Ready]
```

### 2. Incremental Updates

```mermaid
graph TD
    A[File Change Detection] --> B[Hash Comparison]
    B --> C{File Changed?}
    C -->|Yes| D[Re-parse File]
    C -->|No| E[Skip File]
    D --> F[Update Database]
    F --> G[Update Relationships]
    E --> G
    G --> H[Update Complete]
```

### 3. Query Execution

```mermaid
graph TD
    A[HTTP Request] --> B[Query Validation]
    B --> C[Parameter Sanitization]
    C --> D[Execute Cypher Query]
    D --> E[Format Results]
    E --> F[HTTP Response]
```

### 4. Visualization Rendering

```mermaid
graph TD
    A[API Request] --> B[Filter Application]
    B --> C[Data Transformation]
    C --> D[Force Simulation]
    D --> E[Performance Optimization]
    E --> F[DOM Rendering]
    F --> G[User Interaction]
```

## Performance Characteristics

### Time Complexity

| Operation | Complexity | Notes |
|-----------|------------|-------|
| File Parsing | O(n) | n = lines of code |
| Graph Query | O(log n) | With proper indexing |
| Incremental Update | O(m) | m = changed files |
| Visualization | O(n²) | Force simulation |

### Space Complexity

| Component | Memory Usage | Notes |
|-----------|--------------|-------|
| Database | O(n + e) | n = nodes, e = edges |
| Parser | O(d) | d = max depth |
| Frontend | O(v) | v = visible elements |
| Total | 100MB per 10K LOC | Approximate |

### Scaling Considerations

**Vertical Scaling**:
- More RAM for larger codebases
- Faster CPU for parsing speed
- SSD for database performance

**Optimization Strategies**:
- Chunked processing for large files
- Background parsing with progress updates
- Database partitioning by project modules
- CDN for static assets in production

## Error Handling

### Parser Errors
- Graceful AST parsing failure handling
- File encoding detection and conversion
- Syntax error reporting with line numbers
- Partial graph updates on failures

### Database Errors
- Connection retry with backoff
- Transaction rollback on failures
- Database corruption detection
- Automatic backup and recovery

### API Errors
- Structured error responses
- Request validation with clear messages
- Query timeout handling
- Rate limiting (optional)

### Frontend Errors
- Network failure recovery
- Progressive enhancement
- Graceful degradation for older browsers
- User-friendly error messages

## Security Considerations

### Query Security
- Read-only query enforcement
- Parameter injection prevention
- Query complexity limits
- Timeout protection

### File System Access
- Path traversal prevention
- Permission checking
- Symlink following controls
- Excluded pattern enforcement

### Network Security
- CORS configuration
- No sensitive data exposure
- Local-only binding by default
- HTTPS support for production

## Extension Points

### Adding New Languages
```python
class JavaScriptParser(BaseParser):
    def parse_file(self, file_path: Path) -> ParseResult
    def extract_entities(self, ast_node) -> List[Entity]
    def extract_relationships(self, ast_node) -> List[Relationship]
```

### Custom Visualizations
```javascript
class CustomVisualizer extends GraphVisualizer {
    renderNodes() { /* Custom node rendering */ }
    renderEdges() { /* Custom edge rendering */ }
    setupInteractions() { /* Custom interactions */ }
}
```

### Additional Metrics
```python
class ComplexityAnalyzer:
    def calculate_cyclomatic_complexity(self, func_node) -> int
    def detect_code_smells(self, class_node) -> List[str]
    def analyze_coupling(self, module_node) -> float
```

## Testing Architecture

### Unit Tests
- Parser component testing
- Database service testing
- API endpoint testing
- Individual function testing

### Integration Tests
- End-to-end parsing pipeline
- API + Database integration
- CLI command testing
- Configuration loading

### Performance Tests
- Large codebase parsing
- Query performance benchmarks
- Memory usage profiling
- Concurrent access testing

This architecture provides a solid foundation for code analysis while maintaining flexibility for future enhancements and scaling.
</file>

<file path="docs/codebased_improvements_implemented.md">
# CodeBased Improvements Implemented

**Date**: July 22, 2025  
**Summary**: Fixed critical issues with the CodeBased tool's graph database implementation

## Issues Identified and Fixed

### 1. Schema Design Issue (Fixed) ✅
**Problem**: The `file_id` field in entity nodes was storing file paths instead of File entity IDs, breaking referential integrity.

**Solution**: Modified `EntityExtractor._create_entity_insert_query()` to properly extract file_id from entity metadata instead of using the file_path.

**File Changed**: `/src/codebased/parsers/extractor.py` (lines 419-422)

### 2. Import Parsing Issue (Fixed) ✅
**Problem**: Import statements were incorrectly parsed, showing "from ast import ast" instead of just "import ast".

**Solution**: Fixed the Import entity name to use the alias if available, otherwise the module name.

**File Changed**: `/src/codebased/parsers/python.py` (line 320)

### 3. Database Clearing Issue (Fixed) ✅
**Problem**: Database clearing failed because nodes had connected edges that needed to be deleted first.

**Solution**: Changed `DELETE` to `DETACH DELETE` to remove nodes and their relationships together.

**File Changed**: `/src/codebased/database/service.py` (line 199)

### 4. Inheritance Relationship Issue (Fixed) ✅
**Problem**: INHERITS relationships included an `inheritance_type` property that wasn't in the schema.

**Solution**: Removed the metadata property from INHERITS relationship creation.

**File Changed**: `/src/codebased/parsers/python.py` (lines 216-220)

## Verification Results

After implementing the fixes:

1. **Entities Extracted**: 1,003 entities from 26 Python files
2. **Relationships Created**: 2,652 relationships
3. **Relationship Types Working**:
   - FILE_CONTAINS_MODULE: 25
   - FILE_CONTAINS_CLASS: 11
   - FILE_CONTAINS_FUNCTION: 15
   - MODULE_CONTAINS_CLASS: 25
   - CLASS_CONTAINS_FUNCTION: 127
   - CALLS: 177
   - INHERITS: Working (inheritance_type removed)

## Key Improvements Achieved

1. **Proper Graph Structure**: Relationships now use entity IDs instead of paths, enabling proper graph traversal
2. **Complete Relationship Coverage**: All containment relationships (FILE->CLASS, CLASS->FUNCTION) are properly created
3. **Accurate Import Tracking**: Import statements are correctly parsed and stored
4. **Robust Database Operations**: Database clearing now works reliably with DETACH DELETE

## Testing

Created comprehensive test scripts:
- `test_graph_relationships.py`: Verifies relationships are created correctly
- `test_reparse.py`: Tests single file parsing with fixes
- `reparse_codebase.py`: Full codebase reparse utility

## Comparison with FalkorDB

Our implementation now matches FalkorDB's approach:
- ✅ Uses proper entity IDs for relationships
- ✅ Creates explicit containment relationships
- ✅ Supports Python AST parsing
- ✅ Extracts CALLS relationships
- ❌ Still needs multi-language support (JS/TS for ParticleFun)

## Next Steps

1. **Add Multi-Language Support**: Integrate tree-sitter for JavaScript/TypeScript parsing
2. **Enhance Visualization**: Fix the edge position update bug in the web UI
3. **Add More Relationship Types**: USES (variable usage), DECORATES (already extracted)
4. **Performance Optimization**: Add batch processing for large codebases

The CodeBased tool is now functionally complete for Python codebases with proper graph relationships!
</file>

<file path="docs/FAQ.md">
# Frequently Asked Questions (FAQ)

## General Questions

### What is CodeBased?

CodeBased is a lightweight, self-contained code graph generator and visualization tool that helps developers and AI agents understand code relationships through knowledge graphs. It uses Python AST parsing to extract code entities and stores them in an embedded Kuzu graph database.

### How is CodeBased different from other code analysis tools?

- **Zero Dependencies**: No external servers or databases required
- **Self-Contained**: Everything runs locally with embedded database
- **AI Agent Ready**: Pre-built queries designed for AI-assisted development
- **Incremental Updates**: Fast updates using file hashing
- **Interactive Visualization**: Real-time graph exploration with filtering
- **Language Agnostic Design**: Easy to extend for other programming languages

### What programming languages does CodeBased support?

Currently, CodeBased supports Python out of the box. The architecture is designed for easy extension to other languages. JavaScript/TypeScript, Go, and Java parsers are planned for future releases.

## Installation & Setup

### What are the system requirements?

**Minimum**:
- Python 3.8+
- 512MB RAM
- 100MB disk space
- Any modern web browser

**Recommended**:
- Python 3.9+
- 2GB+ RAM
- 1GB+ disk space
- SSD storage for better performance

### Can I use CodeBased with virtual environments?

Yes! CodeBased works perfectly with virtual environments and is actually recommended. The setup scripts automatically create and configure a virtual environment for you.

### Does CodeBased work on Windows?

Yes, CodeBased supports Windows, macOS, and Linux. We provide both Unix shell scripts and PowerShell scripts for setup.

### Can I use CodeBased in a Docker container?

Yes, CodeBased can run in Docker. Here's a basic Dockerfile:

```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
RUN pip install -e .
EXPOSE 8000
CMD ["codebased", "serve", "--host", "0.0.0.0"]
```

## Usage Questions

### How do I analyze my first project?

1. Navigate to your project directory
2. Run `./codebased/setup.sh` (or `setup.ps1` on Windows)
3. Run `codebased init` to initialize
4. Run `codebased update` to analyze your code
5. Run `codebased serve` and open http://localhost:8000

### How long does it take to analyze a project?

Analysis time depends on project size:
- 1,000 LOC: ~2 seconds
- 10,000 LOC: ~15 seconds
- 100,000 LOC: ~2 minutes

Subsequent incremental updates are much faster (typically under 5 seconds).

### Can I analyze multiple projects?

Yes, but each project needs its own CodeBased instance. You can have multiple projects with CodeBased installed, each maintaining its own graph database.

### How do I update the analysis when my code changes?

Run `codebased update` to perform an incremental update. CodeBased automatically detects changed files and only re-parses what's necessary. For a complete refresh, use `codebased update --full`.

## Configuration Questions

### How do I exclude certain directories from analysis?

Edit `.codebased.yml` and add patterns to `exclude_patterns`:

```yaml
parsing:
  exclude_patterns:
    - "venv/**"
    - "tests/**"
    - "*.pyc"
    - "__pycache__/**"
    - ".git/**"
```

### Can I change the web interface port?

Yes, use `codebased serve --port 8080` or set it in configuration:

```yaml
api:
  port: 8080
```

### How do I configure memory limits for large projects?

Adjust these settings in `.codebased.yml`:

```yaml
web:
  max_nodes: 10000
  max_edges: 15000
  performance_mode: "fast"
```

### Can I run CodeBased without the web interface?

Yes, CodeBased provides CLI commands for all operations:

```bash
# Query from command line
codebased query "MATCH (f:Function) RETURN f.name LIMIT 10"

# Export graph data
codebased export --format json --output graph.json
```

## Performance Questions

### My project is very large. Will CodeBased handle it?

CodeBased can handle large projects, but you may need to:

1. Increase memory limits in configuration
2. Use filtering to focus on relevant code sections
3. Enable performance mode: `performance_mode: "fast"`
4. Consider excluding test files and third-party libraries

### The web interface is slow. How can I improve it?

Try these optimizations:

1. Reduce `max_nodes` and `max_edges` in configuration
2. Use node type filters to show only relevant elements
3. Enable WebGL rendering in your browser
4. Use a modern browser with hardware acceleration

### Can I run CodeBased on a remote server?

Yes, configure the API to bind to all interfaces:

```bash
codebased serve --host 0.0.0.0 --port 8000
```

Make sure to secure access appropriately for production use.

## Troubleshooting

### "Python not found" error

Make sure Python 3.8+ is installed and in your PATH:

```bash
# Check Python version
python3 --version

# Use python3 explicitly if python isn't available
python3 -m codebased.cli --help
```

### "Permission denied" on setup scripts

Make the script executable:

```bash
# Unix/Linux/macOS
chmod +x setup.sh

# Windows: Run PowerShell as Administrator
```

### Database initialization fails

Try these steps:

1. Remove any existing database files: `rm -rf .codebased/data/`
2. Reinitialize: `codebased init --force`
3. Check directory permissions
4. Ensure adequate disk space

### Web interface won't load

Check these common issues:

1. Is the server running? `codebased serve`
2. Correct URL? Default is http://localhost:8000
3. Port already in use? Try a different port
4. Browser blocking localhost? Check security settings

### Out of memory errors

Reduce memory usage:

1. Lower `max_nodes` and `max_edges` settings
2. Use filtering to analyze smaller code sections
3. Close other memory-intensive applications
4. Add more system RAM if possible

## Query Questions

### How do I write Cypher queries?

Cypher is Neo4j's query language. Here are some examples:

```cypher
# Find all functions
MATCH (f:Function) RETURN f.name

# Find function callers
MATCH (caller)-[:CALLS]->(target:Function {name: "my_function"})
RETURN caller.name

# Find class hierarchy
MATCH path = (child:Class)-[:INHERITS*]->(parent:Class)
RETURN path
```

### What query templates are available?

CodeBased includes several built-in templates:
- Find Function Callers
- Class Inheritance Hierarchy
- File Dependencies
- Circular Dependencies
- Unused Functions
- Complex Functions
- Impact Analysis

Access them via the web interface or `GET /api/templates`.

### Can I save my custom queries?

Currently, custom query saving is not built-in, but you can:

1. Save queries in text files
2. Create your own template JSON file
3. Use the CLI: `codebased query "YOUR_QUERY_HERE"`

### How do I find performance bottlenecks in my code?

Use queries like:

```cypher
# Find complex functions
MATCH (f:Function) 
WHERE f.complexity > 10 
RETURN f.name, f.file_path, f.complexity 
ORDER BY f.complexity DESC

# Find heavily called functions
MATCH (f:Function)<-[:CALLS]-(caller)
RETURN f.name, count(caller) as call_count
ORDER BY call_count DESC
```

## Integration Questions

### Can I integrate CodeBased with my IDE?

While there are no official IDE plugins yet, you can:

1. Use the web interface alongside your IDE
2. Run CLI commands from your IDE's terminal
3. Create custom IDE scripts using the API
4. Use the API to build your own integrations

### Can I export data to other tools?

Yes, CodeBased supports several export formats:

```bash
# Export as JSON
codebased export --format json --output graph.json

# Export as CSV (nodes and edges separately)
codebased export --format csv --output nodes.csv

# Use the API for custom integrations
curl http://localhost:8000/api/graph > graph.json
```

### How do I use CodeBased with CI/CD?

Example GitHub Actions integration:

```yaml
- name: Analyze Code Graph
  run: |
    ./.codebased/setup.sh
    codebased update
    codebased query "MATCH (f:Function) WHERE f.complexity > 20 RETURN count(f)" > complexity_report.txt
```

### Can I use CodeBased programmatically?

Yes, via the REST API:

```python
import requests

# Get graph data
response = requests.get('http://localhost:8000/api/graph')
graph_data = response.json()

# Execute queries
query_response = requests.post('http://localhost:8000/api/query', 
    json={'query': 'MATCH (f:Function) RETURN f.name LIMIT 10'})
results = query_response.json()
```

## Contributing

### How can I contribute to CodeBased?

1. Report bugs and feature requests on GitHub
2. Submit pull requests for improvements
3. Add support for new programming languages
4. Improve documentation
5. Create examples and tutorials

### How do I add support for a new programming language?

1. Create a new parser class inheriting from `BaseParser`
2. Implement AST parsing for the target language
3. Map language constructs to CodeBased entities
4. Add tests for the new parser
5. Submit a pull request

### Can I create custom visualizations?

Yes! The frontend is modular. You can:

1. Extend the existing `GraphVisualizer` class
2. Create new visualization types (3D, timeline, etc.)
3. Add custom styling and interactions
4. Use the API data with your own visualization library

## Support

### Where can I get help?

1. Check this FAQ
2. Read the documentation in the `docs/` directory
3. Enable debug logging: `export CODEBASED_DEBUG=1`
4. Report issues on GitHub with:
   - Your operating system and Python version
   - Complete error messages
   - Steps to reproduce the issue

### Is commercial support available?

CodeBased is open-source. Community support is available through GitHub issues and discussions.

### How do I report security issues?

For security-related issues, please report them responsibly by:

1. Not posting public issues
2. Contacting the maintainers directly
3. Providing detailed information about the vulnerability
4. Allowing time for fixes before disclosure

## Roadmap

### What features are planned?

Near-term:
- JavaScript/TypeScript support
- Better performance for large codebases
- More visualization options
- IDE integrations

Long-term:
- Multi-language support (Go, Java, Rust)
- 3D visualizations
- Advanced code metrics
- Collaborative features

### Can I request features?

Yes! Please open a GitHub issue with:
- Clear description of the feature
- Use cases and benefits
- Any technical considerations
- Willingness to help implement
</file>

<file path="docs/INSTALL.md">
# Installation Guide

This guide covers different ways to install and set up CodeBased.

## Prerequisites

- Python 3.8 or higher
- At least 512MB RAM (2GB+ recommended for large codebases)
- 100MB+ disk space
- Modern web browser (Chrome, Firefox, Safari, Edge)

## Common Installation Error Fix

**If you see "externally-managed-environment" error:**

This means your system Python is protected and you MUST use a virtual environment. Do NOT use `--break-system-packages`. Instead:

```bash
# Create virtual environment FIRST
python3 -m venv venv

# Activate it
source venv/bin/activate  # Unix/Linux/macOS
# OR
venv\Scripts\activate     # Windows

# NOW install packages
pip install -r requirements.txt
```

## Quick Install (Recommended)

### Unix/Linux/macOS

```bash
# Clone or download CodeBased to your project
cd your-project-directory

# Run the automated setup script
chmod +x .codebased/setup.sh
./.codebased/setup.sh
```

### Windows

```powershell
# Open PowerShell as Administrator
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Navigate to your project
cd your-project-directory

# Run the setup script
.\.codebased\setup.ps1
```

The setup script will:
1. Check Python version (requires 3.8+)
2. Create and activate a virtual environment
3. Install all dependencies
4. Initialize the database
5. Run basic tests

## Manual Installation

If the automated script doesn't work for your environment:

### Step 1: Python Environment

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
# Unix/macOS:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# Upgrade pip
pip install --upgrade pip
```

### Step 2: Install Dependencies

```bash
# Navigate to CodeBased directory
cd .codebased

# Install required packages
pip install -r requirements.txt

# Install CodeBased in development mode
pip install -e .
```

### Step 3: Initialize Database

```bash
# Initialize CodeBased in your project
codebased init

# Perform initial code parsing
codebased update
```

### Step 4: Verify Installation

```bash
# Run tests
python3 -m pytest tests/ -v

# Start the server
codebased serve

# Open http://localhost:8000 in your browser
```

## Dependencies

### Core Dependencies
- `kuzu>=0.0.9` - Graph database
- `fastapi>=0.104.0` - Web API framework
- `uvicorn>=0.24.0` - ASGI server
- `click>=8.1.0` - CLI framework
- `pyyaml>=6.0` - Configuration parsing
- `pydantic>=2.0.0` - Data validation

### Development Dependencies
- `pytest>=7.0.0` - Testing framework
- `black>=23.0.0` - Code formatting
- `mypy>=1.0.0` - Type checking
- `isort>=5.12.0` - Import sorting

### Optional Dependencies
- `watchdog>=3.0.0` - File system monitoring
- `rich>=13.0.0` - Enhanced console output

## Configuration

After installation, configure CodeBased by editing `.codebased.yml`:

```yaml
# Project settings
project_root: "."
cache_dir: ".codebased/cache"

# Database configuration
database:
  path: ".codebased/data/graph.kuzu"
  backup_enabled: true
  backup_interval_hours: 24

# Parsing configuration
parsing:
  include_patterns:
    - "**/*.py"
  exclude_patterns:
    - "venv/**"
    - "*.pyc"
    - "__pycache__/**"
    - ".git/**"
    - "node_modules/**"
  max_file_size_mb: 10
  timeout_seconds: 30

# API configuration
api:
  host: "localhost"
  port: 8000
  cors_origins:
    - "http://localhost:3000"
    - "http://localhost:8080"

# Web interface
web:
  max_nodes: 5000
  max_edges: 10000
  enable_webgl: true
  performance_mode: "auto"  # auto, fast, quality

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: ".codebased/logs/codebased.log"
```

## Troubleshooting Installation

### Python Version Issues

**Error**: "Python 3.8+ required"
```bash
# Check Python version
python3 --version

# Install Python 3.8+ if needed
# Ubuntu/Debian:
sudo apt update
sudo apt install python3.8 python3.8-pip python3.8-venv

# macOS (with Homebrew):
brew install python@3.8

# Windows: Download from python.org
```

### Permission Errors

**Error**: "Permission denied"
```bash
# Unix/Linux - Make scripts executable
chmod +x .codebased/setup.sh

# Windows - Run PowerShell as Administrator
# Or change execution policy:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### Virtual Environment Issues

**Error**: "venv command not found"
```bash
# Install venv module
# Ubuntu/Debian:
sudo apt install python3-venv

# Or use alternative:
pip install virtualenv
virtualenv venv
```

### Database Initialization Errors

**Error**: "Failed to initialize database"
```bash
# Check directory permissions
ls -la .codebased/

# Create data directory if missing
mkdir -p .codebased/data

# Remove corrupted database files
rm -rf .codebased/data/graph.kuzu*
codebased init --force
```

### Port Already in Use

**Error**: "Port 8000 already in use"
```bash
# Use different port
codebased serve --port 8001

# Or kill existing process
# Unix/Linux:
lsof -ti:8000 | xargs kill -9
# Windows:
netstat -ano | findstr :8000
taskkill /PID <PID> /F
```

### Memory Issues

**Error**: "Out of memory"
- Reduce `max_nodes` and `max_edges` in configuration
- Use filtering to focus on specific code sections
- Increase system swap space
- Close other memory-intensive applications

### Network Issues

**Error**: "Connection refused"
- Check firewall settings
- Verify port is not blocked
- Try `--host 0.0.0.0` for external access
- Check antivirus software

## Docker Installation (Alternative)

If you prefer containerized deployment:

```dockerfile
# Dockerfile
FROM python:3.8-slim

WORKDIR /app
COPY .codebased/requirements.txt .
RUN pip install -r requirements.txt

COPY . .
RUN pip install -e .codebased/

EXPOSE 8000
CMD ["codebased", "serve", "--host", "0.0.0.0"]
```

```bash
# Build and run
docker build -t codebased .
docker run -p 8000:8000 -v $(pwd):/app codebased
```

## Upgrading

To upgrade CodeBased:

```bash
# Activate virtual environment
source venv/bin/activate

# Update dependencies
pip install -r .codebased/requirements.txt --upgrade

# Update database schema if needed
codebased init --force

# Refresh graph data
codebased update --full
```

## Uninstalling

To remove CodeBased:

```bash
# Remove virtual environment
rm -rf venv

# Remove CodeBased files (optional - keeps your code)
rm -rf .codebased

# Remove configuration (optional)
rm .codebased.yml
```

## System Requirements

### Minimum Requirements
- **CPU**: 1 core, 1GHz
- **RAM**: 512MB available
- **Disk**: 100MB free space
- **Network**: None (offline capable)

### Recommended Requirements
- **CPU**: 2+ cores, 2GHz+
- **RAM**: 2GB+ available
- **Disk**: 1GB+ free space (for large codebases)
- **SSD**: Recommended for better performance

### Performance Guidelines

| Codebase Size | Recommended RAM | Parse Time | Database Size |
|---------------|----------------|------------|---------------|
| < 1,000 LOC   | 512MB         | < 10s      | < 1MB        |
| < 10,000 LOC  | 1GB           | < 1min     | < 10MB       |
| < 100,000 LOC | 2GB           | < 5min     | < 100MB      |
| 100,000+ LOC  | 4GB+          | 5min+      | 100MB+       |

## Getting Help

If you encounter issues:

1. Check the [Troubleshooting Guide](TROUBLESHOOTING.md)
2. Review the [FAQ](FAQ.md)
3. Enable debug logging: `export CODEBASED_DEBUG=1`
4. Check the logs: `.codebased/logs/codebased.log`
5. Report issues on GitHub with:
   - Operating system and Python version
   - Complete error message
   - Steps to reproduce
   - Log files (if relevant)
</file>

<file path="docs/TROUBLESHOOTING.md">
# Troubleshooting Guide

This guide helps you diagnose and fix common issues with CodeBased.

## Quick Diagnostic Steps

Before diving into specific issues, try these basic diagnostic steps:

1. **Check Python version**: `python3 --version` (requires 3.8+)
2. **Verify installation**: `codebased --help`
3. **Check disk space**: Ensure at least 100MB free
4. **Enable debug logging**: `export CODEBASED_DEBUG=1`
5. **Check log files**: `.codebased/logs/codebased.log`

## Installation Issues

### Python Version Problems

**Error**: `Python 3.8+ required, found 3.7.x`

**Solutions**:
```bash
# Ubuntu/Debian - Install newer Python
sudo apt update
sudo apt install python3.8 python3.8-pip python3.8-venv
sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1

# macOS with Homebrew
brew install python@3.8
echo 'export PATH="/usr/local/opt/python@3.8/bin:$PATH"' >> ~/.bashrc

# Windows - Download from python.org and reinstall
# Make sure to check "Add Python to PATH" during installation
```

### Virtual Environment Issues

**Error**: `venv: command not found`

**Solutions**:
```bash
# Install venv module
sudo apt install python3-venv  # Ubuntu/Debian
brew install python3           # macOS

# Alternative: use virtualenv
pip install virtualenv
virtualenv venv
```

**Error**: `Cannot activate virtual environment`

**Solutions**:
```bash
# Unix/Linux/macOS
source venv/bin/activate

# Windows Command Prompt
venv\Scripts\activate.bat

# Windows PowerShell
venv\Scripts\Activate.ps1

# If PowerShell blocks scripts:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### Permission Errors

**Error**: `Permission denied: './setup.sh'`

**Solutions**:
```bash
# Make script executable
chmod +x setup.sh
chmod +x .codebased/setup.sh

# Or run with bash explicitly
bash setup.sh
```

**Error**: `Permission denied: writing to directory`

**Solutions**:
```bash
# Check directory permissions
ls -la .codebased/

# Fix permissions
chmod 755 .codebased/
chmod -R 755 .codebased/data/

# Create directories if missing
mkdir -p .codebased/data
mkdir -p .codebased/logs
```

### Dependency Installation Problems

**Error**: `Failed building wheel for package X`

**Solutions**:
```bash
# Update pip and setuptools
pip install --upgrade pip setuptools wheel

# Install build dependencies
sudo apt install build-essential python3-dev  # Ubuntu/Debian
xcode-select --install                         # macOS

# Use pre-compiled wheels
pip install --only-binary=all package_name
```

**Error**: `No module named 'kuzu'`

**Solutions**:
```bash
# Verify virtual environment is activated
which python  # Should point to venv/bin/python

# Reinstall kuzu
pip uninstall kuzu
pip install kuzu>=0.0.9

# If still failing, install from source
pip install kuzu --no-binary kuzu
```

## Database Issues

### Database Initialization Failures

**Error**: `Failed to initialize database`

**Solutions**:
```bash
# Remove corrupted database files
rm -rf .codebased/data/graph.kuzu*

# Check directory permissions
ls -la .codebased/data/

# Reinitialize with force flag
codebased init --force

# Check available disk space
df -h .
```

**Error**: `Database is locked`

**Solutions**:
```bash
# Stop all CodeBased processes
pkill -f "codebased serve"
pkill -f "uvicorn"

# Remove lock files
rm -f .codebased/data/graph.kuzu.lock
rm -f .codebased/data/*.lock

# Restart CodeBased
codebased serve
```

### Database Corruption

**Error**: `Database appears to be corrupted`

**Solutions**:
```bash
# Backup existing data (if recoverable)
cp -r .codebased/data/graph.kuzu .codebased/backup/

# Remove corrupted database
rm -rf .codebased/data/graph.kuzu*

# Reinitialize and re-parse
codebased init --force
codebased update --full

# Check filesystem integrity
fsck /dev/your-disk  # Unix/Linux
```

## Parsing Issues

### File Parsing Errors

**Error**: `SyntaxError: invalid syntax in file.py`

**Solutions**:
```bash
# Check Python version compatibility
python3 -m py_compile problematic_file.py

# Exclude problematic files
# Add to .codebased.yml:
parsing:
  exclude_patterns:
    - "problematic_file.py"
    - "legacy_code/**"

# Check file encoding
file problematic_file.py
```

**Error**: `UnicodeDecodeError: invalid start byte`

**Solutions**:
```bash
# Check file encoding
file -bi problematic_file.py

# Convert to UTF-8 if needed
iconv -f ENCODING -t UTF-8 file.py > file_utf8.py

# Exclude binary files
parsing:
  exclude_patterns:
    - "*.so"
    - "*.pyc"
    - "*.pyo"
```

### Memory Issues During Parsing

**Error**: `MemoryError during parsing`

**Solutions**:
```yaml
# Reduce memory usage in .codebased.yml
parsing:
  max_file_size_mb: 5  # Reduce from default 10
  exclude_patterns:
    - "large_generated_files/**"
    - "data/**/*.py"

web:
  max_nodes: 2000      # Reduce from default 5000
  max_edges: 3000      # Reduce from default 10000
```

```bash
# Process files in smaller batches
codebased update --path specific_directory/

# Increase system swap space (Linux)
sudo swapon --show
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

## Server Issues

### Port Already in Use

**Error**: `Port 8000 already in use`

**Solutions**:
```bash
# Find what's using the port
lsof -ti:8000           # Unix/Linux/macOS
netstat -ano | find "8000"  # Windows

# Kill the process
kill -9 $(lsof -ti:8000)    # Unix/Linux/macOS
taskkill /PID <PID> /F      # Windows

# Use a different port
codebased serve --port 8001

# Or set in configuration
api:
  port: 8001
```

### Cannot Connect to Server

**Error**: `Connection refused to localhost:8000`

**Solutions**:
```bash
# Check if server is running
ps aux | grep codebased
curl http://localhost:8000/api/status

# Start server if not running
codebased serve

# Check firewall settings
sudo ufw status                    # Ubuntu
sudo firewall-cmd --list-ports     # CentOS/RHEL

# Bind to all interfaces for remote access
codebased serve --host 0.0.0.0
```

### Server Crashes

**Error**: `Server exits unexpectedly`

**Solutions**:
```bash
# Enable debug logging
export CODEBASED_DEBUG=1
codebased serve

# Check logs
tail -f .codebased/logs/codebased.log

# Run with verbose output
uvicorn codebased.api.app:app --host localhost --port 8000 --log-level debug

# Check system resources
top
free -h
df -h
```

## Web Interface Issues

### Blank Page or Loading Forever

**Solutions**:
```bash
# Check browser console (F12)
# Look for JavaScript errors

# Clear browser cache
# Ctrl+Shift+Delete (Chrome/Firefox)

# Try different browser
# Chrome, Firefox, Safari, Edge

# Check API connectivity
curl http://localhost:8000/api/status

# Disable browser extensions
# Try incognito/private mode
```

### Performance Issues in Browser

**Solutions**:
```javascript
// Reduce graph size in configuration
web:
  max_nodes: 1000
  max_edges: 1500
  performance_mode: "fast"

// Enable WebGL in browser
// Chrome: chrome://flags/#enable-webgl
// Firefox: about:config -> webgl.force-enabled = true

// Use filtering to show fewer nodes
// Filter by node type or file path
```

### Graph Not Displaying

**Solutions**:
```bash
# Check if graph data exists
curl http://localhost:8000/api/graph | jq '.nodes | length'

# Regenerate graph data
codebased update --full

# Check browser compatibility
# Requires modern browser with ES6 support

# Disable ad blockers
# Some may block D3.js or API calls
```

## Query Issues

### Invalid Cypher Queries

**Error**: `Parser Error: Invalid Cypher syntax`

**Solutions**:
```cypher
-- Check basic syntax
MATCH (n) RETURN n LIMIT 10;

-- Use proper quotes
MATCH (f:Function {name: "my_function"}) RETURN f;

-- Check node/relationship types
MATCH (n) RETURN DISTINCT labels(n);
MATCH ()-[r]->() RETURN DISTINCT type(r);
```

### Query Timeouts

**Error**: `Query execution timeout`

**Solutions**:
```yaml
# Increase timeout in configuration
api:
  query_timeout_seconds: 60  # Default is 30

# Optimize queries
# Add LIMIT clauses
# Use indexed properties
# Avoid expensive operations

# Example optimizations:
MATCH (f:Function) RETURN f.name LIMIT 100;  // Instead of returning all
MATCH (f:Function {name: "specific"}) RETURN f;  // Use specific matches
```

### Empty Query Results

**Solutions**:
```cypher
-- Check if data exists
MATCH (n) RETURN count(n);

-- Check node types
MATCH (n) RETURN DISTINCT labels(n);

-- Check relationships
MATCH ()-[r]->() RETURN DISTINCT type(r) LIMIT 10;

-- Verify node properties
MATCH (f:Function) RETURN f.name, f.file_path LIMIT 10;
```

## Performance Issues

### Slow Initial Parsing

**Solutions**:
```yaml
# Exclude unnecessary files
parsing:
  exclude_patterns:
    - "venv/**"
    - "*.pyc"
    - "__pycache__/**"
    - ".git/**"
    - "tests/**"
    - "docs/**"
    - "build/**"
    - "dist/**"

# Limit file size
parsing:
  max_file_size_mb: 5
```

```bash
# Use SSD storage for better I/O
# Ensure adequate RAM (2GB+ for large projects)
# Close other memory-intensive applications

# Parse specific directories first
codebased update --path src/
codebased update --path lib/
```

### Slow Web Interface

**Solutions**:
```yaml
# Reduce visualization complexity
web:
  max_nodes: 1000
  max_edges: 1500
  performance_mode: "fast"
```

```javascript
// Enable hardware acceleration in browser settings
// Chrome: Settings > Advanced > System > Use hardware acceleration
// Firefox: Settings > General > Performance > Use hardware acceleration
```

### High Memory Usage

**Solutions**:
```bash
# Monitor memory usage
htop
ps aux --sort=-%mem | head

# Reduce graph size
web:
  max_nodes: 2000
  max_edges: 3000

# Use filtering to focus analysis
# Filter by specific directories or file types
```

## Network Issues

### Cannot Access from Remote Machine

**Solutions**:
```bash
# Bind to all interfaces
codebased serve --host 0.0.0.0 --port 8000

# Check firewall settings
sudo ufw allow 8000/tcp           # Ubuntu
sudo firewall-cmd --permanent --add-port=8000/tcp  # CentOS/RHEL

# Configure CORS if needed
api:
  cors_origins:
    - "http://your-remote-ip:3000"
    - "http://another-domain.com"
```

### SSL/HTTPS Issues

**Solutions**:
```bash
# For development, disable HTTPS requirements
# Use HTTP instead of HTTPS

# For production, use reverse proxy
# nginx, Apache, or Traefik with SSL termination

# Example nginx configuration:
server {
    listen 443 ssl;
    server_name your-domain.com;
    
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## Environment-Specific Issues

### Windows-Specific Issues

**PowerShell Execution Policy**:
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

**Path Separator Issues**:
- Use forward slashes in configuration files
- CodeBased handles path conversion automatically

**Antivirus Interference**:
- Add .codebased directory to antivirus exclusions
- Kuzu database files may trigger false positives

### macOS-Specific Issues

**Xcode Command Line Tools**:
```bash
xcode-select --install
```

**Python from Mac App Store vs Homebrew**:
```bash
# Use Homebrew Python for better compatibility
brew install python@3.9
echo 'export PATH="/usr/local/opt/python@3.9/bin:$PATH"' >> ~/.zshrc
```

### Linux Distribution Issues

**Package Manager Dependencies**:
```bash
# Ubuntu/Debian
sudo apt install python3-dev build-essential

# CentOS/RHEL
sudo yum groupinstall "Development Tools"
sudo yum install python3-devel

# Arch Linux
sudo pacman -S python3 gcc make
```

## Getting More Help

### Enable Debug Logging

```bash
export CODEBASED_DEBUG=1
export CODEBASED_LOG_LEVEL=DEBUG
codebased serve
```

### Collect Diagnostic Information

```bash
# System information
uname -a
python3 --version
pip list | grep -E "(kuzu|fastapi|uvicorn|click)"

# CodeBased status
codebased status

# Log files
tail -100 .codebased/logs/codebased.log

# Configuration
cat .codebased.yml
```

### Report Issues

When reporting issues, include:

1. **Environment Details**:
   - Operating system and version
   - Python version
   - CodeBased version

2. **Error Information**:
   - Complete error message
   - Steps to reproduce
   - Expected vs actual behavior

3. **Relevant Files**:
   - Configuration file (.codebased.yml)
   - Log files (last 50-100 lines)
   - Sample code that causes issues (if applicable)

4. **Diagnostic Output**:
   - `codebased status` output
   - `codebased --version` output
   - Browser console errors (if web interface issue)

### Emergency Recovery

If CodeBased is completely broken:

```bash
# Nuclear option - fresh start
rm -rf venv/
rm -rf .codebased/data/
rm -rf .codebased/logs/
rm .codebased.yml

# Reinstall from scratch
./setup.sh
codebased init
codebased update
```

Remember: Always backup your code and any custom configurations before attempting emergency recovery procedures.
</file>

<file path="examples/queries.md">
# CodeBased Example Queries

This document contains a comprehensive library of Cypher queries for analyzing codebases with CodeBased. These queries are particularly useful for AI agents and developers who need to understand code relationships and dependencies.

## Table of Contents

1. [Basic Queries](#basic-queries)
2. [Function Analysis](#function-analysis)
3. [Class Analysis](#class-analysis)
4. [Dependency Analysis](#dependency-analysis)
5. [Impact Analysis](#impact-analysis)
6. [Code Quality Queries](#code-quality-queries)
7. [Architecture Analysis](#architecture-analysis)
8. [Refactoring Queries](#refactoring-queries)

## Basic Queries

### Find All Files

```cypher
MATCH (f:File)
RETURN f.name, f.path, f.lines_of_code
ORDER BY f.lines_of_code DESC
```

**Use Case**: Get an overview of all files in the codebase, sorted by size.

### Count Entities by Type

```cypher
MATCH (n)
RETURN labels(n)[0] AS entity_type, COUNT(n) AS count
ORDER BY count DESC
```

**Use Case**: Understand the composition of your codebase.

### Files with Most Entities

```cypher
MATCH (f:File)-[:CONTAINS]->(entity)
RETURN f.name, f.path, COUNT(entity) AS entity_count
ORDER BY entity_count DESC
LIMIT 10
```

**Use Case**: Find the most complex files that might need refactoring.

## Function Analysis

### Find All Callers of a Function

```cypher
MATCH (caller:Function)-[:CALLS]->(target:Function {name: $function_name})
RETURN DISTINCT caller.name AS caller_name, 
       caller.file_path AS caller_file,
       caller.line_start AS caller_line
ORDER BY caller_file, caller_line
```

**Parameters**: `function_name` (string)
**Use Case**: Find all places where a specific function is called.

### Find Functions Never Called

```cypher
MATCH (f:Function)
WHERE NOT ()-[:CALLS]->(f) 
  AND f.name <> '__init__'
  AND f.name <> '__main__'
  AND NOT f.name STARTS WITH 'test_'
RETURN f.name, f.file_path, f.line_start
ORDER BY f.file_path, f.line_start
```

**Use Case**: Identify potentially unused functions (dead code).

### Find Most Called Functions

```cypher
MATCH (caller:Function)-[:CALLS]->(target:Function)
RETURN target.name AS function_name,
       target.file_path AS file_path,
       COUNT(caller) AS call_count
ORDER BY call_count DESC
LIMIT 20
```

**Use Case**: Identify the most critical functions in your codebase.

### Find Functions with High Complexity

```cypher
MATCH (f:Function)
WHERE f.complexity > $min_complexity
RETURN f.name, f.file_path, f.complexity, f.line_start
ORDER BY f.complexity DESC
```

**Parameters**: `min_complexity` (integer, default: 10)
**Use Case**: Find functions that might need refactoring due to high complexity.

### Find Recursive Functions

```cypher
MATCH (f:Function)-[:CALLS]->(f)
RETURN f.name, f.file_path, f.line_start
```

**Use Case**: Identify recursive function calls.

### Find Long Functions

```cypher
MATCH (f:Function)
WHERE (f.line_end - f.line_start) > $min_lines
RETURN f.name, f.file_path, 
       (f.line_end - f.line_start + 1) AS line_count
ORDER BY line_count DESC
```

**Parameters**: `min_lines` (integer, default: 50)
**Use Case**: Find functions that might be too long and need breaking down.

## Class Analysis

### Find Class Inheritance Hierarchy

```cypher
MATCH path = (child:Class)-[:INHERITS*]->(parent:Class {name: $class_name})
RETURN path
```

**Parameters**: `class_name` (string)
**Use Case**: Understand inheritance relationships for a specific class.

### Find All Subclasses

```cypher
MATCH (parent:Class {name: $class_name})<-[:INHERITS*]-(child:Class)
RETURN DISTINCT child.name AS subclass_name, child.file_path
ORDER BY subclass_name
```

**Parameters**: `class_name` (string)
**Use Case**: Find all classes that inherit from a given class.

### Find Classes with No Subclasses

```cypher
MATCH (c:Class)
WHERE NOT (c)<-[:INHERITS]-()
RETURN c.name, c.file_path
ORDER BY c.name
```

**Use Case**: Identify leaf classes in the inheritance hierarchy.

### Find Classes with Many Methods

```cypher
MATCH (c:Class)-[:CONTAINS]->(m:Function)
RETURN c.name AS class_name, c.file_path, COUNT(m) AS method_count
ORDER BY method_count DESC
LIMIT 10
```

**Use Case**: Find classes that might be doing too much (violating Single Responsibility Principle).

### Find Abstract Classes

```cypher
MATCH (c:Class)
WHERE c.is_abstract = true
RETURN c.name, c.file_path, c.docstring
ORDER BY c.name
```

**Use Case**: Find all abstract classes and interfaces.

## Dependency Analysis

### Find File Dependencies

```cypher
MATCH (source:File {name: $file_name})-[:CONTAINS]->(:Import)-[:IMPORTS]->(target:File)
RETURN DISTINCT target.name AS dependency, target.path
ORDER BY dependency
```

**Parameters**: `file_name` (string)
**Use Case**: Find all files that a specific file depends on.

### Find Files That Depend On a Specific File

```cypher
MATCH (dependent:File)-[:CONTAINS]->(:Import)-[:IMPORTS]->(target:File {name: $file_name})
RETURN DISTINCT dependent.name AS dependent_file, dependent.path
ORDER BY dependent_file
```

**Parameters**: `file_name` (string)
**Use Case**: Find all files that depend on a specific file.

### Detect Circular Dependencies

```cypher
MATCH path = (f1:File)-[:IMPORTS*2..10]->(f1)
WHERE length(path) > 2
RETURN path
LIMIT 10
```

**Use Case**: Find circular import dependencies that could cause issues.

### Find Most Imported Modules

```cypher
MATCH (file:File)-[:CONTAINS]->(import:Import)-[:IMPORTS]->(target)
RETURN target.module_name AS module, COUNT(DISTINCT file) AS import_count
ORDER BY import_count DESC
LIMIT 20
```

**Use Case**: Identify the most commonly used external libraries.

### Find External Dependencies

```cypher
MATCH (import:Import)
WHERE NOT EXISTS((import)-[:IMPORTS]->(:File))
RETURN DISTINCT import.module_name AS external_module, 
       COUNT(*) AS usage_count
ORDER BY usage_count DESC
```

**Use Case**: List all external dependencies and their usage frequency.

## Impact Analysis

### Find All Code Affected by Changing a Function

```cypher
MATCH path = (target:Function {name: $function_name})<-[:CALLS*1..3]-(affected)
RETURN DISTINCT affected.name AS affected_entity,
       labels(affected)[0] AS entity_type,
       affected.file_path AS file_path,
       length(path) AS distance
ORDER BY distance, file_path, affected_entity
```

**Parameters**: `function_name` (string)
**Use Case**: Understand the impact of changing or removing a function.

### Find All Code That Uses a Class

```cypher
MATCH (target:Class {name: $class_name})<-[:USES]-(user)
RETURN DISTINCT user.name AS user_name,
       labels(user)[0] AS user_type,
       user.file_path AS file_path
ORDER BY file_path, user_name
```

**Parameters**: `class_name` (string)
**Use Case**: Find all code that would be affected by changing a class.

### Find Dependency Chain

```cypher
MATCH path = (start:Function {name: $start_function})-[:CALLS*1..5]->(end:Function {name: $end_function})
RETURN path
ORDER BY length(path)
LIMIT 5
```

**Parameters**: `start_function` (string), `end_function` (string)
**Use Case**: Find how two functions are connected through call chains.

### Find Code Reachable from Entry Point

```cypher
MATCH path = (entry:Function {name: $entry_point})-[:CALLS*]->(reachable:Function)
RETURN DISTINCT reachable.name AS reachable_function,
       reachable.file_path AS file_path,
       MIN(length(path)) AS min_distance
ORDER BY min_distance, file_path, reachable_function
```

**Parameters**: `entry_point` (string, e.g., "main")
**Use Case**: Find all code reachable from a main entry point.

## Query Templates for AI Agents

### Template: Analyze Function

```cypher
MATCH (f:Function {name: $function_name})
OPTIONAL MATCH (f)<-[:CALLS]-(caller:Function)
OPTIONAL MATCH (f)-[:CALLS]->(called:Function)
OPTIONAL MATCH (f)<-[:CONTAINS]-(containing_class:Class)
RETURN f AS function_details,
       COLLECT(DISTINCT caller.name) AS callers,
       COLLECT(DISTINCT called.name) AS calls,
       containing_class.name AS class_name
```

### Template: Analyze Class

```cypher
MATCH (c:Class {name: $class_name})
OPTIONAL MATCH (c)-[:CONTAINS]->(method:Function)
OPTIONAL MATCH (c)-[:INHERITS]->(parent:Class)
OPTIONAL MATCH (child:Class)-[:INHERITS]->(c)
RETURN c AS class_details,
       COLLECT(DISTINCT method.name) AS methods,
       parent.name AS parent_class,
       COLLECT(DISTINCT child.name) AS child_classes
```

### Template: Analyze File

```cypher
MATCH (f:File {name: $file_name})
OPTIONAL MATCH (f)-[:CONTAINS]->(entity)
OPTIONAL MATCH (f)-[:CONTAINS]->(:Import)-[:IMPORTS]->(dep:File)
RETURN f AS file_details,
       COLLECT(DISTINCT labels(entity)[0]) AS entity_types,
       COLLECT(DISTINCT dep.name) AS dependencies
```

These queries provide a comprehensive toolkit for analyzing codebases with CodeBased. They can be executed through the web interface, CLI, or API endpoints.
</file>

<file path="examples/query_templates.json">
{
  "basic_queries": {
    "all_files": {
      "name": "List All Files",
      "description": "Get an overview of all files in the codebase, sorted by size",
      "query": "MATCH (f:File) RETURN f.name, f.path, f.lines_of_code ORDER BY f.lines_of_code DESC",
      "parameters": [],
      "category": "basic"
    },
    "entity_counts": {
      "name": "Count Entities by Type",
      "description": "Understand the composition of your codebase",
      "query": "MATCH (n) RETURN labels(n)[0] AS entity_type, COUNT(n) AS count ORDER BY count DESC",
      "parameters": [],
      "category": "basic"
    },
    "complex_files": {
      "name": "Files with Most Entities",
      "description": "Find the most complex files that might need refactoring",
      "query": "MATCH (f:File)-[:CONTAINS]->(entity) RETURN f.name, f.path, COUNT(entity) AS entity_count ORDER BY entity_count DESC LIMIT 10",
      "parameters": [],
      "category": "basic"
    }
  },
  "function_analysis": {
    "find_callers": {
      "name": "Find Function Callers",
      "description": "Find all places where a specific function is called",
      "query": "MATCH (caller:Function)-[:CALLS]->(target:Function {name: $function_name}) RETURN DISTINCT caller.name AS caller_name, caller.file_path AS caller_file, caller.line_start AS caller_line ORDER BY caller_file, caller_line",
      "parameters": ["function_name"],
      "example_params": {"function_name": "process_data"},
      "category": "functions"
    },
    "unused_functions": {
      "name": "Find Unused Functions",
      "description": "Identify potentially unused functions (dead code)",
      "query": "MATCH (f:Function) WHERE NOT ()-[:CALLS]->(f) AND f.name <> '__init__' AND f.name <> '__main__' AND NOT f.name STARTS WITH 'test_' RETURN f.name, f.file_path, f.line_start ORDER BY f.file_path, f.line_start",
      "parameters": [],
      "category": "functions"
    },
    "most_called": {
      "name": "Most Called Functions",
      "description": "Identify the most critical functions in your codebase",
      "query": "MATCH (caller:Function)-[:CALLS]->(target:Function) RETURN target.name AS function_name, target.file_path AS file_path, COUNT(caller) AS call_count ORDER BY call_count DESC LIMIT 20",
      "parameters": [],
      "category": "functions"
    },
    "high_complexity": {
      "name": "High Complexity Functions",
      "description": "Find functions that might need refactoring due to high complexity",
      "query": "MATCH (f:Function) WHERE f.complexity > $min_complexity RETURN f.name, f.file_path, f.complexity, f.line_start ORDER BY f.complexity DESC",
      "parameters": ["min_complexity"],
      "example_params": {"min_complexity": 10},
      "category": "functions"
    },
    "recursive_functions": {
      "name": "Recursive Functions",
      "description": "Identify recursive function calls",
      "query": "MATCH (f:Function)-[:CALLS]->(f) RETURN f.name, f.file_path, f.line_start",
      "parameters": [],
      "category": "functions"
    },
    "long_functions": {
      "name": "Long Functions",
      "description": "Find functions that might be too long and need breaking down",
      "query": "MATCH (f:Function) WHERE (f.line_end - f.line_start) > $min_lines RETURN f.name, f.file_path, (f.line_end - f.line_start + 1) AS line_count ORDER BY line_count DESC",
      "parameters": ["min_lines"],
      "example_params": {"min_lines": 50},
      "category": "functions"
    }
  },
  "class_analysis": {
    "inheritance_hierarchy": {
      "name": "Class Inheritance Hierarchy",
      "description": "Understand inheritance relationships for a specific class",
      "query": "MATCH path = (child:Class)-[:INHERITS*]->(parent:Class {name: $class_name}) RETURN path",
      "parameters": ["class_name"],
      "example_params": {"class_name": "BaseClass"},
      "category": "classes"
    },
    "find_subclasses": {
      "name": "Find All Subclasses",
      "description": "Find all classes that inherit from a given class",
      "query": "MATCH (parent:Class {name: $class_name})<-[:INHERITS*]-(child:Class) RETURN DISTINCT child.name AS subclass_name, child.file_path ORDER BY subclass_name",
      "parameters": ["class_name"],
      "example_params": {"class_name": "BaseClass"},
      "category": "classes"
    },
    "leaf_classes": {
      "name": "Leaf Classes",
      "description": "Identify leaf classes in the inheritance hierarchy",
      "query": "MATCH (c:Class) WHERE NOT (c)<-[:INHERITS]-() RETURN c.name, c.file_path ORDER BY c.name",
      "parameters": [],
      "category": "classes"
    },
    "classes_many_methods": {
      "name": "Classes with Many Methods",
      "description": "Find classes that might be doing too much (violating Single Responsibility Principle)",
      "query": "MATCH (c:Class)-[:CONTAINS]->(m:Function) RETURN c.name AS class_name, c.file_path, COUNT(m) AS method_count ORDER BY method_count DESC LIMIT 10",
      "parameters": [],
      "category": "classes"
    },
    "abstract_classes": {
      "name": "Abstract Classes",
      "description": "Find all abstract classes and interfaces",
      "query": "MATCH (c:Class) WHERE c.is_abstract = true RETURN c.name, c.file_path, c.docstring ORDER BY c.name",
      "parameters": [],
      "category": "classes"
    }
  },
  "dependency_analysis": {
    "file_dependencies": {
      "name": "File Dependencies",
      "description": "Find all files that a specific file depends on",
      "query": "MATCH (source:File {name: $file_name})-[:CONTAINS]->(:Import)-[:IMPORTS]->(target:File) RETURN DISTINCT target.name AS dependency, target.path ORDER BY dependency",
      "parameters": ["file_name"],
      "example_params": {"file_name": "main.py"},
      "category": "dependencies"
    },
    "dependent_files": {
      "name": "Files That Depend On a File",
      "description": "Find all files that depend on a specific file",
      "query": "MATCH (dependent:File)-[:CONTAINS]->(:Import)-[:IMPORTS]->(target:File {name: $file_name}) RETURN DISTINCT dependent.name AS dependent_file, dependent.path ORDER BY dependent_file",
      "parameters": ["file_name"],
      "example_params": {"file_name": "utils.py"},
      "category": "dependencies"
    },
    "circular_dependencies": {
      "name": "Circular Dependencies",
      "description": "Find circular import dependencies that could cause issues",
      "query": "MATCH path = (f1:File)-[:IMPORTS*2..10]->(f1) WHERE length(path) > 2 RETURN path LIMIT 10",
      "parameters": [],
      "category": "dependencies"
    },
    "most_imported": {
      "name": "Most Imported Modules",
      "description": "Identify the most commonly used external libraries",
      "query": "MATCH (file:File)-[:CONTAINS]->(import:Import)-[:IMPORTS]->(target) RETURN target.module_name AS module, COUNT(DISTINCT file) AS import_count ORDER BY import_count DESC LIMIT 20",
      "parameters": [],
      "category": "dependencies"
    },
    "external_dependencies": {
      "name": "External Dependencies",
      "description": "List all external dependencies and their usage frequency",
      "query": "MATCH (import:Import) WHERE NOT EXISTS((import)-[:IMPORTS]->(:File)) RETURN DISTINCT import.module_name AS external_module, COUNT(*) AS usage_count ORDER BY usage_count DESC",
      "parameters": [],
      "category": "dependencies"
    }
  },
  "impact_analysis": {
    "function_impact": {
      "name": "Function Change Impact",
      "description": "Understand the impact of changing or removing a function",
      "query": "MATCH path = (target:Function {name: $function_name})<-[:CALLS*1..3]-(affected) RETURN DISTINCT affected.name AS affected_entity, labels(affected)[0] AS entity_type, affected.file_path AS file_path, length(path) AS distance ORDER BY distance, file_path, affected_entity",
      "parameters": ["function_name"],
      "example_params": {"function_name": "critical_function"},
      "category": "impact"
    },
    "class_usage": {
      "name": "Class Usage Analysis",
      "description": "Find all code that would be affected by changing a class",
      "query": "MATCH (target:Class {name: $class_name})<-[:USES]-(user) RETURN DISTINCT user.name AS user_name, labels(user)[0] AS user_type, user.file_path AS file_path ORDER BY file_path, user_name",
      "parameters": ["class_name"],
      "example_params": {"class_name": "DatabaseConnection"},
      "category": "impact"
    },
    "dependency_chain": {
      "name": "Find Dependency Chain",
      "description": "Find how two functions are connected through call chains",
      "query": "MATCH path = (start:Function {name: $start_function})-[:CALLS*1..5]->(end:Function {name: $end_function}) RETURN path ORDER BY length(path) LIMIT 5",
      "parameters": ["start_function", "end_function"],
      "example_params": {"start_function": "main", "end_function": "save_data"},
      "category": "impact"
    },
    "reachable_from_entry": {
      "name": "Code Reachable from Entry Point",
      "description": "Find all code reachable from a main entry point",
      "query": "MATCH path = (entry:Function {name: $entry_point})-[:CALLS*]->(reachable:Function) RETURN DISTINCT reachable.name AS reachable_function, reachable.file_path AS file_path, MIN(length(path)) AS min_distance ORDER BY min_distance, file_path, reachable_function",
      "parameters": ["entry_point"],
      "example_params": {"entry_point": "main"},
      "category": "impact"
    }
  },
  "code_quality": {
    "missing_docstrings": {
      "name": "Functions Without Docstrings",
      "description": "Identify functions that need documentation",
      "query": "MATCH (f:Function) WHERE (f.docstring IS NULL OR f.docstring = '') AND NOT f.name STARTS WITH '_' RETURN f.name, f.file_path, f.line_start ORDER BY f.file_path, f.line_start",
      "parameters": [],
      "category": "quality"
    },
    "large_classes": {
      "name": "Large Classes",
      "description": "Find classes that might be too large and need refactoring",
      "query": "MATCH (c:Class) WHERE (c.line_end - c.line_start) > $min_lines RETURN c.name, c.file_path, (c.line_end - c.line_start + 1) AS line_count ORDER BY line_count DESC",
      "parameters": ["min_lines"],
      "example_params": {"min_lines": 100},
      "category": "quality"
    },
    "many_parameters": {
      "name": "Functions with Many Parameters",
      "description": "Find functions that might be hard to use due to many parameters",
      "query": "MATCH (f:Function) WHERE f.signature CONTAINS ',' WITH f, size(split(f.signature, ',')) AS param_count WHERE param_count > $max_params RETURN f.name, f.file_path, f.signature, param_count ORDER BY param_count DESC",
      "parameters": ["max_params"],
      "example_params": {"max_params": 5},
      "category": "quality"
    },
    "duplicate_names": {
      "name": "Duplicate Function Names",
      "description": "Find functions with the same name in different files (potential confusion)",
      "query": "MATCH (f1:Function), (f2:Function) WHERE f1.name = f2.name AND f1.file_path <> f2.file_path RETURN f1.name AS function_name, COLLECT(DISTINCT f1.file_path) AS files ORDER BY function_name",
      "parameters": [],
      "category": "quality"
    }
  },
  "ai_agent_templates": {
    "analyze_function": {
      "name": "Complete Function Analysis",
      "description": "Get comprehensive information about a specific function",
      "query": "MATCH (f:Function {name: $function_name}) OPTIONAL MATCH (f)<-[:CALLS]-(caller:Function) OPTIONAL MATCH (f)-[:CALLS]->(called:Function) OPTIONAL MATCH (f)<-[:CONTAINS]-(containing_class:Class) RETURN f AS function_details, COLLECT(DISTINCT caller.name) AS callers, COLLECT(DISTINCT called.name) AS calls, containing_class.name AS class_name",
      "parameters": ["function_name"],
      "example_params": {"function_name": "process_data"},
      "category": "templates"
    },
    "analyze_class": {
      "name": "Complete Class Analysis",
      "description": "Get comprehensive information about a specific class",
      "query": "MATCH (c:Class {name: $class_name}) OPTIONAL MATCH (c)-[:CONTAINS]->(method:Function) OPTIONAL MATCH (c)-[:INHERITS]->(parent:Class) OPTIONAL MATCH (child:Class)-[:INHERITS]->(c) RETURN c AS class_details, COLLECT(DISTINCT method.name) AS methods, parent.name AS parent_class, COLLECT(DISTINCT child.name) AS child_classes",
      "parameters": ["class_name"],
      "example_params": {"class_name": "DatabaseManager"},
      "category": "templates"
    },
    "analyze_file": {
      "name": "Complete File Analysis",
      "description": "Get comprehensive information about a specific file",
      "query": "MATCH (f:File {name: $file_name}) OPTIONAL MATCH (f)-[:CONTAINS]->(entity) OPTIONAL MATCH (f)-[:CONTAINS]->(:Import)-[:IMPORTS]->(dep:File) RETURN f AS file_details, COLLECT(DISTINCT labels(entity)[0]) AS entity_types, COLLECT(DISTINCT dep.name) AS dependencies",
      "parameters": ["file_name"],
      "example_params": {"file_name": "main.py"},
      "category": "templates"
    }
  }
}
</file>

<file path="src/codebased/api/__init__.py">

</file>

<file path="src/codebased/api/endpoints.py">
import logging
import time
from typing import Dict, Any, List, Optional
from pathlib import Path
from fastapi import APIRouter, HTTPException, Query, Body
from pydantic import BaseModel, Field
from ..config import CodeBasedConfig
from ..database.service import DatabaseService
from ..parsers.incremental import IncrementalUpdater
from .models import *
logger = logging.getLogger(__name__)
def create_router(config: CodeBasedConfig, db_service: DatabaseService) -> APIRouter:
    router = APIRouter()
    updater = IncrementalUpdater(config, db_service)
    @router.post("/query", response_model=QueryResponse)
    async def execute_query(request: QueryRequest) -> QueryResponse:
        start_time = time.time()
        try:
            if not request.query.strip():
                raise HTTPException(status_code=400, detail="Empty query")
            dangerous_keywords = ['DELETE', 'DROP', 'CREATE', 'SET', 'MERGE', 'REMOVE']
            query_upper = request.query.upper()
            if any(keyword in query_upper for keyword in dangerous_keywords):
                raise HTTPException(status_code=403, detail="Write operations not allowed")
            result = db_service.execute_query(request.query, request.parameters)
            if result is None:
                raise HTTPException(status_code=500, detail="Query execution failed")
            execution_time = time.time() - start_time
            return QueryResponse(
                data=result,
                execution_time=execution_time,
                record_count=len(result)
            )
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Query execution error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    @router.post("/update", response_model=UpdateResponse)
    async def update_graph(request: UpdateRequest = None) -> UpdateResponse:
        try:
            if request and request.force_full:
                logger.info("Starting full graph update")
                results = updater.force_full_update(request.directory_path)
            else:
                logger.info("Starting incremental graph update")
                results = updater.update_graph(request.directory_path if request else None)
            return UpdateResponse(
                success=not results.get('errors'),
                message="Update completed successfully" if not results.get('errors') else "Update completed with errors",
                statistics=UpdateStatistics(**results),
                errors=results.get('errors', [])
            )
        except Exception as e:
            logger.error(f"Update error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    @router.get("/graph", response_model=GraphResponse)
    async def get_graph_data(
        node_types: Optional[str] = Query(None, description="Comma-separated node types to include"),
        max_nodes: Optional[int] = Query(config.web.max_nodes, description="Maximum nodes to return"),
        max_edges: Optional[int] = Query(config.web.max_edges, description="Maximum edges to return"),
        file_filter: Optional[str] = Query(None, description="File path filter pattern")
    ) -> GraphResponse:
        try:
            where_clauses = []
            if node_types:
                type_list = [t.strip() for t in node_types.split(',')]
                type_conditions = [f"n:{t}" for t in type_list]
                where_clauses.append(f"({' OR '.join(type_conditions)})")
            if file_filter:
                where_clauses.append(f"n.file_path CONTAINS '{file_filter}'")
            where_clause = f"WHERE {' AND '.join(where_clauses)}" if where_clauses else ""
            node_tables = ['File', 'Module', 'Class', 'Function', 'Variable', 'Import']
            nodes_result = []
            for table_type in node_tables:
                if node_types and table_type not in [t.strip() for t in node_types.split(',')]:
                    continue
                # Build where clause for this table type
                table_where_clauses = []
                # Temporarily comment out .codebased filter for testing
                # if table_type == 'File':
                #     table_where_clauses.append("NOT n.path CONTAINS '.codebased'")
                # else:
                #     table_where_clauses.append("NOT n.file_id CONTAINS '.codebased'")
                if file_filter:
                    if table_type == 'File':
                        table_where_clauses.append(f"n.path CONTAINS '{file_filter}'")
                    else:
                        table_where_clauses.append(f"n.file_id CONTAINS '{file_filter}'")
                table_where = f"WHERE {' AND '.join(table_where_clauses)}" if table_where_clauses else ""
                # Build query with only properties that exist for this table type
                if table_type == 'File':
                    table_query = f"""
                    MATCH (n:{table_type})
                    {table_where}
                    RETURN n.id AS id, n.name AS name, '{table_type}' AS type, n.path AS file_path, 1 AS line_start, 1 AS line_end
                    LIMIT {max_nodes // len(node_tables) + 1}
                    """
                elif table_type in ['Variable', 'Import']:
                    # Variable and Import tables use line_number instead of line_start/line_end
                    table_query = f"""
                    MATCH (n:{table_type})
                    {table_where}
                    RETURN n.id AS id, n.name AS name, '{table_type}' AS type, n.file_id AS file_path, n.line_number AS line_start, n.line_number AS line_end
                    LIMIT {max_nodes // len(node_tables) + 1}
                    """
                else:
                    table_query = f"""
                    MATCH (n:{table_type})
                    {table_where}
                    RETURN n.id AS id, n.name AS name, '{table_type}' AS type, n.file_id AS file_path, n.line_start AS line_start, n.line_end AS line_end
                    LIMIT {max_nodes // len(node_tables) + 1}
                    """
                table_result = db_service.execute_query(table_query)
                if table_result:
                    nodes_result.extend(table_result)
                else:
                    logger.debug(f"No results from {table_type} query")
            if not nodes_result:
                raise HTTPException(status_code=500, detail="Failed to fetch nodes")
            nodes = []
            node_ids = set()
            for row in nodes_result:
                # Handle list format from Kuzu
                if isinstance(row, list) and len(row) >= 6:
                    node_id = row[0]
                    node_name = row[1]
                    node_type = row[2]
                    file_path = row[3]
                    line_start = row[4] if row[4] and row[4] != 1 else None
                    line_end = row[5] if row[5] and row[5] != 1 else None
                elif isinstance(row, dict):
                    node_id = row['id']
                    node_name = row['name']
                    node_type = row['type']
                    file_path = row.get('file_path')
                    line_start = row.get('line_start')
                    line_end = row.get('line_end')
                else:
                    continue
                node_ids.add(node_id)
                nodes.append(GraphNode(
                    id=node_id,
                    name=node_name,
                    type=node_type,
                    file_path=file_path,
                    line_start=line_start,
                    line_end=line_end,
                    metadata={}
                ))
            # Get edges between the selected nodes
            if node_ids:
                node_ids_str = "', '".join(node_ids)
                # Try different relationship table types
                edge_tables = [
                    'FILE_CONTAINS_MODULE', 'FILE_CONTAINS_CLASS', 'FILE_CONTAINS_FUNCTION',
                    'FILE_CONTAINS_VARIABLE', 'FILE_CONTAINS_IMPORT',
                    'MODULE_CONTAINS_CLASS', 'MODULE_CONTAINS_FUNCTION', 'MODULE_CONTAINS_VARIABLE',
                    'CLASS_CONTAINS_FUNCTION', 'CLASS_CONTAINS_VARIABLE',
                    'CALLS', 'INHERITS', 'IMPORTS', 'USES', 'DECORATES'
                ]
                edges_result = []
                for rel_type in edge_tables:
                    rel_query = f"""
                    MATCH (n1)-[r:{rel_type}]->(n2)
                    WHERE n1.id IN ['{node_ids_str}'] AND n2.id IN ['{node_ids_str}']
                    RETURN n1.id AS source, n2.id AS target, '{rel_type}' AS relationship_type
                    LIMIT {max_edges // len(edge_tables) + 1}
                    """
                    rel_result = db_service.execute_query(rel_query)
                    if rel_result:
                        edges_result.extend(rel_result)
                # edges_result is already populated above
                edges = []
                for row in edges_result:
                    if isinstance(row, list) and len(row) >= 3:
                        edges.append(GraphEdge(
                            source=row[0],
                            target=row[1],
                            relationship_type=row[2] or 'RELATED',
                            metadata={}
                        ))
                    elif isinstance(row, dict):
                        edges.append(GraphEdge(
                            source=row['source'],
                            target=row['target'],
                            relationship_type=row['relationship_type'],
                            metadata=dict(row.get('r', {}))
                        ))
            else:
                edges = []
            return GraphResponse(
                nodes=nodes,
                edges=edges,
                metadata=GraphMetadata(
                    total_nodes=len(nodes),
                    total_edges=len(edges),
                    node_types=list(set(node.type for node in nodes)),
                    filters={
                        'node_types': node_types,
                        'file_filter': file_filter,
                        'max_nodes': max_nodes,
                        'max_edges': max_edges
                    }
                )
            )
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Graph data error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    @router.get("/tree", response_model=TreeResponse)
    async def get_project_tree(
        path: Optional[str] = Query(".", description="Root path to generate tree for")
    ) -> TreeResponse:
        try:
            root_path = Path(config.project_root) / path
            if not root_path.exists():
                raise HTTPException(status_code=404, detail="Path not found")
            def build_tree_node(path: Path) -> TreeNode:
                node = TreeNode(
                    name=path.name if path.name else str(path),
                    path=str(path.relative_to(Path(config.project_root))),
                    type="directory" if path.is_dir() else "file",
                    children=[]
                )
                if path.is_dir():
                    try:
                        # Filter out excluded directories and files
                        exclude_patterns = config.parsing.exclude_patterns
                        children = []
                        for child in sorted(path.iterdir()):
                            # Skip excluded items
                            should_skip = False
                            for pattern in exclude_patterns:
                                if child.name == pattern or child.match(pattern):
                                    should_skip = True
                                    break
                            if not should_skip:
                                children.append(build_tree_node(child))
                        node.children = children
                    except PermissionError:
                        pass  # Skip directories we can't read
                else:
                    try:
                        stat = path.stat()
                        node.size = stat.st_size
                        node.modified_time = stat.st_mtime
                    except:
                        pass
                return node
            tree = build_tree_node(root_path)
            return TreeResponse(
                tree=tree,
                root_path=str(root_path.relative_to(Path(config.project_root)))
            )
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Tree generation error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    @router.get("/templates", response_model=TemplatesResponse)
    async def get_query_templates() -> TemplatesResponse:
        templates = [
            QueryTemplate(
                id="find_callers",
                name="Find Function Callers",
                description="Find all functions that call a specific function",
                query="MATCH (caller:Function)-[:CALLS]->(target:Function {name: $function_name}) RETURN caller.name, caller.file_path, caller.line_start",
                parameters=["function_name"],
                example_params={"function_name": "example_function"}
            ),
            QueryTemplate(
                id="class_hierarchy",
                name="Class Inheritance Hierarchy",
                description="Get the inheritance hierarchy for a class",
                query="MATCH path = (child:Class)-[:INHERITS*]->(parent:Class {name: $class_name}) RETURN path",
                parameters=["class_name"],
                example_params={"class_name": "BaseClass"}
            ),
            QueryTemplate(
                id="file_dependencies",
                name="File Dependencies",
                description="Find all files that a specific file depends on",
                query="MATCH (file:File {name: $file_name})-[:CONTAINS]->(:Import)-[:IMPORTS]->(dep:File) RETURN DISTINCT dep.path, dep.name",
                parameters=["file_name"],
                example_params={"file_name": "main.py"}
            ),
            QueryTemplate(
                id="circular_dependencies",
                name="Circular Dependencies",
                description="Detect circular import dependencies",
                query="MATCH path = (f1:File)-[:IMPORTS*2..]->(f1) WHERE length(path) > 2 RETURN path LIMIT 10",
                parameters=[],
                example_params={}
            ),
            QueryTemplate(
                id="unused_functions",
                name="Potentially Unused Functions",
                description="Find functions that are not called by other functions",
                query="MATCH (f:Function) WHERE NOT ()-[:CALLS]->(f) AND f.name <> '__init__' RETURN f.name, f.file_path, f.line_start",
                parameters=[],
                example_params={}
            ),
            QueryTemplate(
                id="complex_functions",
                name="Complex Functions",
                description="Find functions with high complexity",
                query="MATCH (f:Function) WHERE f.complexity > $min_complexity RETURN f.name, f.file_path, f.complexity ORDER BY f.complexity DESC",
                parameters=["min_complexity"],
                example_params={"min_complexity": 10}
            ),
            QueryTemplate(
                id="impact_analysis",
                name="Impact Analysis",
                description="Find all code that would be affected by changing a function",
                query="MATCH path = (f:Function {name: $function_name})<-[:CALLS*1..3]-(caller) RETURN DISTINCT caller.name, caller.file_path, length(path) AS depth ORDER BY depth",
                parameters=["function_name"],
                example_params={"function_name": "critical_function"}
            )
        ]
        return TemplatesResponse(templates=templates)
    @router.get("/status", response_model=StatusResponse)
    async def get_system_status() -> StatusResponse:
        try:
            db_stats = db_service.get_stats()
            db_health = db_service.health_check()
            update_status = updater.get_update_status()
            return StatusResponse(
                database_stats=db_stats,
                database_health=db_health,
                update_status=update_status,
                configuration={
                    "project_root": config.project_root,
                    "database_path": config.database.path,
                    "api_host": config.api.host,
                    "api_port": config.api.port,
                    "max_nodes": config.web.max_nodes,
                    "max_edges": config.web.max_edges
                }
            )
        except Exception as e:
            logger.error(f"Status error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    return router
</file>

<file path="src/codebased/api/main.py">
import logging
import time
from typing import Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from ..config import get_config
from ..database.service import get_database_service
from ..database.schema import GraphSchema
from .endpoints import create_router
logger = logging.getLogger(__name__)
def create_app(config_path: str = ".codebased.yml") -> FastAPI:
    config = get_config(config_path)
    db_service = get_database_service(config.database.path)
    if not db_service.initialize():
        raise RuntimeError("Failed to initialize database")
    schema = GraphSchema(db_service)
    if not schema.create_schema():
        raise RuntimeError("Failed to create database schema")
    app = FastAPI(
        title="CodeBased API",
        description="A lightweight code graph generator and visualization tool",
        version="0.1.0",
        docs_url="/docs" if config.api.enable_docs else None,
        redoc_url="/redoc" if config.api.enable_docs else None
    )
    app.add_middleware(
        CORSMiddleware,
        allow_origins=config.api.cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    @app.middleware("http")
    async def log_requests(request: Request, call_next):
        start_time = time.time()
        logger.info(f"{request.method} {request.url.path}")
        response = await call_next(request)
        process_time = time.time() - start_time
        logger.info(f"Response: {response.status_code} in {process_time:.3f}s")
        response.headers["X-Process-Time"] = str(process_time)
        return response
    @app.exception_handler(HTTPException)
    async def http_exception_handler(request: Request, exc: HTTPException):
        logger.error(f"HTTP {exc.status_code}: {exc.detail}")
        return JSONResponse(
            status_code=exc.status_code,
            content={
                "error": exc.detail,
                "status_code": exc.status_code,
                "path": str(request.url.path)
            }
        )
    @app.exception_handler(Exception)
    async def general_exception_handler(request: Request, exc: Exception):
        logger.error(f"Unhandled exception: {exc}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal server error",
                "status_code": 500,
                "path": str(request.url.path)
            }
        )
    @app.get("/health")
    async def health_check() -> Dict[str, Any]:
        try:
            db_health = db_service.health_check()
            return {
                "status": "healthy" if db_health["status"] == "healthy" else "degraded",
                "timestamp": time.time(),
                "database": db_health,
                "version": "0.1.0"
            }
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return {
                "status": "unhealthy",
                "timestamp": time.time(),
                "error": str(e),
                "version": "0.1.0"
            }
    api_router = create_router(config, db_service)
    app.include_router(api_router, prefix="/api")
    try:
        app.mount("/", StaticFiles(directory=config.web.static_path, html=True), name="static")
    except RuntimeError:
        logger.warning(f"Static files directory not found: {config.web.static_path}")
    app.state.config = config
    app.state.db_service = db_service
    app.state.schema = schema
    logger.info("FastAPI application created successfully")
    return app
app = create_app()
</file>

<file path="src/codebased/api/models.py">
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
class QueryRequest(BaseModel):
    query: str = Field(..., description="Cypher query to execute")
    parameters: Optional[Dict[str, Any]] = Field(None, description="Query parameters")
class UpdateRequest(BaseModel):
    directory_path: Optional[str] = Field(None, description="Directory to update (defaults to project root)")
    force_full: bool = Field(False, description="Force full rebuild instead of incremental")
class QueryResponse(BaseModel):
    data: List[Dict[str, Any]] = Field(..., description="Query results")
    execution_time: float = Field(..., description="Query execution time in seconds")
    record_count: int = Field(..., description="Number of records returned")
class UpdateStatistics(BaseModel):
    total_files: int = 0
    files_added: int = 0
    files_modified: int = 0
    files_removed: int = 0
    files_unchanged: int = 0
    entities_added: int = 0
    entities_updated: int = 0
    entities_removed: int = 0
    relationships_added: int = 0
    relationships_updated: int = 0
    relationships_removed: int = 0
    update_time: float = 0.0
class UpdateResponse(BaseModel):
    success: bool = Field(..., description="Whether update was successful")
    message: str = Field(..., description="Update result message")
    statistics: UpdateStatistics = Field(..., description="Update statistics")
    errors: List[str] = Field(default_factory=list, description="Any errors encountered")
class GraphNode(BaseModel):
    id: str = Field(..., description="Unique node identifier")
    name: str = Field(..., description="Node display name")
    type: str = Field(..., description="Node type (File, Class, Function, etc.)")
    file_path: Optional[str] = Field(None, description="Source file path")
    line_start: Optional[int] = Field(None, description="Starting line number")
    line_end: Optional[int] = Field(None, description="Ending line number")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional node metadata")
class GraphEdge(BaseModel):
    source: str = Field(..., description="Source node ID")
    target: str = Field(..., description="Target node ID")
    relationship_type: str = Field(..., description="Type of relationship")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional edge metadata")
class GraphMetadata(BaseModel):
    total_nodes: int = Field(..., description="Total number of nodes")
    total_edges: int = Field(..., description="Total number of edges")
    node_types: List[str] = Field(..., description="Available node types")
    filters: Dict[str, Any] = Field(..., description="Applied filters")
class GraphResponse(BaseModel):
    nodes: List[GraphNode] = Field(..., description="Graph nodes")
    edges: List[GraphEdge] = Field(..., description="Graph edges")
    metadata: GraphMetadata = Field(..., description="Graph metadata")
class TreeNode(BaseModel):
    name: str = Field(..., description="File or directory name")
    path: str = Field(..., description="Relative path from project root")
    type: str = Field(..., description="Type: 'file' or 'directory'")
    size: Optional[int] = Field(None, description="File size in bytes")
    modified_time: Optional[float] = Field(None, description="Last modified timestamp")
    children: List['TreeNode'] = Field(default_factory=list, description="Child nodes")
TreeNode.model_rebuild()
class TreeResponse(BaseModel):
    tree: TreeNode = Field(..., description="Root tree node")
    root_path: str = Field(..., description="Root path used for tree generation")
class QueryTemplate(BaseModel):
    id: str = Field(..., description="Unique template identifier")
    name: str = Field(..., description="Template display name")
    description: str = Field(..., description="Template description")
    query: str = Field(..., description="Cypher query template")
    parameters: List[str] = Field(..., description="Required parameter names")
    example_params: Dict[str, Any] = Field(..., description="Example parameter values")
class TemplatesResponse(BaseModel):
    templates: List[QueryTemplate] = Field(..., description="Available query templates")
class StatusResponse(BaseModel):
    database_stats: Dict[str, Any] = Field(..., description="Database statistics")
    database_health: Dict[str, Any] = Field(..., description="Database health information")
    update_status: Dict[str, Any] = Field(..., description="Update system status")
    configuration: Dict[str, Any] = Field(..., description="Current configuration")
</file>

<file path="src/codebased/database/__init__.py">

</file>

<file path="src/codebased/database/service.py">
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import kuzu
logger = logging.getLogger(__name__)
class DatabaseService:
    def __init__(self, db_path: str = None):
        if db_path is None:
            db_path = ".codebased/data/graph.kuzu"
        self.db_path = Path(db_path)
        self.db = None
        self.conn = None
    def initialize(self) -> bool:
        try:
            self.db_path.parent.mkdir(parents=True, exist_ok=True)
            # Initialize Kuzu database
            self.db = kuzu.Database(str(self.db_path))
            self.conn = kuzu.Connection(self.db)
            logger.info(f"Database initialized at {self.db_path}")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize database: {e}")
            return False
    def connect(self) -> bool:
        try:
            if self.db is None:
                self.db = kuzu.Database(str(self.db_path))
            if self.conn is None:
                self.conn = kuzu.Connection(self.db)
            logger.debug("Database connection established")
            return True
        except Exception as e:
            logger.error(f"Failed to connect to database: {e}")
            return False
    def disconnect(self) -> None:
        try:
            if self.conn:
                self.conn.close()
                self.conn = None
            if self.db:
                self.db.close()
                self.db = None
            logger.debug("Database connection closed")
        except Exception as e:
            logger.error(f"Error closing database connection: {e}")
    def execute_query(self, query: str, parameters: Dict[str, Any] = None) -> Optional[List[Dict[str, Any]]]:
        if not self.conn:
            if not self.connect():
                return None
        try:
            # Execute query with parameters if provided
            if parameters:
                result = self.conn.execute(query, parameters)
            else:
                result = self.conn.execute(query)
            # Convert result to list of dictionaries
            records = []
            # Get column names from the query result
            columns = []
            if hasattr(result, 'get_column_names'):
                columns = result.get_column_names()
            else:
                # Parse column names from the RETURN clause as fallback
                import re
                return_match = re.search(r'RETURN\s+(.+?)(?:\s+LIMIT|\s+ORDER\s+BY|$)', query, re.IGNORECASE)
                if return_match:
                    return_clause = return_match.group(1)
                    # Extract column names/aliases
                    columns = [col.strip().split(' AS ')[-1].strip() for col in return_clause.split(',')]
            while result.has_next():
                record = result.get_next()
                if isinstance(record, list) and columns:
                    # Convert list to dictionary using column names
                    record_dict = {columns[i]: record[i] for i in range(min(len(columns), len(record)))}
                    records.append(record_dict)
                elif isinstance(record, dict):
                    records.append(record)
                else:
                    # Fallback: return as is if we can't determine structure
                    records.append(record)
            return records
        except Exception as e:
            logger.error(f"Query execution failed: {e}")
            logger.debug(f"Query: {query}")
            if parameters:
                logger.debug(f"Parameters: {parameters}")
            return None
    def execute_batch(self, queries: List[str]) -> bool:
        if not self.conn:
            if not self.connect():
                return False
        try:
            for query in queries:
                result = self.conn.execute(query)
            logger.debug(f"Executed batch of {len(queries)} queries")
            return True
        except Exception as e:
            logger.error(f"Batch execution failed: {e}")
            return False
    def clear_graph(self) -> bool:
        try:
            node_tables_result = self.execute_query("CALL show_tables() RETURN *")
            if node_tables_result:
                for table in node_tables_result:
                    if isinstance(table, list) and len(table) >= 2:
                        table_name = table[0]
                        table_type = table[1]
                    elif isinstance(table, dict):
                        table_name = table.get('name', '')
                        table_type = table.get('type', '')
                    else:
                        continue
                    if table_name and table_type == 'NODE':
                        self.execute_query(f"MATCH (n:{table_name}) DETACH DELETE n")
            logger.info("Graph database cleared")
            return True
        except Exception as e:
            logger.error(f"Failed to clear graph: {e}")
            return False
    def get_stats(self) -> Dict[str, int]:
        stats = {"nodes": 0, "relationships": 0, "tables": 0}
        try:
            tables_result = self.execute_query("CALL show_tables() RETURN *")
            if tables_result:
                stats["tables"] = len(tables_result)
                for table in tables_result:
                    table_name = table.get('name', '')
                    table_type = table.get('type', '')
                    if table_type == 'NODE' and table_name:
                        count_result = self.execute_query(f"MATCH (n:{table_name}) RETURN COUNT(n) AS count")
                        if count_result and len(count_result) > 0:
                            stats["nodes"] += count_result[0].get('count', 0)
                    elif table_type == 'REL' and table_name:
                        count_result = self.execute_query(f"MATCH ()-[r:{table_name}]->() RETURN COUNT(r) AS count")
                        if count_result and len(count_result) > 0:
                            stats["relationships"] += count_result[0].get('count', 0)
        except Exception as e:
            logger.error(f"Failed to get database stats: {e}")
        return stats
    def health_check(self) -> Dict[str, Any]:
        health = {
            "status": "unknown",
            "db_path": str(self.db_path),
            "db_exists": self.db_path.exists(),
            "connected": False,
            "stats": {}
        }
        try:
            if self.connect():
                health["connected"] = True
                health["stats"] = self.get_stats()
                health["status"] = "healthy"
            else:
                health["status"] = "connection_failed"
        except Exception as e:
            health["status"] = "error"
            health["error"] = str(e)
        return health
_db_service: Optional[DatabaseService] = None
def get_database_service(db_path: str = None) -> DatabaseService:
    global _db_service
    if _db_service is None:
        _db_service = DatabaseService(db_path)
    return _db_service
</file>

<file path="src/codebased/parsers/__init__.py">

</file>

<file path="src/codebased/parsers/angular.py">
import time
import logging
from typing import List, Dict, Any
from .base import BaseParser, ParsedEntity, ParsedRelationship, ParseResult
logger = logging.getLogger(__name__)
class AngularParser(BaseParser):
    SUPPORTED_FILE_TYPES = {"angular"}
    def parse_file(self, file_path: str) -> ParseResult:
        start_time = time.time()
        entities = []
        relationships = []
        errors = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            file_hash = self._calculate_file_hash(file_path)
            file_entity = ParsedEntity(
                id=self._generate_entity_id("file", file_path, 1),
                name=file_path,
                type="File",
                file_path=file_path,
                line_start=1,
                line_end=len(content.splitlines()),
                metadata={}
            )
            entities.append(file_entity)
        except Exception as e:
            error_msg = f"Failed to parse {file_path}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
            file_hash = ""
        parse_time = time.time() - start_time
        return ParseResult(entities, relationships, file_hash, file_path, errors, parse_time)
</file>

<file path="src/codebased/parsers/css.py">
import time
import logging
from typing import List, Dict, Any
from .base import BaseParser, ParsedEntity, ParsedRelationship, ParseResult
logger = logging.getLogger(__name__)
class CSSParser(BaseParser):
    SUPPORTED_FILE_TYPES = {"css"}
    def parse_file(self, file_path: str) -> ParseResult:
        start_time = time.time()
        entities = []
        relationships = []
        errors = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            file_hash = self._calculate_file_hash(file_path)
            file_entity = ParsedEntity(
                id=self._generate_entity_id("file", file_path, 1),
                name=file_path,
                type="File",
                file_path=file_path,
                line_start=1,
                line_end=len(content.splitlines()),
                metadata={}
            )
            entities.append(file_entity)
        except Exception as e:
            error_msg = f"Failed to parse {file_path}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
            file_hash = ""
        parse_time = time.time() - start_time
        return ParseResult(entities, relationships, file_hash, file_path, errors, parse_time)
</file>

<file path="src/codebased/parsers/html.py">
import time
import logging
from typing import List, Dict, Any
from .base import BaseParser, ParsedEntity, ParsedRelationship, ParseResult
logger = logging.getLogger(__name__)
class HTMLParser(BaseParser):
    SUPPORTED_FILE_TYPES = {"html"}
    def parse_file(self, file_path: str) -> ParseResult:
        start_time = time.time()
        entities = []
        relationships = []
        errors = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            file_hash = self._calculate_file_hash(file_path)
            file_entity = ParsedEntity(
                id=self._generate_entity_id("file", file_path, 1),
                name=file_path,
                type="File",
                file_path=file_path,
                line_start=1,
                line_end=len(content.splitlines()),
                metadata={}
            )
            entities.append(file_entity)
        except Exception as e:
            error_msg = f"Failed to parse {file_path}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
            file_hash = ""
        parse_time = time.time() - start_time
        return ParseResult(entities, relationships, file_hash, file_path, errors, parse_time)
</file>

<file path="src/codebased/parsers/javascript.py">
import time
import logging
from typing import List, Dict, Any
from .base import BaseParser, ParsedEntity, ParsedRelationship, ParseResult
logger = logging.getLogger(__name__)
class JavaScriptParser(BaseParser):
    SUPPORTED_FILE_TYPES = {"javascript"}
    def parse_file(self, file_path: str) -> ParseResult:
        start_time = time.time()
        entities = []
        relationships = []
        errors = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            file_hash = self._calculate_file_hash(file_path)
            file_entity = ParsedEntity(
                id=self._generate_entity_id("file", file_path, 1),
                name=file_path,
                type="File",
                file_path=file_path,
                line_start=1,
                line_end=len(content.splitlines()),
                metadata={}
            )
            entities.append(file_entity)
        except Exception as e:
            error_msg = f"Failed to parse {file_path}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
            file_hash = ""
        parse_time = time.time() - start_time
        return ParseResult(entities, relationships, file_hash, file_path, errors, parse_time)
</file>

<file path="src/codebased/parsers/nodejs.py">
import time
import logging
from typing import List, Dict, Any
from .base import BaseParser, ParsedEntity, ParsedRelationship, ParseResult
logger = logging.getLogger(__name__)
class NodeJSParser(BaseParser):
    SUPPORTED_FILE_TYPES = {"nodejs"}
    def parse_file(self, file_path: str) -> ParseResult:
        start_time = time.time()
        entities = []
        relationships = []
        errors = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            file_hash = self._calculate_file_hash(file_path)
            file_entity = ParsedEntity(
                id=self._generate_entity_id("file", file_path, 1),
                name=file_path,
                type="File",
                file_path=file_path,
                line_start=1,
                line_end=len(content.splitlines()),
                metadata={}
            )
            entities.append(file_entity)
        except Exception as e:
            error_msg = f"Failed to parse {file_path}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
            file_hash = ""
        parse_time = time.time() - start_time
        return ParseResult(entities, relationships, file_hash, file_path, errors, parse_time)
</file>

<file path="src/codebased/parsers/treesitter_setup.py">
from __future__ import annotations
import importlib
import logging
from functools import lru_cache
from tree_sitter import Language
logger = logging.getLogger(__name__)
@lru_cache(maxsize=None)
def ensure_language(lang: str) -> Language:
    try:
        ts_langs = importlib.import_module("tree_sitter_languages")
        return ts_langs.get_language(lang)
    except Exception as e:
        logger.debug("tree_sitter_languages not usable: %s", e)
    so_path = __name__.replace(".", "/")
    so_path = f"{__file__[:-3]}_{lang}.so"
    try:
        return Language(so_path, lang)
    except Exception as e:
        logger.error("Tree-sitter language '%s' not installed: %s", lang, e)
        raise
</file>

<file path="src/codebased/parsers/typescript.py">
import time
import logging
from typing import List, Dict, Any
from .base import BaseParser, ParsedEntity, ParsedRelationship, ParseResult
logger = logging.getLogger(__name__)
class TypeScriptParser(BaseParser):
    SUPPORTED_FILE_TYPES = {"typescript"}
    def parse_file(self, file_path: str) -> ParseResult:
        start_time = time.time()
        entities = []
        relationships = []
        errors = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            file_hash = self._calculate_file_hash(file_path)
            file_entity = ParsedEntity(
                id=self._generate_entity_id("file", file_path, 1),
                name=file_path,
                type="File",
                file_path=file_path,
                line_start=1,
                line_end=len(content.splitlines()),
                metadata={}
            )
            entities.append(file_entity)
        except Exception as e:
            error_msg = f"Failed to parse {file_path}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
            file_hash = ""
        parse_time = time.time() - start_time
        return ParseResult(entities, relationships, file_hash, file_path, errors, parse_time)
</file>

<file path="src/codebased/__init__.py">
__version__ = "0.1.0"
__author__ = "CodeBased"
__email__ = "dev@codebased.ai"
</file>

<file path="src/codebased/cli.py">
import os
import sys
import logging
import subprocess
from pathlib import Path
from typing import Optional
import click
import uvicorn
from .config import get_config, create_default_config
from .database.service import get_database_service
from .database.schema import GraphSchema
from .parsers.incremental import IncrementalUpdater
def setup_logging(level: str = "INFO"):
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
@click.group(invoke_without_command=True)
@click.option('--config', '-c', default='.codebased.yml', help='Configuration file path')
@click.option('--verbose', '-v', is_flag=True, help='Enable verbose logging')
@click.option('--version', is_flag=True, help='Show version and exit')
@click.pass_context
def main(ctx, config: str, verbose: bool, version: bool):
    if version:
        click.echo("CodeBased 0.1.0")
        return
    ctx.ensure_object(dict)
    ctx.obj['config_path'] = config
    ctx.obj['verbose'] = verbose
    setup_logging("DEBUG" if verbose else "INFO")
    if ctx.invoked_subcommand is None:
        click.echo(ctx.get_help())
@main.command()
@click.option('--force', is_flag=True, help='Overwrite existing configuration')
@click.pass_context
def init(ctx, force: bool):
    config_path = ctx.obj['config_path']
    try:
        if Path(config_path).exists() and not force:
            click.echo(f"CodeBased already initialized. Use --force to overwrite.")
            return
        base_dir = Path('.codebased')
        directories = [
            base_dir,
            base_dir / 'data',
            base_dir / 'web',
            base_dir / 'config',
            base_dir / 'logs'
        ]
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            click.echo(f"Created directory: {directory}")
        config = create_default_config(config_path)
        click.echo(f"Created configuration file: {config_path}")
        db_service = get_database_service(config.database.path)
        if not db_service.initialize():
            click.echo("Error: Failed to initialize database", err=True)
            return
        schema = GraphSchema(db_service)
        if not schema.create_schema():
            click.echo("Error: Failed to create database schema", err=True)
            return
        click.echo("Database initialized successfully")
        web_dir = Path(config.web.static_path)
        web_dir.mkdir(parents=True, exist_ok=True)
        index_html = web_dir / 'index.html'
        if not index_html.exists():
            index_html.write_text("""<!DOCTYPE html>
<html>
<head>
    <title>CodeBased</title>
</head>
<body>
    <h1>CodeBased</h1>
    <p>Code graph visualization will be available here.</p>
    <p>API documentation: <a href="/docs">/docs</a></p>
</body>
</html>""")
            click.echo(f"Created web interface: {index_html}")
        click.echo("")
        click.echo("✅ CodeBased initialized successfully!")
        click.echo("")
        click.echo("Next steps:")
        click.echo("1. Run 'codebased update' to analyze your code")
        click.echo("2. Run 'codebased serve' to start the web interface")
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        sys.exit(1)
@main.command()
@click.option('--full', is_flag=True, help='Force full rebuild instead of incremental')
@click.option('--path', help='Directory to update (defaults to current directory)')
@click.pass_context
def update(ctx, full: bool, path: Optional[str]):
    config_path = ctx.obj['config_path']
    try:
        config = get_config(config_path)
        db_service = get_database_service(config.database.path)
        if not db_service.connect():
            click.echo("Error: Could not connect to database", err=True)
            return
        updater = IncrementalUpdater(config, db_service)
        click.echo("Starting code graph update...")
        if full:
            click.echo("Performing full rebuild...")
            results = updater.force_full_update(path)
        else:
            click.echo("Performing incremental update...")
            results = updater.update_graph(path)
        click.echo("")
        click.echo("Update Results:")
        click.echo(f"  Files processed: {results.get('files_added', 0) + results.get('files_modified', 0)}")
        click.echo(f"  Files added: {results.get('files_added', 0)}")
        click.echo(f"  Files modified: {results.get('files_modified', 0)}")
        click.echo(f"  Files removed: {results.get('files_removed', 0)}")
        click.echo(f"  Entities: {results.get('entities_added', 0)} added, {results.get('entities_removed', 0)} removed")
        click.echo(f"  Relationships: {results.get('relationships_added', 0)} added, {results.get('relationships_removed', 0)} removed")
        click.echo(f"  Update time: {results.get('update_time', 0):.2f}s")
        if results.get('errors'):
            click.echo("")
            click.echo("Errors encountered:")
            for error in results['errors']:
                click.echo(f"  - {error}", err=True)
        if not results.get('errors'):
            click.echo("")
            click.echo("✅ Update completed successfully!")
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        sys.exit(1)
@main.command()
@click.option('--host', help='Host to bind to (overrides config)')
@click.option('--port', type=int, help='Port to bind to (overrides config)')
@click.option('--reload', is_flag=True, help='Enable auto-reload for development')
@click.option('--debug', is_flag=True, help='Enable debug mode')
@click.pass_context
def serve(ctx, host: Optional[str], port: Optional[int], reload: bool, debug: bool):
    config_path = ctx.obj['config_path']
    try:
        config = get_config(config_path)
        if host:
            config.api.host = host
        if port:
            config.api.port = port
        if reload:
            config.api.reload = reload
        if debug:
            config.api.debug = debug
        click.echo(f"Starting server at http://{config.api.host}:{config.api.port}")
        click.echo("Press Ctrl+C to stop")
        uvicorn.run(
            "codebased.api.main:app",
            host=config.api.host,
            port=config.api.port,
            reload=config.api.reload,
            log_level="debug" if config.api.debug else "info"
        )
    except KeyboardInterrupt:
        click.echo("\nServer stopped")
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        sys.exit(1)
@main.command()
@click.argument('query', required=True)
@click.option('--format', 'output_format', default='table', type=click.Choice(['table', 'json', 'csv']),
              help='Output format')
@click.option('--limit', type=int, default=100, help='Limit number of results')
@click.pass_context
def query(ctx, query: str, output_format: str, limit: int):
    config_path = ctx.obj['config_path']
    try:
        config = get_config(config_path)
        db_service = get_database_service(config.database.path)
        if not db_service.connect():
            click.echo("Error: Could not connect to database", err=True)
            return
        if 'LIMIT' not in query.upper():
            query = f"{query} LIMIT {limit}"
        results = db_service.execute_query(query)
        if results is None:
            click.echo("Query execution failed", err=True)
            return
        if not results:
            click.echo("No results found")
            return
        if output_format == 'json':
            import json
            click.echo(json.dumps(results, indent=2, default=str))
        elif output_format == 'csv':
            import csv
            import io
            if results:
                output = io.StringIO()
                writer = csv.DictWriter(output, fieldnames=results[0].keys())
                writer.writeheader()
                writer.writerows(results)
                click.echo(output.getvalue().strip())
        else:
            if results:
                headers = list(results[0].keys())
                widths = {header: len(header) for header in headers}
                for row in results:
                    for header in headers:
                        value_len = len(str(row.get(header, '')))
                        if value_len > widths[header]:
                            widths[header] = min(value_len, 50)
                header_row = ' | '.join(header.ljust(widths[header]) for header in headers)
                separator = '-+-'.join('-' * widths[header] for header in headers)
                click.echo(header_row)
                click.echo(separator)
                for row in results:
                    row_data = []
                    for header in headers:
                        value = str(row.get(header, ''))
                        if len(value) > 50:
                            value = value[:47] + '...'
                        row_data.append(value.ljust(widths[header]))
                    click.echo(' | '.join(row_data))
        click.echo(f"\n{len(results)} rows returned")
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        sys.exit(1)
@main.command()
@click.pass_context
def status(ctx):
    config_path = ctx.obj['config_path']
    try:
        config = get_config(config_path)
        db_service = get_database_service(config.database.path)
        health = db_service.health_check()
        click.echo("CodeBased Status")
        click.echo("================")
        click.echo(f"Project root: {config.project_root}")
        click.echo(f"Database path: {config.database.path}")
        click.echo(f"Database status: {health['status']}")
        if health['db_exists']:
            stats = db_service.get_stats()
            click.echo(f"Nodes: {stats.get('nodes', 0)}")
            click.echo(f"Relationships: {stats.get('relationships', 0)}")
            click.echo(f"Tables: {stats.get('tables', 0)}")
        updater = IncrementalUpdater(config, db_service)
        update_status = updater.get_update_status()
        click.echo(f"Tracked files: {update_status.get('tracked_files', 0)}")
        if health['status'] == 'healthy':
            click.echo("\n✅ System is healthy")
        else:
            click.echo(f"\n❌ System status: {health['status']}")
            if 'error' in health:
                click.echo(f"Error: {health['error']}")
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        sys.exit(1)
@main.command()
@click.confirmation_option(prompt='Are you sure you want to reset the database?')
@click.pass_context
def reset(ctx):
    config_path = ctx.obj['config_path']
    try:
        config = get_config(config_path)
        db_service = get_database_service(config.database.path)
        if not db_service.connect():
            click.echo("Error: Could not connect to database", err=True)
            return
        schema = GraphSchema(db_service)
        if not schema.reset_schema():
            click.echo("Error: Failed to reset database schema", err=True)
            return
        click.echo("✅ Database reset successfully")
        click.echo("Run 'codebased update' to rebuild the graph")
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        sys.exit(1)
if __name__ == '__main__':
    main()
</file>

<file path="src/codebased/config.py">
import os
import yaml
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings
@dataclass
class ParsingConfig:
    file_extensions: List[str] = field(default_factory=lambda: ['.py'])
    exclude_patterns: List[str] = field(default_factory=lambda: [
        '__pycache__',
        '*.pyc',
        '.git',
        'node_modules',
        '.env',
        'venv',
        'env'
    ])
    include_docstrings: bool = True
    max_file_size: int = 1024 * 1024
    follow_symlinks: bool = False
@dataclass
class DatabaseConfig:
    path: str = ".codebased/data/graph.kuzu"
    query_timeout: int = 30
    batch_size: int = 1000
    auto_backup: bool = True
    backup_retention_days: int = 7
@dataclass
class APIConfig:
    host: str = "127.0.0.1"
    port: int = 8000
    debug: bool = False
    reload: bool = False
    cors_origins: List[str] = field(default_factory=lambda: ["*"])
    max_query_time: int = 30
    enable_docs: bool = True
@dataclass
class WebConfig:
    static_path: str = ".codebased/web"
    template_path: str = ".codebased/web/templates"
    max_nodes: int = 1000
    max_edges: int = 5000
    default_layout: str = "force"
@dataclass
class CodeBasedConfig:
    project_root: str = "."
    parsing: ParsingConfig = field(default_factory=ParsingConfig)
    database: DatabaseConfig = field(default_factory=DatabaseConfig)
    api: APIConfig = field(default_factory=APIConfig)
    web: WebConfig = field(default_factory=WebConfig)
    log_level: str = "INFO"
    @classmethod
    def find_config_file(cls, start_path: str = ".") -> Optional[Path]:
        current_path = Path(start_path).resolve()
        while True:
            config_file = current_path / ".codebased.yml"
            if config_file.exists():
                return config_file
            parent = current_path.parent
            if parent == current_path:
                break
            current_path = parent
        return None
    @classmethod
    def load_from_project_root(cls, start_path: str = ".") -> 'CodeBasedConfig':
        config_file = cls.find_config_file(start_path)
        if config_file:
            return cls.from_file(str(config_file))
        else:
            # Return default config with current directory as project root
            config = cls()
            config.project_root = str(Path(start_path).resolve())
            return config
    @classmethod
    def from_file(cls, config_path: str) -> 'CodeBasedConfig':
        config_path = Path(config_path)
        if not config_path.exists():
            return cls()
        try:
            # Get the directory containing the config file to resolve relative paths
            config_dir = config_path.parent.resolve()
            with open(config_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f) or {}
            # Create config with nested dataclasses
            config = cls()
            # Resolve project_root relative to config file location
            if 'project_root' in data:
                project_root = Path(data['project_root'])
                if not project_root.is_absolute():
                    project_root = (config_dir / project_root).resolve()
                config.project_root = str(project_root)
            else:
                # Default to the directory containing the config file
                config.project_root = str(config_dir)
            if 'log_level' in data:
                config.log_level = data['log_level']
            if 'parsing' in data:
                parsing_data = data['parsing']
                config.parsing = ParsingConfig(
                    file_extensions=parsing_data.get('file_extensions', config.parsing.file_extensions),
                    exclude_patterns=parsing_data.get('exclude_patterns', config.parsing.exclude_patterns),
                    include_docstrings=parsing_data.get('include_docstrings', config.parsing.include_docstrings),
                    max_file_size=parsing_data.get('max_file_size', config.parsing.max_file_size),
                    follow_symlinks=parsing_data.get('follow_symlinks', config.parsing.follow_symlinks)
                )
            if 'database' in data:
                db_data = data['database']
                # Resolve database path relative to project root
                db_path = db_data.get('path', config.database.path)
                if db_path and not Path(db_path).is_absolute():
                    db_path = str((Path(config.project_root) / db_path).resolve())
                config.database = DatabaseConfig(
                    path=db_path,
                    query_timeout=db_data.get('query_timeout', config.database.query_timeout),
                    batch_size=db_data.get('batch_size', config.database.batch_size),
                    auto_backup=db_data.get('auto_backup', config.database.auto_backup),
                    backup_retention_days=db_data.get('backup_retention_days', config.database.backup_retention_days)
                )
            if 'api' in data:
                api_data = data['api']
                config.api = APIConfig(
                    host=api_data.get('host', config.api.host),
                    port=api_data.get('port', config.api.port),
                    debug=api_data.get('debug', config.api.debug),
                    reload=api_data.get('reload', config.api.reload),
                    cors_origins=api_data.get('cors_origins', config.api.cors_origins),
                    max_query_time=api_data.get('max_query_time', config.api.max_query_time),
                    enable_docs=api_data.get('enable_docs', config.api.enable_docs)
                )
            if 'web' in data:
                web_data = data['web']
                # Resolve web paths relative to project root
                static_path = web_data.get('static_path', config.web.static_path)
                template_path = web_data.get('template_path', config.web.template_path)
                if static_path and not Path(static_path).is_absolute():
                    static_path = str((Path(config.project_root) / static_path).resolve())
                if template_path and not Path(template_path).is_absolute():
                    template_path = str((Path(config.project_root) / template_path).resolve())
                config.web = WebConfig(
                    static_path=static_path,
                    template_path=template_path,
                    max_nodes=web_data.get('max_nodes', config.web.max_nodes),
                    max_edges=web_data.get('max_edges', config.web.max_edges),
                    default_layout=web_data.get('default_layout', config.web.default_layout)
                )
            return config
        except Exception as e:
            raise ValueError(f"Failed to load configuration from {config_path}: {e}")
    def to_file(self, config_path: str) -> None:
        config_path = Path(config_path)
        config_path.parent.mkdir(parents=True, exist_ok=True)
        # Convert dataclasses to dictionaries
        data = {
            'project_root': self.project_root,
            'log_level': self.log_level,
            'parsing': {
                'file_extensions': self.parsing.file_extensions,
                'exclude_patterns': self.parsing.exclude_patterns,
                'include_docstrings': self.parsing.include_docstrings,
                'max_file_size': self.parsing.max_file_size,
                'follow_symlinks': self.parsing.follow_symlinks
            },
            'database': {
                'path': self.database.path,
                'query_timeout': self.database.query_timeout,
                'batch_size': self.database.batch_size,
                'auto_backup': self.database.auto_backup,
                'backup_retention_days': self.database.backup_retention_days
            },
            'api': {
                'host': self.api.host,
                'port': self.api.port,
                'debug': self.api.debug,
                'reload': self.api.reload,
                'cors_origins': self.api.cors_origins,
                'max_query_time': self.api.max_query_time,
                'enable_docs': self.api.enable_docs
            },
            'web': {
                'static_path': self.web.static_path,
                'template_path': self.web.template_path,
                'max_nodes': self.web.max_nodes,
                'max_edges': self.web.max_edges,
                'default_layout': self.web.default_layout
            }
        }
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(data, f, default_flow_style=False, indent=2)
        except Exception as e:
            raise ValueError(f"Failed to save configuration to {config_path}: {e}")
class EnvironmentSettings(BaseSettings):
    codebased_project_root: Optional[str] = None
    codebased_log_level: Optional[str] = None
    codebased_api_host: Optional[str] = None
    codebased_api_port: Optional[int] = None
    codebased_api_debug: Optional[bool] = None
    codebased_database_path: Optional[str] = None
    class Config:
        env_prefix = "CODEBASED_"
        case_sensitive = False
def load_config(config_path: str = ".codebased.yml") -> CodeBasedConfig:
    config = CodeBasedConfig.from_file(config_path)
    env_settings = EnvironmentSettings()
    if env_settings.codebased_project_root:
        config.project_root = env_settings.codebased_project_root
    if env_settings.codebased_log_level:
        config.log_level = env_settings.codebased_log_level
    if env_settings.codebased_api_host:
        config.api.host = env_settings.codebased_api_host
    if env_settings.codebased_api_port:
        config.api.port = env_settings.codebased_api_port
    if env_settings.codebased_api_debug is not None:
        config.api.debug = env_settings.codebased_api_debug
    if env_settings.codebased_database_path:
        config.database.path = env_settings.codebased_database_path
    return config
def create_default_config(config_path: str = ".codebased.yml") -> CodeBasedConfig:
    config = CodeBasedConfig()
    config.to_file(config_path)
    return config
_config: Optional[CodeBasedConfig] = None
def get_config(config_path: str = ".codebased.yml") -> CodeBasedConfig:
    global _config
    if _config is None:
        _config = load_config(config_path)
    return _config
</file>

<file path="tests/__init__.py">

</file>

<file path="tests/test_api.py">
import unittest
import tempfile
import shutil
import json
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))
try:
    from fastapi.testclient import TestClient
    from codebased.api.main import create_app
    from codebased.config import CodeBasedConfig
    FASTAPI_AVAILABLE = True
except ImportError:
    FASTAPI_AVAILABLE = False
@unittest.skipUnless(FASTAPI_AVAILABLE, "FastAPI not available")
class TestAPIEndpoints(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.temp_dir = Path(tempfile.mkdtemp())
        cls.db_path = cls.temp_dir / "test_api.kuzu"
        config = CodeBasedConfig()
        config.database.path = str(cls.db_path)
        config.project_root = str(cls.temp_dir)
        config.api.debug = True
        try:
            cls.app = create_app()
            cls.client = TestClient(cls.app)
        except Exception as e:
            print(f"Failed to create test app: {e}")
            cls.app = None
            cls.client = None
    @classmethod
    def tearDownClass(cls):
        if cls.temp_dir.exists():
            shutil.rmtree(cls.temp_dir)
    def setUp(self):
        if self.client is None:
            self.skipTest("Failed to create test client")
    def test_health_endpoint(self):
        response = self.client.get("/health")
        self.assertEqual(response.status_code, 200)
        data = response.json()
        self.assertIn('status', data)
        self.assertIn('timestamp', data)
        self.assertIn('version', data)
    def test_query_endpoint_simple(self):
        query_data = {
            "query": "RETURN 1 AS test_value",
            "parameters": {}
        }
        response = self.client.post("/api/query", json=query_data)
        self.assertIn(response.status_code, [200, 500])
        if response.status_code == 200:
            data = response.json()
            self.assertIn('data', data)
            self.assertIn('execution_time', data)
            self.assertIn('record_count', data)
    def test_query_endpoint_validation(self):
        query_data = {
            "query": "",
            "parameters": {}
        }
        response = self.client.post("/api/query", json=query_data)
        self.assertEqual(response.status_code, 400)
        query_data = {
            "query": "DROP TABLE File",
            "parameters": {}
        }
        response = self.client.post("/api/query", json=query_data)
        self.assertEqual(response.status_code, 403)
    def test_update_endpoint(self):
        update_data = {
            "directory_path": str(self.temp_dir),
            "force_full": False
        }
        response = self.client.post("/api/update", json=update_data)
        self.assertIn(response.status_code, [200, 500])
        if response.status_code == 200:
            data = response.json()
            self.assertIn('success', data)
            self.assertIn('statistics', data)
    def test_graph_endpoint(self):
        response = self.client.get("/api/graph")
        self.assertIn(response.status_code, [200, 500])
        if response.status_code == 200:
            data = response.json()
            self.assertIn('nodes', data)
            self.assertIn('edges', data)
            self.assertIn('metadata', data)
    def test_graph_endpoint_with_filters(self):
        params = {
            "node_types": "Function,Class",
            "max_nodes": 50,
            "max_edges": 100,
            "file_filter": "test"
        }
        response = self.client.get("/api/graph", params=params)
        self.assertIn(response.status_code, [200, 500])
    def test_tree_endpoint(self):
        response = self.client.get("/api/tree")
        self.assertIn(response.status_code, [200, 404, 500])
        if response.status_code == 200:
            data = response.json()
            self.assertIn('tree', data)
            self.assertIn('root_path', data)
    def test_templates_endpoint(self):
        response = self.client.get("/api/templates")
        self.assertEqual(response.status_code, 200)
        data = response.json()
        self.assertIn('templates', data)
        self.assertIsInstance(data['templates'], list)
        if data['templates']:
            template = data['templates'][0]
            expected_keys = {'id', 'name', 'description', 'query', 'parameters', 'example_params'}
            self.assertTrue(expected_keys.issubset(template.keys()))
    def test_status_endpoint(self):
        response = self.client.get("/api/status")
        self.assertIn(response.status_code, [200, 500])
        if response.status_code == 200:
            data = response.json()
            expected_keys = {'database_stats', 'database_health', 'update_status', 'configuration'}
            self.assertTrue(expected_keys.issubset(data.keys()))
    def test_api_documentation(self):
        response = self.client.get("/docs")
        self.assertIn(response.status_code, [200, 307, 404])
    def test_cors_headers(self):
        response = self.client.options("/api/query")
        self.assertIn(response.status_code, [200, 405])
    def test_error_handling(self):
        response = self.client.get("/api/nonexistent")
        self.assertEqual(response.status_code, 404)
        response = self.client.post(
            "/api/query",
            data="invalid json",
            headers={"content-type": "application/json"}
        )
        self.assertEqual(response.status_code, 422)
if __name__ == '__main__':
    if not FASTAPI_AVAILABLE:
        print("FastAPI not available - skipping API tests")
        print("Install dependencies with: pip install -r requirements.txt")
    unittest.main()
</file>

<file path="tests/test_extractor_registry.py">
import sys
from types import ModuleType
from pathlib import Path
sys.modules.setdefault('kuzu', ModuleType('kuzu'))
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))
from codebased.parsers.extractor import EntityExtractor
from codebased.config import CodeBasedConfig
from codebased.parsers.registry import PARSER_REGISTRY
class DummyDB:
    def execute_batch(self, queries):
        return True
    def clear_graph(self):
        return True
def test_extractor_initializes_all_parsers():
    cfg = CodeBasedConfig()
    extractor = EntityExtractor(cfg, DummyDB())
    assert set(extractor.parsers.keys()) == set(PARSER_REGISTRY.keys())
</file>

<file path="tests/test_file_types.py">
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))
from codebased.parsers.file_types import get_file_type
def test_get_file_type_mappings():
    assert get_file_type('main.py') == 'python'
    assert get_file_type('script.js') == 'javascript'
    assert get_file_type('styles.css') == 'css'
    assert get_file_type('index.html') == 'html'
    assert get_file_type('data.json') == 'json'
def test_get_file_type_patterns():
    assert get_file_type('app.component.ts') == 'angular'
    assert get_file_type('app.module.ts') == 'angular'
    assert get_file_type('auth.service.ts') == 'angular'
    assert get_file_type('foo.component.html') == 'angular'
    assert get_file_type('foo.component.css') == 'angular'
    assert get_file_type('foo.ts') == 'typescript'
</file>

<file path="web/app.js">
class CodeBasedApp {
    constructor() {
        this.graph = null;
        this.currentFilters = {
            nodeTypes: [],
            searchText: '',
            fileFilter: ''
        };
        this.queryTemplates = [];
        this.init();
    }
    init() {
        // Initialize graph visualization
        this.graph = new GraphVisualizer('#graph-svg');
        this.performanceManager = new PerformanceManager(this.graph);
        this.performanceManager.onPerformanceUpdate = (stats) => this.handlePerformanceUpdate(stats);
        this.graph.onNodeSelected = (node) => this.handleNodeSelected(node);
        this.graph.onNodeDeselected = () => this.handleNodeDeselected();
        this.setupEventHandlers();
        this.loadQueryTemplates();
        this.loadGraphData();
        setInterval(() => this.updateStatus(), 30000);
    }
    setupEventHandlers() {
        const updateBtn = document.getElementById('update-btn');
        updateBtn?.addEventListener('click', () => this.updateGraph());
        const freezeBtn = document.getElementById('freeze-btn');
        this.isFrozen = false;
        freezeBtn?.addEventListener('click', () => {
            if (this.isFrozen) {
                this.graph.unfreezeLayout();
                freezeBtn.textContent = 'Freeze Layout';
                freezeBtn.classList.remove('btn-primary');
                freezeBtn.classList.add('btn-secondary');
            } else {
                this.graph.freezeLayout();
                freezeBtn.textContent = 'Unfreeze Layout';
                freezeBtn.classList.remove('btn-secondary');
                freezeBtn.classList.add('btn-primary');
            }
            this.isFrozen = !this.isFrozen;
        });
        this.freezeBtn = freezeBtn;
        const resetBtn = document.getElementById('reset-btn');
        resetBtn?.addEventListener('click', () => this.resetView());
        const fileFilter = document.getElementById('file-filter');
        fileFilter?.addEventListener('input', (e) => {
            this.currentFilters.fileFilter = e.target.value;
            this.debounce(() => this.applyFilters(), 300);
        });
        const searchInput = document.getElementById('search-input');
        searchInput?.addEventListener('input', (e) => {
            this.currentFilters.searchText = e.target.value;
            this.debounce(() => this.applyFilters(), 300);
        });
        const modal = document.getElementById('query-modal');
        const closeModal = document.getElementById('close-modal');
        const cancelBtn = document.getElementById('cancel-query-btn');
        const executeBtn = document.getElementById('execute-query-btn');
        closeModal?.addEventListener('click', () => this.hideQueryModal());
        cancelBtn?.addEventListener('click', () => this.hideQueryModal());
        executeBtn?.addEventListener('click', () => this.executeQuery());
        modal?.addEventListener('click', (e) => {
            if (e.target === modal) {
                this.hideQueryModal();
            }
        });
        const retryBtn = document.getElementById('retry-btn');
        retryBtn?.addEventListener('click', () => this.loadGraphData());
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape') {
                this.hideQueryModal();
            }
        });
    }
    async loadGraphData() {
        this.showLoading(true);
        this.hideError();
        try {
            const response = await fetch('/api/graph');
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${response.statusText}`);
            }
            const data = await response.json();
            this.performanceManager.optimizeForNodeCount(data.nodes.length);
            this.graph.updateData(data);
            this.updateStatistics(data.metadata);
            setTimeout(() => {
                if (!this.isFrozen && this.freezeBtn) {
                    this.graph.freezeLayout();
                    this.freezeBtn.textContent = 'Unfreeze Layout';
                    this.freezeBtn.classList.remove('btn-secondary');
                    this.freezeBtn.classList.add('btn-primary');
                    this.isFrozen = true;
                    this.updateStatus('Layout frozen - drag nodes to reposition');
                }
            }, 5000);
            this.setupNodeTypeFilters(data.metadata.node_types);
            setTimeout(() => this.graph.centerGraph(), 1000);
            this.updateStatus('Graph loaded successfully');
        } catch (error) {
            console.error('Failed to load graph data:', error);
            this.showError(`Failed to load graph data: ${error.message}`);
        } finally {
            this.showLoading(false);
        }
    }
    async updateGraph() {
        const updateBtn = document.getElementById('update-btn');
        const originalText = updateBtn.textContent;
        try {
            updateBtn.textContent = 'Updating...';
            updateBtn.disabled = true;
            this.updateStatus('Updating graph...');
            const response = await fetch('/api/update', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({})
            });
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${response.statusText}`);
            }
            const result = await response.json();
            if (result.success) {
                this.updateStatus('Graph updated successfully');
                await this.loadGraphData();
            } else {
                throw new Error(result.message || 'Update failed');
            }
        } catch (error) {
            console.error('Failed to update graph:', error);
            this.updateStatus(`Update failed: ${error.message}`);
        } finally {
            updateBtn.textContent = originalText;
            updateBtn.disabled = false;
        }
    }
    resetView() {
        this.graph.resetZoom();
        setTimeout(() => this.graph.centerGraph(), 100);
        this.updateStatus('View reset');
    }
    async loadQueryTemplates() {
        try {
            const response = await fetch('/api/templates');
            if (!response.ok) return;
            const data = await response.json();
            this.queryTemplates = data.templates;
            this.renderQueryTemplates();
        } catch (error) {
            console.error('Failed to load query templates:', error);
        }
    }
    renderQueryTemplates() {
        const container = document.getElementById('query-templates');
        if (!container) return;
        container.innerHTML = '';
        this.queryTemplates.forEach(template => {
            const templateElement = document.createElement('div');
            templateElement.className = 'template-item';
            templateElement.innerHTML = `
                <h4>${template.name}</h4>
                <p>${template.description}</p>
            `;
            templateElement.addEventListener('click', () => {
                this.showQueryModal(template);
            });
            container.appendChild(templateElement);
        });
    }
    setupNodeTypeFilters(nodeTypes) {
        const container = document.getElementById('node-type-filters');
        if (!container) return;
        container.innerHTML = '';
        nodeTypes.forEach(nodeType => {
            const filterItem = document.createElement('div');
            filterItem.className = 'checkbox-item';
            filterItem.innerHTML = `
                <input type="checkbox" id="filter-${nodeType.toLowerCase()}" value="${nodeType}" checked>
                <label for="filter-${nodeType.toLowerCase()}">${nodeType}</label>
            `;
            const checkbox = filterItem.querySelector('input');
            checkbox.addEventListener('change', () => {
                this.updateNodeTypeFilters();
                this.applyFilters();
            });
            container.appendChild(filterItem);
        });
    }
    updateNodeTypeFilters() {
        const checkboxes = document.querySelectorAll('#node-type-filters input[type="checkbox"]');
        this.currentFilters.nodeTypes = Array.from(checkboxes)
            .filter(cb => cb.checked)
            .map(cb => cb.value);
    }
    applyFilters() {
        this.graph.filterNodes(this.currentFilters);
        const stats = this.graph.getStats();
        document.getElementById('node-count').textContent = stats.nodeCount;
        document.getElementById('edge-count').textContent = stats.edgeCount;
    }
    updateStatistics(metadata) {
        const nodeCount = document.getElementById('node-count');
        const edgeCount = document.getElementById('edge-count');
        if (nodeCount) nodeCount.textContent = metadata.total_nodes;
        if (edgeCount) edgeCount.textContent = metadata.total_edges;
    }
    handleNodeSelected(node) {
        const detailsContainer = document.getElementById('node-details');
        if (!detailsContainer) return;
        detailsContainer.innerHTML = `
            <div class="detail-item">
                <div class="detail-label">Name:</div>
                <div class="detail-value">${node.name}</div>
            </div>
            <div class="detail-item">
                <div class="detail-label">Type:</div>
                <div class="detail-value">${node.type}</div>
            </div>
            ${node.file_path ? `
                <div class="detail-item">
                    <div class="detail-label">File:</div>
                    <div class="detail-value">${node.file_path}</div>
                </div>
            ` : ''}
            ${node.line_start ? `
                <div class="detail-item">
                    <div class="detail-label">Lines:</div>
                    <div class="detail-value">${node.line_start}${node.line_end ? `-${node.line_end}` : ''}</div>
                </div>
            ` : ''}
            ${Object.keys(node.metadata).length > 0 ? `
                <div class="detail-item">
                    <div class="detail-label">Metadata:</div>
                    <div class="detail-value font-mono">${JSON.stringify(node.metadata, null, 2)}</div>
                </div>
            ` : ''}
        `;
    }
    handleNodeDeselected() {
        const detailsContainer = document.getElementById('node-details');
        if (detailsContainer) {
            detailsContainer.innerHTML = '<p>Click a node to see details</p>';
        }
    }
    showQueryModal(template) {
        const modal = document.getElementById('query-modal');
        const queryInput = document.getElementById('query-input');
        const paramsInput = document.getElementById('query-params');
        if (!modal || !queryInput || !paramsInput) return;
        queryInput.value = template.query;
        paramsInput.value = JSON.stringify(template.example_params, null, 2);
        modal.style.display = 'flex';
    }
    hideQueryModal() {
        const modal = document.getElementById('query-modal');
        if (modal) {
            modal.style.display = 'none';
        }
    }
    async executeQuery() {
        const queryInput = document.getElementById('query-input');
        const paramsInput = document.getElementById('query-params');
        const executeBtn = document.getElementById('execute-query-btn');
        if (!queryInput || !executeBtn) return;
        const query = queryInput.value.trim();
        if (!query) return;
        let parameters = {};
        try {
            if (paramsInput.value.trim()) {
                parameters = JSON.parse(paramsInput.value);
            }
        } catch (error) {
            alert('Invalid JSON parameters');
            return;
        }
        const originalText = executeBtn.textContent;
        try {
            executeBtn.textContent = 'Executing...';
            executeBtn.disabled = true;
            const response = await fetch('/api/query', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    query: query,
                    parameters: parameters
                })
            });
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${response.statusText}`);
            }
            const result = await response.json();
            console.log('Query Results:', result);
            alert(`Query executed successfully! ${result.record_count} records returned. Check console for details.`);
            this.hideQueryModal();
        } catch (error) {
            console.error('Query execution failed:', error);
            alert(`Query failed: ${error.message}`);
        } finally {
            executeBtn.textContent = originalText;
            executeBtn.disabled = false;
        }
    }
    showLoading(show) {
        const loading = document.getElementById('loading');
        if (loading) {
            loading.style.display = show ? 'flex' : 'none';
        }
    }
    showError(message) {
        const error = document.getElementById('error');
        const errorMessage = document.getElementById('error-message');
        if (error && errorMessage) {
            errorMessage.textContent = message;
            error.style.display = 'flex';
        }
    }
    hideError() {
        const error = document.getElementById('error');
        if (error) {
            error.style.display = 'none';
        }
    }
    updateStatus(message) {
        const statusInfo = document.getElementById('status-info');
        if (statusInfo) {
            statusInfo.textContent = message;
        }
    }
    debounce(func, delay) {
        if (this.debounceTimer) {
            clearTimeout(this.debounceTimer);
        }
        this.debounceTimer = setTimeout(func, delay);
    }
    handlePerformanceUpdate(stats) {
        const statusControls = document.querySelector('.status-controls');
        if (statusControls) {
            let perfIndicator = document.getElementById('perf-indicator');
            if (!perfIndicator) {
                perfIndicator = document.createElement('span');
                perfIndicator.id = 'perf-indicator';
                statusControls.appendChild(perfIndicator);
            }
            perfIndicator.textContent = `${stats.fps} FPS`;
            if (stats.fps >= 55) {
                perfIndicator.style.color = 'var(--success-color)';
            } else if (stats.fps >= 30) {
                perfIndicator.style.color = 'var(--warning-color)';
            } else {
                perfIndicator.style.color = 'var(--error-color)';
            }
        }
    }
}
document.addEventListener('DOMContentLoaded', () => {
    window.codeBasedApp = new CodeBasedApp();
});
</file>

<file path="web/graph.js">
class GraphVisualizer {
    constructor(containerId) {
        this.containerId = containerId;
        this.svg = d3.select(containerId);
        this.width = 0;
        this.height = 0;
        this.nodes = [];
        this.edges = [];
        this.filteredNodes = [];
        this.filteredEdges = [];
        this.simulation = null;
        this.selectedNode = null;
        this.isFrozen = false;
        this.g = null;
        this.nodeGroup = null;
        this.edgeGroup = null;
        this.zoom = null;
        this.currentZoom = 1;
        this.nodeColors = {
            'File': '#8b5cf6',
            'Module': '#06b6d4',
            'Class': '#10b981',
            'Function': '#f59e0b',
            'Variable': '#ef4444',
            'Import': '#6b7280'
        };
        this.nodeSizes = {
            'File': 20,
            'Module': 18,
            'Class': 16,
            'Function': 14,
            'Variable': 12,
            'Import': 10
        };
        this.init();
        this.createLegend();
    }
    init() {
        const container = d3.select(this.containerId).node();
        this.width = container.clientWidth;
        this.height = container.clientHeight;
        this.svg
            .attr('width', this.width)
            .attr('height', this.height);
        this.g = this.svg.append('g')
            .attr('class', 'graph-group');
        this.edgeGroup = this.g.append('g')
            .attr('class', 'edges');
        this.edgeLabelGroup = this.g.append('g')
            .attr('class', 'edge-labels');
        this.nodeGroup = this.g.append('g')
            .attr('class', 'nodes');
        this.tooltip = d3.select('body').append('div')
            .attr('class', 'graph-tooltip')
            .style('opacity', 0)
            .style('position', 'absolute')
            .style('pointer-events', 'none');
        this.zoom = d3.zoom()
            .scaleExtent([0.1, 10])
            .on('zoom', (event) => {
                this.g.attr('transform', event.transform);
                this.currentZoom = event.transform.k;
                this.updateZoomDisplay();
            });
        this.svg.call(this.zoom);
        this.simulation = d3.forceSimulation()
            .force('link', d3.forceLink().id(d => d.id).distance(120))
            .force('charge', d3.forceManyBody().strength(-500))
            .force('center', d3.forceCenter(this.width / 2, this.height / 2))
            .force('collision', d3.forceCollide().radius(d => this.getNodeSize(d) + 5))
            .alphaDecay(0.05);
        window.addEventListener('resize', () => this.handleResize());
    }
    updateData(graphData) {
        this.nodes = graphData.nodes || [];
        this.edges = graphData.edges || [];
        this.filteredNodes = [...this.nodes];
        this.filteredEdges = [...this.edges];
        this.simulation.nodes(this.nodes);
        this.simulation.force('link').links(this.edges);
        this.render();
        this.restart();
    }
    render() {
        this.renderEdges();
        this.renderNodes();
    }
    renderEdges() {
        const edges = this.edgeGroup
            .selectAll('.edge')
            .data(this.edges, d => `${d.source.id || d.source}-${d.target.id || d.target}-${d.relationship_type}`);
        const edgesEnter = edges.enter()
            .append('line')
            .attr('class', d => `edge ${d.relationship_type.toLowerCase()}`)
            .attr('stroke', '#64748b')
            .attr('stroke-width', 2)
            .attr('opacity', 0.5);
        const allEdges = edgesEnter.merge(edges);
        const visibleNodeIds = new Set(this.filteredNodes.map(n => n.id));
        allEdges.style('opacity', d => {
            const sourceVisible = visibleNodeIds.has(d.source.id || d.source);
            const targetVisible = visibleNodeIds.has(d.target.id || d.target);
            return (sourceVisible && targetVisible) ? 0.5 : 0;
        });
        edgesEnter.on('mouseenter', (event, d) => {
            const edgeId = `${d.source.id || d.source}-${d.target.id || d.target}-${d.relationship_type}`;
            this.edgeLabelElements
                .filter(function(label) {
                    const labelId = `${label.source.id || label.source}-${label.target.id || label.target}-${label.relationship_type}`;
                    return labelId === edgeId;
                })
                .transition()
                .duration(200)
                .style('opacity', 1);
        }).on('mouseleave', (event, d) => {
            const edgeId = `${d.source.id || d.source}-${d.target.id || d.target}-${d.relationship_type}`;
            this.edgeLabelElements
                .filter(function(label) {
                    const labelId = `${label.source.id || label.source}-${label.target.id || label.target}-${label.relationship_type}`;
                    return labelId === edgeId;
                })
                .transition()
                .duration(200)
                .style('opacity', 0);
        });
        const edgeLabels = this.edgeLabelGroup
            .selectAll('.edge-label-group')
            .data(this.edges, d => `${d.source.id || d.source}-${d.target.id || d.target}-${d.relationship_type}`);
        const edgeLabelsEnter = edgeLabels.enter()
            .append('g')
            .attr('class', 'edge-label-group')
            .style('opacity', 0)
            .style('pointer-events', 'none');
        edgeLabelsEnter.append('rect')
            .attr('class', 'edge-label-bg')
            .attr('fill', 'white')
            .attr('stroke', '#e2e8f0')
            .attr('stroke-width', 1)
            .attr('rx', 3);
        edgeLabelsEnter.append('text')
            .attr('class', 'edge-label')
            .attr('font-size', '11px')
            .attr('font-weight', '500')
            .attr('fill', '#475569')
            .attr('text-anchor', 'middle')
            .attr('dy', '0.35em')
            .text(d => this.formatRelationshipType(d.relationship_type));
        edgeLabelsEnter.each(function() {
            const text = d3.select(this).select('text');
            const bbox = text.node().getBBox();
            d3.select(this).select('rect')
                .attr('x', bbox.x - 4)
                .attr('y', bbox.y - 2)
                .attr('width', bbox.width + 8)
                .attr('height', bbox.height + 4);
        });
        this.edgeElements = allEdges;
        this.edgeElements.on('mouseenter', (event, d) => {
            const edgeId = `${d.source.id || d.source}-${d.target.id || d.target}-${d.relationship_type}`;
            this.edgeLabelElements
                .filter(function(label) {
                    const labelId = `${label.source.id || label.source}-${label.target.id || label.target}-${label.relationship_type}`;
                    return labelId === edgeId;
                })
                .transition()
                .duration(200)
                .style('opacity', 1);
        }).on('mouseleave', (event, d) => {
            const edgeId = `${d.source.id || d.source}-${d.target.id || d.target}-${d.relationship_type}`;
            this.edgeLabelElements
                .filter(function(label) {
                    const labelId = `${label.source.id || label.source}-${label.target.id || label.target}-${label.relationship_type}`;
                    return labelId === edgeId;
                })
                .transition()
                .duration(200)
                .style('opacity', 0);
        });
        this.edgeLabelElements = edgeLabelsEnter.merge(edgeLabels);
    }
    renderNodes() {
        const nodeSelection = this.nodeGroup
            .selectAll('.node-group')
            .data(this.nodes, d => d.id);
        const nodeGroupsEnter = nodeSelection.enter()
            .append('g')
            .attr('class', 'node-group')
            .call(this.dragBehavior());
        nodeGroupsEnter.append('circle')
            .attr('class', d => `node ${d.type.toLowerCase()}`)
            .attr('r', d => this.getNodeSize(d))
            .attr('fill', d => this.getNodeColor(d))
            .attr('stroke', '#ffffff')
            .attr('stroke-width', 2);
        nodeGroupsEnter.append('text')
            .attr('class', 'node-label')
            .attr('dy', d => this.getNodeSize(d) + 12)
            .attr('text-anchor', 'middle')
            .attr('font-size', '12px')
            .attr('fill', '#1e293b')
            .text(d => this.getNodeLabel(d));
        const allNodes = nodeGroupsEnter.merge(nodeSelection);
        allNodes.on('click', (event, d) => {
            event.stopPropagation();
            this.selectNode(d);
        });
        allNodes.on('mouseenter', (event, d) => {
            this.highlightNode(d, true);
            this.showTooltip(event, d);
        }).on('mouseleave', (event, d) => {
            this.highlightNode(d, false);
            this.hideTooltip();
        });
        this.nodeElements = allNodes;
    }
    dragBehavior() {
        return d3.drag()
            .on('start', (event, d) => {
                d.fx = d.x;
                d.fy = d.y;
            })
            .on('drag', (event, d) => {
                d.fx = event.x;
                d.fy = event.y;
                this.updateNodePosition(d);
            })
            .on('end', (event, d) => {
            });
    }
    restart() {
        this.simulation.on('tick', () => {
            this.edgeElements
                .attr('x1', d => d.source.x)
                .attr('y1', d => d.source.y)
                .attr('x2', d => d.target.x)
                .attr('y2', d => d.target.y);
            this.edgeLabelElements
                .attr('transform', d => {
                    const x = (d.source.x + d.target.x) / 2;
                    const y = (d.source.y + d.target.y) / 2;
                    return `translate(${x}, ${y})`;
                });
            this.nodeElements
                .attr('transform', d => `translate(${d.x},${d.y})`);
        });
        this.simulation.alpha(1).restart();
        setTimeout(() => {
            this.simulation.stop();
            this.nodes.forEach(node => {
                node.fx = node.x;
                node.fy = node.y;
            });
            this.isFrozen = true;
        }, 5000);
    }
    selectNode(node) {
        this.nodeElements.selectAll('.node').classed('selected', false);
        if (this.selectedNode !== node) {
            this.selectedNode = node;
            this.nodeElements
                .filter(d => d.id === node.id)
                .selectAll('.node')
                .classed('selected', true);
            this.onNodeSelected(node);
        } else {
            this.selectedNode = null;
            this.onNodeDeselected();
        }
    }
    highlightNode(node, highlight) {
        const nodeElement = this.nodeElements.filter(d => d.id === node.id);
        if (highlight) {
            nodeElement.selectAll('.node')
                .transition()
                .duration(200)
                .attr('transform', 'scale(1.2)');
            this.edgeElements
                .filter(d => d.source.id === node.id || d.target.id === node.id)
                .transition()
                .duration(200)
                .attr('opacity', 1)
                .attr('stroke-width', 3);
        } else {
            nodeElement.selectAll('.node')
                .transition()
                .duration(200)
                .attr('transform', 'scale(1)');
            this.edgeElements
                .transition()
                .duration(200)
                .attr('opacity', 0.5)
                .attr('stroke-width', 2);
        }
    }
    getNodeColor(node) {
        return this.nodeColors[node.type] || '#6b7280';
    }
    getNodeSize(node) {
        return this.nodeSizes[node.type] || 6;
    }
    getNodeLabel(node) {
        if (node.name.length > 15) {
            return node.name.substring(0, 12) + '...';
        }
        return node.name;
    }
    centerGraph() {
        if (this.nodes.length === 0) return;
        const bounds = this.getGraphBounds();
        const scale = 0.9 * Math.min(
            this.width / (bounds.maxX - bounds.minX),
            this.height / (bounds.maxY - bounds.minY)
        );
        const translate = [
            this.width / 2 - scale * (bounds.minX + bounds.maxX) / 2,
            this.height / 2 - scale * (bounds.minY + bounds.maxY) / 2
        ];
        this.svg.transition()
            .duration(750)
            .call(this.zoom.transform, d3.zoomIdentity.translate(translate[0], translate[1]).scale(scale));
    }
    getGraphBounds() {
        let minX = Infinity, maxX = -Infinity;
        let minY = Infinity, maxY = -Infinity;
        this.nodes.forEach(node => {
            if (node.x < minX) minX = node.x;
            if (node.x > maxX) maxX = node.x;
            if (node.y < minY) minY = node.y;
            if (node.y > maxY) maxY = node.y;
        });
        return { minX, maxX, minY, maxY };
    }
    resetZoom() {
        this.svg.transition()
            .duration(750)
            .call(this.zoom.transform, d3.zoomIdentity);
    }
    updateZoomDisplay() {
        const zoomPercent = Math.round(this.currentZoom * 100);
        const zoomDisplay = document.getElementById('zoom-level');
        if (zoomDisplay) {
            zoomDisplay.textContent = `${zoomPercent}%`;
        }
    }
    filterNodes(filters) {
        const { nodeTypes, searchText, fileFilter } = filters;
        let filteredNodes = this.nodes;
        let filteredEdges = this.edges;
        if (nodeTypes && nodeTypes.length > 0) {
            const lowerCaseNodeTypes = nodeTypes.map(t => String(t).toLowerCase());
            filteredNodes = filteredNodes.filter(node =>
                node && node.type && lowerCaseNodeTypes.includes(String(node.type).toLowerCase())
            );
        }
        if (searchText) {
            const searchLower = searchText.toLowerCase();
            filteredNodes = filteredNodes.filter(node =>
                node.name.toLowerCase().includes(searchLower) ||
                (node.file_path && node.file_path.toLowerCase().includes(searchLower))
            );
        }
        if (fileFilter) {
            const filterLower = fileFilter.toLowerCase();
            filteredNodes = filteredNodes.filter(node =>
                node.file_path && node.file_path.toLowerCase().includes(filterLower)
            );
        }
        const visibleNodeIds = new Set(filteredNodes.map(n => n.id));
        filteredEdges = filteredEdges.filter(edge =>
            visibleNodeIds.has(edge.source.id || edge.source) &&
            visibleNodeIds.has(edge.target.id || edge.target)
        );
        this.updateFilteredData(filteredNodes, filteredEdges);
    }
    updateFilteredData(nodes, edges) {
        this.filteredNodes = nodes;
        this.filteredEdges = edges;
        this.applyFilterVisibility();
    }
    applyFilterVisibility() {
        const visibleNodeIds = new Set(this.filteredNodes.map(n => n.id));
        if (this.nodeElements) {
            this.nodeElements
                .style('opacity', d => visibleNodeIds.has(d.id) ? 1 : 0)
                .style('pointer-events', d => visibleNodeIds.has(d.id) ? 'all' : 'none');
        }
        if (this.edgeElements) {
            this.edgeElements
                .style('opacity', d => {
                    const sourceVisible = visibleNodeIds.has(d.source.id || d.source);
                    const targetVisible = visibleNodeIds.has(d.target.id || d.target);
                    return (sourceVisible && targetVisible) ? 0.8 : 0;
                })
                .style('pointer-events', d => {
                    const sourceVisible = visibleNodeIds.has(d.source.id || d.source);
                    const targetVisible = visibleNodeIds.has(d.target.id || d.target);
                    return (sourceVisible && targetVisible) ? 'all' : 'none';
                });
        }
    }
    handleResize() {
        const container = d3.select(this.containerId).node();
        this.width = container.clientWidth;
        this.height = container.clientHeight;
        this.svg
            .attr('width', this.width)
            .attr('height', this.height);
        this.simulation.force('center', d3.forceCenter(this.width / 2, this.height / 2));
        this.simulation.alpha(0.3).restart();
    }
    onNodeSelected(node) {
    }
    onNodeDeselected() {
    }
    clear() {
        this.nodes = [];
        this.edges = [];
        this.filteredNodes = [];
        this.filteredEdges = [];
        this.selectedNode = null;
        this.nodeGroup.selectAll('*').remove();
        this.edgeGroup.selectAll('*').remove();
        this.edgeLabelGroup.selectAll('*').remove();
        this.simulation.nodes([]);
        this.simulation.force('link').links([]);
    }
    getSelectedNode() {
        return this.selectedNode;
    }
    getStats() {
        return {
            nodeCount: this.filteredNodes.length,
            edgeCount: this.filteredEdges.length,
            nodeTypes: [...new Set(this.filteredNodes.map(n => n.type))]
        };
    }
    createLegend() {
        const legendContainer = d3.select('#legend-items');
        legendContainer.selectAll('*').remove();
        const nodeTypes = Object.keys(this.nodeColors);
        nodeTypes.forEach(type => {
            const item = legendContainer.append('div')
                .attr('class', 'legend-item');
            item.append('div')
                .attr('class', 'legend-color')
                .style('background-color', this.nodeColors[type]);
            item.append('span')
                .text(type);
        });
    }
    formatRelationshipType(type) {
        const typeMap = {
            'FILE_CONTAINS_MODULE': 'contains',
            'FILE_CONTAINS_CLASS': 'contains',
            'FILE_CONTAINS_FUNCTION': 'contains',
            'FILE_CONTAINS_VARIABLE': 'contains',
            'FILE_CONTAINS_IMPORT': 'contains',
            'MODULE_CONTAINS_CLASS': 'contains',
            'MODULE_CONTAINS_FUNCTION': 'contains',
            'MODULE_CONTAINS_VARIABLE': 'contains',
            'CLASS_CONTAINS_FUNCTION': 'contains',
            'CLASS_CONTAINS_VARIABLE': 'contains',
            'FUNCTION_CONTAINS_VARIABLE': 'contains',
            'FUNCTION_CONTAINS_FUNCTION': 'contains',
            'CLASS_CONTAINS_CLASS': 'contains',
            'CALLS': 'calls',
            'IMPORTS': 'imports',
            'INHERITS': 'inherits',
            'USES': 'uses',
            'DECORATES': 'decorates'
        };
        return typeMap[type] || type.toLowerCase();
    }
    updateNodePosition(node) {
        this.nodeElements
            .filter(d => d.id === node.id)
            .attr('transform', d => `translate(${d.fx},${d.fy})`);
        this.edgeElements
            .filter(d => d.source.id === node.id || d.target.id === node.id)
            .attr('x1', d => d.source.id === node.id ? node.fx : d.source.x)
            .attr('y1', d => d.source.id === node.id ? node.fy : d.source.y)
            .attr('x2', d => d.target.id === node.id ? node.fx : d.target.x)
            .attr('y2', d => d.target.id === node.id ? node.fy : d.target.y);
        this.edgeLabelElements
            .filter(d => d.source.id === node.id || d.target.id === node.id)
            .attr('transform', d => {
                const x1 = d.source.id === node.id ? node.fx : d.source.x;
                const y1 = d.source.id === node.id ? node.fy : d.source.y;
                const x2 = d.target.id === node.id ? node.fx : d.target.x;
                const y2 = d.target.id === node.id ? node.fy : d.target.y;
                return `translate(${(x1 + x2) / 2}, ${(y1 + y2) / 2})`;
            });
    }
    freezeLayout() {
        this.simulation.stop();
        this.nodes.forEach(node => {
            node.fx = node.x;
            node.fy = node.y;
        });
        this.isFrozen = true;
    }
    unfreezeLayout() {
        this.nodes.forEach(node => {
            node.fx = null;
            node.fy = null;
        });
        this.isFrozen = false;
        this.simulation.alpha(0.3).restart();
    }
    showTooltip(event, node) {
        const content = `
            <div class="tooltip-header">${node.type}: ${node.name}</div>
            <div class="tooltip-body">
                ${node.file_path ? `<div class="tooltip-item"><span class="tooltip-label">File:</span> ${node.file_path}</div>` : ''}
                ${node.line_start ? `<div class="tooltip-item"><span class="tooltip-label">Lines:</span> ${node.line_start}${node.line_end && node.line_end !== node.line_start ? `-${node.line_end}` : ''}</div>` : ''}
                ${node.metadata && Object.keys(node.metadata).length > 0 ? `
                    <div class="tooltip-section">Details:</div>
                    ${Object.entries(node.metadata)
                        .filter(([key, value]) => value !== null && value !== undefined && key !== 'file_id')
                        .map(([key, value]) => `<div class="tooltip-item"><span class="tooltip-label">${this.formatMetadataKey(key)}:</span> ${this.formatMetadataValue(value)}</div>`)
                        .join('')}
                ` : ''}
            </div>
        `;
        this.tooltip
            .html(content)
            .transition()
            .duration(200)
            .style('opacity', 1);
        const tooltipNode = this.tooltip.node();
        const bbox = tooltipNode.getBoundingClientRect();
        const x = event.pageX + 10;
        const y = event.pageY - bbox.height / 2;
        const finalX = Math.min(x, window.innerWidth - bbox.width - 10);
        const finalY = Math.max(10, Math.min(y, window.innerHeight - bbox.height - 10));
        this.tooltip
            .style('left', finalX + 'px')
            .style('top', finalY + 'px');
    }
    hideTooltip() {
        this.tooltip
            .transition()
            .duration(200)
            .style('opacity', 0);
    }
    formatMetadataKey(key) {
        return key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());
    }
    formatMetadataValue(value) {
        if (typeof value === 'boolean') {
            return value ? 'Yes' : 'No';
        }
        if (typeof value === 'string' && value.length > 50) {
            return value.substring(0, 50) + '...';
        }
        return value;
    }
}
</file>

<file path="web/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CodeBased - Code Graph Visualization</title>
    <link rel="stylesheet" href="style.css">
    <script src="d3.v7.min.js"></script>
</head>
<body>
    <div id="app">
        <header class="header">
            <h1>CodeBased</h1>
            <div class="header-controls">
                <button id="update-btn" class="btn btn-primary">Update Graph</button>
                <button id="freeze-btn" class="btn btn-secondary">Freeze Layout</button>
                <button id="reset-btn" class="btn btn-secondary">Reset View</button>
            </div>
        </header>
        <div class="main-content">
            <aside class="sidebar">
                <div class="sidebar-section">
                    <h3>Filters</h3>
                    <div class="filter-group">
                        <label>Node Types:</label>
                        <div id="node-type-filters" class="checkbox-group">
                        </div>
                    </div>
                    <div class="filter-group">
                        <label for="file-filter">File Filter:</label>
                        <input type="text" id="file-filter" placeholder="Filter by file path...">
                    </div>
                    <div class="filter-group">
                        <label for="search-input">Search:</label>
                        <input type="text" id="search-input" placeholder="Search nodes...">
                    </div>
                </div>
                <div class="sidebar-section">
                    <h3>Quick Queries</h3>
                    <div id="query-templates" class="template-list">
                    </div>
                </div>
                <div class="sidebar-section">
                    <h3>Statistics</h3>
                    <div id="graph-stats">
                        <div class="stat-item">
                            <span class="stat-label">Nodes:</span>
                            <span id="node-count">0</span>
                        </div>
                        <div class="stat-item">
                            <span class="stat-label">Edges:</span>
                            <span id="edge-count">0</span>
                        </div>
                    </div>
                </div>
                <div class="sidebar-section">
                    <h3>Selected Node</h3>
                    <div id="node-details" class="node-details">
                        <p>Click a node to see details</p>
                    </div>
                </div>
            </aside>
            <main class="graph-container">
                <div id="graph-svg-container">
                    <svg id="graph-svg"></svg>
                    <div id="graph-legend" class="graph-legend">
                        <h4>Node Types</h4>
                        <div id="legend-items" class="legend-items">
                        </div>
                    </div>
                </div>
                <div id="loading" class="loading-overlay">
                    <div class="loading-spinner"></div>
                    <p>Loading graph data...</p>
                </div>
                <div id="error" class="error-overlay" style="display: none;">
                    <div class="error-content">
                        <h3>Error</h3>
                        <p id="error-message"></p>
                        <button id="retry-btn" class="btn btn-primary">Retry</button>
                    </div>
                </div>
            </main>
        </div>
        <footer class="status-bar">
            <div id="status-info">Ready</div>
            <div class="status-controls">
                <span id="zoom-level">100%</span>
            </div>
        </footer>
    </div>
    <div id="query-modal" class="modal" style="display: none;">
        <div class="modal-content">
            <div class="modal-header">
                <h3>Execute Query</h3>
                <span class="close" id="close-modal">&times;</span>
            </div>
            <div class="modal-body">
                <div class="query-section">
                    <label for="query-input">Cypher Query:</label>
                    <textarea id="query-input" rows="8" placeholder="Enter your Cypher query..."></textarea>
                </div>
                <div class="query-section">
                    <label for="query-params">Parameters (JSON):</label>
                    <textarea id="query-params" rows="4" placeholder='{"param1": "value1"}'></textarea>
                </div>
            </div>
            <div class="modal-footer">
                <button id="execute-query-btn" class="btn btn-primary">Execute</button>
                <button id="cancel-query-btn" class="btn btn-secondary">Cancel</button>
            </div>
        </div>
    </div>
    <script src="graph.js"></script>
    <script src="performance.js"></script>
    <script src="app.js"></script>
</body>
</html>
</file>

<file path="web/performance.js">
class PerformanceManager {
    constructor(graphVisualizer) {
        this.graph = graphVisualizer;
        this.isWebGLAvailable = this.checkWebGLSupport();
        this.useLevelOfDetail = true;
        this.useViewportCulling = true;
        this.animationFrameId = null;
        this.thresholds = {
            webgl: 2000,
            lod: 500,
            culling: 1000,
            lazy: 5000
        };
        this.visibleNodes = new Set();
        this.visibleEdges = new Set();
        this.nodeQuadtree = null;
        this.init();
    }
    init() {
        if (this.useViewportCulling) {
            this.setupViewportCulling();
        }
        this.setupPerformanceMonitoring();
    }
    checkWebGLSupport() {
        try {
            const canvas = document.createElement('canvas');
            return !!(canvas.getContext('webgl') || canvas.getContext('experimental-webgl'));
        } catch (e) {
            return false;
        }
    }
    shouldUseWebGL(nodeCount) {
        return this.isWebGLAvailable && nodeCount > this.thresholds.webgl;
    }
    shouldUseLevelOfDetail(nodeCount) {
        return this.useLevelOfDetail && nodeCount > this.thresholds.lod;
    }
    shouldUseViewportCulling(nodeCount) {
        return this.useViewportCulling && nodeCount > this.thresholds.culling;
    }
    shouldUseLazyLoading(nodeCount) {
        return nodeCount > this.thresholds.lazy;
    }
    optimizeForNodeCount(nodeCount) {
        console.log(`Optimizing for ${nodeCount} nodes`);
        const optimizations = {
            webgl: this.shouldUseWebGL(nodeCount),
            levelOfDetail: this.shouldUseLevelOfDetail(nodeCount),
            viewportCulling: this.shouldUseViewportCulling(nodeCount),
            lazyLoading: this.shouldUseLazyLoading(nodeCount)
        };
        if (optimizations.levelOfDetail) {
            this.enableLevelOfDetail();
        }
        if (optimizations.viewportCulling) {
            this.enableViewportCulling();
        }
        if (optimizations.lazyLoading) {
            this.enableLazyLoading();
        }
        if (optimizations.webgl) {
            this.enableWebGLRenderer();
        }
        console.log('Applied optimizations:', optimizations);
        return optimizations;
    }
    enableLevelOfDetail() {
        const originalRender = this.graph.render.bind(this.graph);
        this.graph.render = () => {
            const zoom = this.graph.currentZoom || 1;
            if (zoom < 0.3) {
                this.renderLowDetail();
            } else if (zoom < 0.7) {
                this.renderMediumDetail();
            } else {
                originalRender();
            }
        };
    }
    renderLowDetail() {
        const majorTypes = ['File', 'Class'];
        this.graph.nodeElements
            .style('opacity', d => majorTypes.includes(d.type) ? 1 : 0.1);
        this.graph.nodeElements.selectAll('.node-label')
            .style('opacity', 0);
        this.graph.edgeElements
            .style('opacity', 0.2)
            .style('stroke-width', 1);
    }
    renderMediumDetail() {
        this.graph.nodeElements
            .style('opacity', d => d.type === 'Variable' ? 0.3 : 1);
        this.graph.nodeElements.selectAll('.node-label')
            .style('opacity', d => ['File', 'Class', 'Function'].includes(d.type) ? 1 : 0);
        this.graph.edgeElements
            .style('opacity', 0.6)
            .style('stroke-width', 1.5);
    }
    enableViewportCulling() {
        if (this.animationFrameId) {
            cancelAnimationFrame(this.animationFrameId);
        }
        const performCulling = () => {
            const bounds = this.getViewportBounds();
            this.updateVisibleElements(bounds);
            this.animationFrameId = requestAnimationFrame(performCulling);
        };
        performCulling();
    }
    getViewportBounds() {
        const svg = this.graph.svg.node();
        const transform = d3.zoomTransform(svg);
        return {
            left: -transform.x / transform.k,
            top: -transform.y / transform.k,
            right: (this.graph.width - transform.x) / transform.k,
            bottom: (this.graph.height - transform.y) / transform.k
        };
    }
    updateVisibleElements(bounds) {
        this.graph.nodeElements.each((d, i, nodes) => {
            const isVisible = (
                d.x >= bounds.left && d.x <= bounds.right &&
                d.y >= bounds.top && d.y <= bounds.bottom
            );
            const element = d3.select(nodes[i]);
            if (isVisible) {
                this.visibleNodes.add(d.id);
                element.style('display', 'block');
            } else {
                this.visibleNodes.delete(d.id);
                element.style('display', 'none');
            }
        });
        this.graph.edgeElements.each((d, i, edges) => {
            const sourceVisible = this.visibleNodes.has(d.source.id);
            const targetVisible = this.visibleNodes.has(d.target.id);
            const isVisible = sourceVisible || targetVisible;
            const element = d3.select(edges[i]);
            if (isVisible) {
                this.visibleEdges.add(d);
                element.style('display', 'block');
            } else {
                this.visibleEdges.delete(d);
                element.style('display', 'none');
            }
        });
    }
    enableLazyLoading() {
        const originalUpdateData = this.graph.updateData.bind(this.graph);
        this.graph.updateData = (graphData) => {
            if (graphData.nodes.length > this.thresholds.lazy) {
                this.loadDataInChunks(graphData);
            } else {
                originalUpdateData(graphData);
            }
        };
    }
    loadDataInChunks(graphData) {
        const chunkSize = 1000;
        const nodeChunks = this.chunkArray(graphData.nodes, chunkSize);
        const edgeChunks = this.chunkArray(graphData.edges, chunkSize);
        let currentChunk = 0;
        const totalChunks = Math.max(nodeChunks.length, edgeChunks.length);
        const loadNextChunk = () => {
            if (currentChunk < totalChunks) {
                const nodes = nodeChunks[currentChunk] || [];
                const edges = edgeChunks[currentChunk] || [];
                this.graph.nodes = [...this.graph.nodes, ...nodes];
                this.graph.edges = [...this.graph.edges, ...edges];
                this.graph.render();
                this.graph.restart();
                currentChunk++;
                setTimeout(loadNextChunk, 50);
                const progress = Math.round((currentChunk / totalChunks) * 100);
                this.updateLoadingProgress(progress);
            } else {
                this.hideLoadingProgress();
            }
        };
        this.showLoadingProgress();
        loadNextChunk();
    }
    chunkArray(array, chunkSize) {
        const chunks = [];
        for (let i = 0; i < array.length; i += chunkSize) {
            chunks.push(array.slice(i, i + chunkSize));
        }
        return chunks;
    }
    enableWebGLRenderer() {
        if (!this.isWebGLAvailable) {
            console.warn('WebGL not available, falling back to SVG renderer');
            return;
        }
        console.log('Enabling WebGL renderer for large graph');
        this.optimizeSVGRenderer();
    }
    optimizeSVGRenderer() {
        const svg = this.graph.svg;
        this.graph.nodeElements
            .style('will-change', 'transform')
            .style('transform-origin', 'center');
        this.graph.edgeElements
            .style('shape-rendering', 'optimizeSpeed');
        const originalRestart = this.graph.restart.bind(this.graph);
        this.graph.restart = () => {
            if (this.animationFrameId) {
                cancelAnimationFrame(this.animationFrameId);
            }
            this.animationFrameId = requestAnimationFrame(() => {
                originalRestart();
            });
        };
    }
    setupViewportCulling() {
        if ('IntersectionObserver' in window) {
            this.intersectionObserver = new IntersectionObserver(
                (entries) => {
                    entries.forEach(entry => {
                        const element = d3.select(entry.target);
                        element.style('opacity', entry.isIntersecting ? 1 : 0);
                    });
                },
                {
                    root: this.graph.svg.node(),
                    threshold: 0.1
                }
            );
        }
    }
    setupPerformanceMonitoring() {
        this.performanceStats = {
            fps: 60,
            frameTime: 0,
            lastFrame: performance.now(),
            frameCount: 0
        };
        const updateStats = () => {
            const now = performance.now();
            this.performanceStats.frameTime = now - this.performanceStats.lastFrame;
            this.performanceStats.lastFrame = now;
            this.performanceStats.frameCount++;
            if (this.performanceStats.frameCount % 60 === 0) {
                this.performanceStats.fps = Math.round(1000 / this.performanceStats.frameTime);
                this.onPerformanceUpdate?.(this.performanceStats);
            }
            requestAnimationFrame(updateStats);
        };
        requestAnimationFrame(updateStats);
    }
    showLoadingProgress() {
        const loading = document.getElementById('loading');
        if (loading) {
            loading.style.display = 'flex';
            loading.innerHTML = `
                <div class="loading-spinner"></div>
                <p>Loading large graph...</p>
                <div class="progress-bar">
                    <div class="progress-fill" id="progress-fill"></div>
                </div>
            `;
        }
    }
    updateLoadingProgress(percent) {
        const progressFill = document.getElementById('progress-fill');
        if (progressFill) {
            progressFill.style.width = `${percent}%`;
        }
    }
    hideLoadingProgress() {
        const loading = document.getElementById('loading');
        if (loading) {
            loading.style.display = 'none';
        }
    }
    getPerformanceStats() {
        return this.performanceStats;
    }
    setThreshold(type, value) {
        if (this.thresholds.hasOwnProperty(type)) {
            this.thresholds[type] = value;
        }
    }
    onPerformanceUpdate(stats) {
    }
    cleanup() {
        if (this.animationFrameId) {
            cancelAnimationFrame(this.animationFrameId);
        }
        if (this.intersectionObserver) {
            this.intersectionObserver.disconnect();
        }
    }
}
window.PerformanceManager = PerformanceManager;
</file>

<file path="web/style.css">
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}
:root {
    --primary-color: #2563eb;
    --secondary-color: #64748b;
    --success-color: #10b981;
    --warning-color: #f59e0b;
    --error-color: #ef4444;
    --background-color: #f8fafc;
    --surface-color: #ffffff;
    --text-color: #1e293b;
    --text-muted: #64748b;
    --border-color: #e2e8f0;
    --shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1), 0 1px 2px 0 rgba(0, 0, 0, 0.06);
}
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background-color: var(--background-color);
    color: var(--text-color);
    line-height: 1.6;
    overflow: hidden;
}
#app {
    height: 100vh;
    display: flex;
    flex-direction: column;
}
.header {
    background: var(--surface-color);
    border-bottom: 1px solid var(--border-color);
    padding: 1rem 1.5rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
    box-shadow: var(--shadow);
    z-index: 10;
}
.header h1 {
    font-size: 1.5rem;
    font-weight: 600;
    color: var(--primary-color);
}
.header-controls {
    display: flex;
    gap: 0.5rem;
}
.btn {
    padding: 0.5rem 1rem;
    border: none;
    border-radius: 0.375rem;
    font-size: 0.875rem;
    font-weight: 500;
    cursor: pointer;
    transition: all 0.2s;
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
}
.btn:hover {
    transform: translateY(-1px);
}
.btn:disabled {
    opacity: 0.5;
    cursor: not-allowed;
    transform: none;
}
.btn-primary {
    background: var(--primary-color);
    color: white;
}
.btn-primary:hover:not(:disabled) {
    background: #1d4ed8;
}
.btn-secondary {
    background: var(--secondary-color);
    color: white;
}
.btn-secondary:hover:not(:disabled) {
    background: #475569;
}
.btn-small {
    padding: 0.25rem 0.5rem;
    font-size: 0.75rem;
}
.main-content {
    display: flex;
    flex: 1;
    overflow: hidden;
}
.sidebar {
    width: 300px;
    background: var(--surface-color);
    border-right: 1px solid var(--border-color);
    overflow-y: auto;
    padding: 1rem;
}
.sidebar-section {
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid var(--border-color);
}
.sidebar-section:last-child {
    border-bottom: none;
}
.sidebar-section h3 {
    font-size: 0.875rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    color: var(--text-muted);
    margin-bottom: 0.75rem;
}
.filter-group {
    margin-bottom: 1rem;
}
.filter-group label {
    display: block;
    font-size: 0.875rem;
    font-weight: 500;
    margin-bottom: 0.5rem;
}
.filter-group input {
    width: 100%;
    padding: 0.5rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-size: 0.875rem;
}
.filter-group input:focus {
    outline: none;
    border-color: var(--primary-color);
    box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.1);
}
.checkbox-group {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}
.checkbox-item {
    display: flex;
    align-items: center;
    gap: 0.5rem;
}
.checkbox-item input[type="checkbox"] {
    width: auto;
}
.checkbox-item label {
    margin: 0;
    cursor: pointer;
    font-weight: normal;
}
.template-list {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}
.template-item {
    padding: 0.75rem;
    background: var(--background-color);
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    cursor: pointer;
    transition: all 0.2s;
}
.template-item:hover {
    background: var(--primary-color);
    color: white;
    transform: translateY(-1px);
}
.template-item h4 {
    font-size: 0.875rem;
    font-weight: 500;
    margin-bottom: 0.25rem;
}
.template-item p {
    font-size: 0.75rem;
    opacity: 0.8;
}
.stat-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0.5rem 0;
    border-bottom: 1px solid var(--border-color);
}
.stat-item:last-child {
    border-bottom: none;
}
.stat-label {
    font-size: 0.875rem;
    color: var(--text-muted);
}
.stat-value {
    font-weight: 600;
    color: var(--primary-color);
}
.node-details {
    font-size: 0.875rem;
}
.node-details .detail-item {
    margin-bottom: 0.5rem;
}
.node-details .detail-label {
    font-weight: 500;
    color: var(--text-muted);
}
.node-details .detail-value {
    color: var(--text-color);
    word-break: break-all;
}
.graph-container {
    flex: 1;
    position: relative;
    overflow: hidden;
}
#graph-svg-container {
    width: 100%;
    height: 100%;
}
#graph-svg {
    width: 100%;
    height: 100%;
    background: var(--background-color);
}
.node {
    cursor: pointer;
    transition: all 0.2s;
}
.node:hover {
    transform: scale(1.1);
}
.node.selected {
    stroke: var(--primary-color);
    stroke-width: 3px;
}
.node.file {
    fill: #8b5cf6;
}
.node.module {
    fill: #06b6d4;
}
.node.class {
    fill: #10b981;
}
.node.function {
    fill: #f59e0b;
}
.node.variable {
    fill: #ef4444;
}
.node.import {
    fill: #6b7280;
}
.edge {
    stroke: var(--secondary-color);
    stroke-width: 2px;
    fill: none;
    opacity: 0.5;
    transition: all 0.2s;
    cursor: pointer;
}
.edge:hover {
    opacity: 1;
    stroke-width: 3px;
}
.edge.contains {
    stroke: #10b981;
}
.edge.calls {
    stroke: #f59e0b;
}
.edge.imports {
    stroke: #8b5cf6;
}
.edge.inherits {
    stroke: #ef4444;
}
.node-label {
    font-size: 10px;
    fill: var(--text-color);
    text-anchor: middle;
    dominant-baseline: central;
    pointer-events: none;
    font-weight: 500;
}
.edge-label {
    font-size: 9px;
    fill: var(--text-muted);
    text-anchor: middle;
    pointer-events: none;
}
.graph-legend {
    position: absolute;
    top: 1rem;
    right: 1rem;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    padding: 1rem;
    box-shadow: var(--shadow);
    z-index: 10;
}
.graph-legend h4 {
    font-size: 0.875rem;
    font-weight: 600;
    margin-bottom: 0.75rem;
    color: var(--text-color);
}
.legend-items {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}
.legend-item {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    font-size: 0.875rem;
}
.legend-color {
    width: 16px;
    height: 16px;
    border-radius: 50%;
    border: 2px solid white;
    box-shadow: 0 0 0 1px var(--border-color);
}
.loading-overlay {
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: rgba(248, 250, 252, 0.9);
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    z-index: 100;
}
.loading-spinner {
    width: 40px;
    height: 40px;
    border: 4px solid var(--border-color);
    border-left: 4px solid var(--primary-color);
    border-radius: 50%;
    animation: spin 1s linear infinite;
    margin-bottom: 1rem;
}
@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}
.error-overlay {
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: rgba(248, 250, 252, 0.9);
    display: flex;
    justify-content: center;
    align-items: center;
    z-index: 100;
}
.error-content {
    background: var(--surface-color);
    padding: 2rem;
    border-radius: 0.5rem;
    box-shadow: var(--shadow);
    text-align: center;
    max-width: 400px;
}
.error-content h3 {
    color: var(--error-color);
    margin-bottom: 1rem;
}
.error-content p {
    margin-bottom: 1.5rem;
    color: var(--text-muted);
}
.status-bar {
    background: var(--surface-color);
    border-top: 1px solid var(--border-color);
    padding: 0.5rem 1rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
    font-size: 0.875rem;
    color: var(--text-muted);
}
.status-controls {
    display: flex;
    gap: 1rem;
    align-items: center;
}
.modal {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: rgba(0, 0, 0, 0.5);
    display: flex;
    justify-content: center;
    align-items: center;
    z-index: 1000;
}
.modal-content {
    background: var(--surface-color);
    border-radius: 0.5rem;
    box-shadow: 0 10px 25px rgba(0, 0, 0, 0.2);
    width: 90%;
    max-width: 600px;
    max-height: 80vh;
    overflow: hidden;
}
.modal-header {
    padding: 1rem 1.5rem;
    border-bottom: 1px solid var(--border-color);
    display: flex;
    justify-content: space-between;
    align-items: center;
}
.modal-header h3 {
    margin: 0;
    font-size: 1.25rem;
}
.close {
    font-size: 1.5rem;
    cursor: pointer;
    color: var(--text-muted);
}
.close:hover {
    color: var(--text-color);
}
.modal-body {
    padding: 1.5rem;
    overflow-y: auto;
}
.modal-footer {
    padding: 1rem 1.5rem;
    border-top: 1px solid var(--border-color);
    display: flex;
    justify-content: flex-end;
    gap: 0.5rem;
}
.query-section {
    margin-bottom: 1rem;
}
.query-section label {
    display: block;
    font-weight: 500;
    margin-bottom: 0.5rem;
}
.query-section textarea {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
    font-size: 0.875rem;
    resize: vertical;
}
.query-section textarea:focus {
    outline: none;
    border-color: var(--primary-color);
    box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.1);
}
@media (max-width: 768px) {
    .sidebar {
        width: 250px;
    }
    .main-content {
        flex-direction: column;
    }
    .sidebar {
        width: 100%;
        height: 200px;
        order: 2;
    }
    .graph-container {
        order: 1;
        flex: 1;
    }
}
.graph-tooltip {
    background: rgba(255, 255, 255, 0.98);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    padding: 0;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    font-size: 0.875rem;
    max-width: 400px;
    z-index: 1000;
}
.tooltip-header {
    background: var(--primary-color);
    color: white;
    padding: 0.75rem 1rem;
    font-weight: 600;
    border-radius: 0.5rem 0.5rem 0 0;
}
.tooltip-body {
    padding: 1rem;
}
.tooltip-section {
    font-weight: 600;
    margin-top: 0.75rem;
    margin-bottom: 0.5rem;
    color: var(--text-muted);
}
.tooltip-item {
    margin-bottom: 0.5rem;
    line-height: 1.4;
}
.tooltip-item:last-child {
    margin-bottom: 0;
}
.tooltip-label {
    font-weight: 500;
    color: var(--text-muted);
    margin-right: 0.5rem;
}
.hidden {
    display: none !important;
}
.text-center {
    text-align: center;
}
.text-muted {
    color: var(--text-muted);
}
.mb-1 {
    margin-bottom: 0.25rem;
}
.mb-2 {
    margin-bottom: 0.5rem;
}
.mb-3 {
    margin-bottom: 0.75rem;
}
.font-mono {
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
}
</file>

<file path="src/codebased/database/schema.py">
import logging
from typing import List, Dict, Any
from .service import DatabaseService
logger = logging.getLogger(__name__)
class GraphSchema:
    NODE_TABLES = {
        'File': """
            CREATE NODE TABLE IF NOT EXISTS File(
                id STRING,
                name STRING,
                path STRING,
                extension STRING,
                size INT64,
                modified_time INT64,
                hash STRING,
                lines_of_code INT64,
                PRIMARY KEY (id)
            )
        """,
        'Module': """
            CREATE NODE TABLE IF NOT EXISTS Module(
                id STRING,
                name STRING,
                file_id STRING,
                docstring STRING,
                line_start INT64,
                line_end INT64,
                PRIMARY KEY (id)
            )
        """,
        'Class': """
            CREATE NODE TABLE IF NOT EXISTS Class(
                id STRING,
                name STRING,
                file_id STRING,
                module_id STRING,
                docstring STRING,
                line_start INT64,
                line_end INT64,
                is_abstract BOOLEAN,
                PRIMARY KEY (id)
            )
        """,
        'Function': """
            CREATE NODE TABLE IF NOT EXISTS Function(
                id STRING,
                name STRING,
                file_id STRING,
                module_id STRING,
                class_id STRING,
                docstring STRING,
                line_start INT64,
                line_end INT64,
                signature STRING,
                return_type STRING,
                is_async BOOLEAN,
                is_generator BOOLEAN,
                is_property BOOLEAN,
                is_staticmethod BOOLEAN,
                is_classmethod BOOLEAN,
                complexity INT64,
                PRIMARY KEY (id)
            )
        """,
        'Variable': """
            CREATE NODE TABLE IF NOT EXISTS Variable(
                id STRING,
                name STRING,
                file_id STRING,
                scope_id STRING,
                type_annotation STRING,
                line_number INT64,
                is_global BOOLEAN,
                is_constant BOOLEAN,
                PRIMARY KEY (id)
            )
        """,
        'Import': """
            CREATE NODE TABLE IF NOT EXISTS Import(
                id STRING,
                name STRING,
                file_id STRING,
                module_name STRING,
                alias STRING,
                line_number INT64,
                is_from_import BOOLEAN,
                PRIMARY KEY (id)
            )
        """
    }
    RELATIONSHIP_TABLES = {
        'FILE_CONTAINS_MODULE': """
            CREATE REL TABLE IF NOT EXISTS FILE_CONTAINS_MODULE(FROM File TO Module)
        """,
        'FILE_CONTAINS_CLASS': """
            CREATE REL TABLE IF NOT EXISTS FILE_CONTAINS_CLASS(FROM File TO Class)
        """,
        'FILE_CONTAINS_FUNCTION': """
            CREATE REL TABLE IF NOT EXISTS FILE_CONTAINS_FUNCTION(FROM File TO Function)
        """,
        'FILE_CONTAINS_VARIABLE': """
            CREATE REL TABLE IF NOT EXISTS FILE_CONTAINS_VARIABLE(FROM File TO Variable)
        """,
        'FILE_CONTAINS_IMPORT': """
            CREATE REL TABLE IF NOT EXISTS FILE_CONTAINS_IMPORT(FROM File TO Import)
        """,
        'MODULE_CONTAINS_CLASS': """
            CREATE REL TABLE IF NOT EXISTS MODULE_CONTAINS_CLASS(FROM Module TO Class)
        """,
        'MODULE_CONTAINS_FUNCTION': """
            CREATE REL TABLE IF NOT EXISTS MODULE_CONTAINS_FUNCTION(FROM Module TO Function)
        """,
        'MODULE_CONTAINS_VARIABLE': """
            CREATE REL TABLE IF NOT EXISTS MODULE_CONTAINS_VARIABLE(FROM Module TO Variable)
        """,
        'CLASS_CONTAINS_FUNCTION': """
            CREATE REL TABLE IF NOT EXISTS CLASS_CONTAINS_FUNCTION(FROM Class TO Function)
        """,
        'CLASS_CONTAINS_VARIABLE': """
            CREATE REL TABLE IF NOT EXISTS CLASS_CONTAINS_VARIABLE(FROM Class TO Variable)
        """,
        'FUNCTION_CONTAINS_VARIABLE': """
            CREATE REL TABLE IF NOT EXISTS FUNCTION_CONTAINS_VARIABLE(FROM Function TO Variable)
        """,
        'FUNCTION_CONTAINS_FUNCTION': """
            CREATE REL TABLE IF NOT EXISTS FUNCTION_CONTAINS_FUNCTION(FROM Function TO Function)
        """,
        'CLASS_CONTAINS_CLASS': """
            CREATE REL TABLE IF NOT EXISTS CLASS_CONTAINS_CLASS(FROM Class TO Class)
        """,
        'CALLS': """
            CREATE REL TABLE IF NOT EXISTS CALLS(FROM Function TO Function, call_type STRING, line_number INT64)
        """,
        'IMPORTS': """
            CREATE REL TABLE IF NOT EXISTS IMPORTS(FROM File TO File, import_type STRING)
        """,
        'INHERITS': """
            CREATE REL TABLE IF NOT EXISTS INHERITS(FROM Class TO Class)
        """,
        'USES': """
            CREATE REL TABLE IF NOT EXISTS USES(FROM Function TO Variable, usage_type STRING, line_number INT64)
        """,
        'DECORATES': """
            CREATE REL TABLE IF NOT EXISTS DECORATES(FROM Function TO Function, decorator_name STRING)
        """
    }
    INDEXES = [
    ]
    def __init__(self, db_service: DatabaseService):
        self.db_service = db_service
    def create_schema(self) -> bool:
        try:
            logger.info("Creating graph schema...")
            for table_name, create_sql in self.NODE_TABLES.items():
                logger.debug(f"Creating node table: {table_name}")
                result = self.db_service.execute_query(create_sql)
                if result is None:
                    logger.error(f"Failed to create node table: {table_name}")
                    return False
            for table_name, create_sql in self.RELATIONSHIP_TABLES.items():
                logger.debug(f"Creating relationship table: {table_name}")
                result = self.db_service.execute_query(create_sql)
                if result is None:
                    logger.error(f"Failed to create relationship table: {table_name}")
                    return False
            for index_sql in self.INDEXES:
                logger.debug(f"Creating index: {index_sql}")
                result = self.db_service.execute_query(index_sql)
                if result is None:
                    logger.warning(f"Failed to create index: {index_sql}")
            logger.info("Graph schema created successfully")
            return True
        except Exception as e:
            logger.error(f"Failed to create graph schema: {e}")
            return False
    def drop_schema(self) -> bool:
        try:
            logger.warning("Dropping graph schema...")
            # Drop known tables in reverse order (relationships first, then nodes)
            for table_name in list(self.RELATIONSHIP_TABLES.keys()):
                drop_sql = f"DROP TABLE IF EXISTS {table_name}"
                logger.debug(f"Dropping relationship table: {table_name}")
                self.db_service.execute_query(drop_sql)
            for table_name in list(self.NODE_TABLES.keys()):
                drop_sql = f"DROP TABLE IF EXISTS {table_name}"
                logger.debug(f"Dropping node table: {table_name}")
                self.db_service.execute_query(drop_sql)
            logger.info("Graph schema dropped successfully")
            return True
        except Exception as e:
            logger.error(f"Failed to drop graph schema: {e}")
            return False
    def validate_schema(self) -> Dict[str, Any]:
        validation = {
            'valid': True,
            'tables': {},
            'missing_tables': [],
            'unexpected_tables': [],
            'errors': []
        }
        try:
            # Get all tables from the database
            tables_result = self.db_service.execute_query("CALL show_tables() RETURN *")
            if tables_result is None:
                validation['valid'] = False
                validation['errors'].append("Could not retrieve tables from database.")
                return validation
            existing_tables = {table['name'] for table in tables_result}
            expected_tables = set(self.NODE_TABLES.keys()) | set(self.RELATIONSHIP_TABLES.keys())
            # Find missing and unexpected tables
            validation['missing_tables'] = list(expected_tables - existing_tables)
            validation['unexpected_tables'] = list(existing_tables - expected_tables)
            if validation['missing_tables'] or validation['unexpected_tables']:
                validation['valid'] = False
            logger.info(f"Schema validation: {'valid' if validation['valid'] else 'invalid'}")
        except Exception as e:
            validation['valid'] = False
            validation['errors'].append(str(e))
            logger.error(f"Schema validation failed: {e}")
        return validation
    def get_schema_info(self) -> Dict[str, Any]:
        info = {
            'node_tables': list(self.NODE_TABLES.keys()),
            'relationship_tables': list(self.RELATIONSHIP_TABLES.keys()),
            'total_tables': len(self.NODE_TABLES) + len(self.RELATIONSHIP_TABLES),
            'indexes': len(self.INDEXES),
            'validation': self.validate_schema()
        }
        return info
    def reset_schema(self) -> bool:
        logger.info("Resetting graph schema...")
        if not self.drop_schema():
            return False
        if not self.create_schema():
            return False
        logger.info("Graph schema reset successfully")
        return True
</file>

<file path="src/codebased/parsers/extractor.py">
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from .base import ParseResult, ParsedEntity, ParsedRelationship
from .registry import PARSER_REGISTRY
from ..database.service import DatabaseService
from ..config import CodeBasedConfig
logger = logging.getLogger(__name__)
class EntityExtractor:
    def __init__(self, config: CodeBasedConfig, db_service: DatabaseService):
        self.config = config
        self.db_service = db_service
        self.parsers = {}
        parser_config = self._get_parser_config()
        for name, parser_cls in PARSER_REGISTRY.items():
            try:
                self.parsers[name] = parser_cls(parser_config)
            except Exception as e:
                logger.error(f"Failed to initialize parser '{name}': {e}")
        self.symbol_registry: Dict[str, ParsedEntity] = {}
        self.unresolved_references: List[Tuple[ParsedRelationship, str]] = []
    def _get_parser_config(self) -> Dict[str, Any]:
        return {
            'file_extensions': self.config.parsing.file_extensions,
            'exclude_patterns': self.config.parsing.exclude_patterns,
            'max_file_size': self.config.parsing.max_file_size,
            'follow_symlinks': self.config.parsing.follow_symlinks,
            'include_docstrings': self.config.parsing.include_docstrings
        }
    def extract_from_directory(self, directory_path: str) -> Dict[str, Any]:
        logger.info(f"Starting entity extraction from {directory_path}")
        start_time = time.time()
        results = {
            'files_processed': 0,
            'files_failed': 0,
            'entities_extracted': 0,
            'relationships_extracted': 0,
            'errors': [],
            'parse_results': []
        }
        try:
            parse_results = self._two_pass_extraction(directory_path)
            self._store_results(parse_results)
            for result in parse_results:
                results['parse_results'].append(result)
                if result.errors:
                    results['files_failed'] += 1
                    results['errors'].extend(result.errors)
                else:
                    results['files_processed'] += 1
                results['entities_extracted'] += len(result.entities)
                results['relationships_extracted'] += len(result.relationships)
            total_time = time.time() - start_time
            logger.info(f"Entity extraction completed in {total_time:.2f}s")
            logger.info(f"Processed {results['files_processed']} files, "
                       f"extracted {results['entities_extracted']} entities, "
                       f"{results['relationships_extracted']} relationships")
        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            results['errors'].append(str(e))
        return results
    def _two_pass_extraction(self, directory_path: str) -> List[ParseResult]:
        directory_path = Path(directory_path)
        parse_results = []
        all_files = []
        for parser in self.parsers.values():
            for file_path in parser._find_parseable_files(directory_path):
                all_files.append(str(file_path))
        logger.info(f"Found {len(all_files)} files to parse")
        logger.info("Starting Pass 1: Entity extraction and symbol registration")
        pass1_results = self._extract_entities_parallel(all_files)
        for result in pass1_results:
            for entity in result.entities:
                self._register_symbol(entity)
        logger.info(f"Pass 1 completed: {len(self.symbol_registry)} symbols registered")
        logger.info("Starting Pass 2: Relationship resolution")
        pass2_results = self._resolve_relationships(pass1_results)
        logger.info("Pass 2 completed: Relationships resolved")
        return pass2_results
    def _extract_entities_parallel(self, file_paths: List[str]) -> List[ParseResult]:
        results = []
        max_workers = min(4, len(file_paths))
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {}
            for file_path in file_paths:
                parser = self._get_parser_for_file(file_path)
                if parser:
                    future = executor.submit(parser.parse_file, file_path)
                    future_to_file[future] = file_path
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.error(f"Failed to parse {file_path}: {e}")
                    error_result = ParseResult([], [], "", file_path, [str(e)], 0.0)
                    results.append(error_result)
        return results
    def _resolve_relationships(self, parse_results: List[ParseResult]) -> List[ParseResult]:
        resolved_results = []
        for result in parse_results:
            resolved_relationships = []
            for relationship in result.relationships:
                if relationship.to_id.startswith("unresolved:"):
                    resolved_id = self._resolve_symbol_reference(relationship.to_id)
                    if resolved_id:
                        resolved_rel = ParsedRelationship(
                            from_id=relationship.from_id,
                            to_id=resolved_id,
                            relationship_type=relationship.relationship_type,
                            metadata=relationship.metadata
                        )
                        resolved_relationships.append(resolved_rel)
                    else:
                        resolved_relationships.append(relationship)
                else:
                    resolved_relationships.append(relationship)
            resolved_result = ParseResult(
                entities=result.entities,
                relationships=resolved_relationships,
                file_hash=result.file_hash,
                file_path=result.file_path,
                errors=result.errors,
                parse_time=result.parse_time
            )
            resolved_results.append(resolved_result)
        return resolved_results
    def _register_symbol(self, entity: ParsedEntity) -> None:
        keys = [entity.name]
        if entity.type == "Function" and "class_id" in entity.metadata:
            class_name = self._get_entity_name_by_id(entity.metadata["class_id"])
            if class_name:
                keys.append(f"{class_name}.{entity.name}")
        if entity.type == "Class" and "module_id" in entity.metadata:
            module_name = self._get_entity_name_by_id(entity.metadata["module_id"])
            if module_name:
                keys.append(f"{module_name}.{entity.name}")
        for key in keys:
            if key not in self.symbol_registry:
                self.symbol_registry[key] = entity
            else:
                existing = self.symbol_registry[key]
                if self._is_more_specific(entity, existing):
                    self.symbol_registry[key] = entity
    def _resolve_symbol_reference(self, unresolved_ref: str) -> Optional[str]:
        if not unresolved_ref.startswith("unresolved:"):
            return unresolved_ref
        symbol_name = unresolved_ref[11:]
        if symbol_name in self.symbol_registry:
            return self.symbol_registry[symbol_name].id
        for registered_name, entity in self.symbol_registry.items():
            if registered_name.endswith(f".{symbol_name}") or symbol_name.endswith(f".{registered_name}"):
                return entity.id
        return None
    def _get_parser_for_file(self, file_path: str) -> Optional[Any]:
        for parser in self.parsers.values():
            if parser.can_parse(file_path):
                return parser
        return None
    def _get_entity_name_by_id(self, entity_id: str) -> Optional[str]:
        for entity in self.symbol_registry.values():
            if entity.id == entity_id:
                return entity.name
        return None
    def _is_more_specific(self, entity1: ParsedEntity, entity2: ParsedEntity) -> bool:
        weight1 = 0
        weight2 = 0
        if "class_id" in entity1.metadata and entity1.metadata["class_id"]:
            weight1 += 2
        if "module_id" in entity1.metadata and entity1.metadata["module_id"]:
            weight1 += 1
        if "class_id" in entity2.metadata and entity2.metadata["class_id"]:
            weight2 += 2
        if "module_id" in entity2.metadata and entity2.metadata["module_id"]:
            weight2 += 1
        return weight1 > weight2
    def _store_results(self, parse_results: List[ParseResult]) -> None:
        logger.info("Storing extraction results in database...")
        try:
            entity_queries = []
            relationship_queries = []
            for result in parse_results:
                for entity in result.entities:
                    query = self._create_entity_insert_query(entity)
                    if query:
                        entity_queries.append(query)
                for relationship in result.relationships:
                    query = self._create_relationship_insert_query(relationship)
                    if query:
                        relationship_queries.append(query)
            batch_size = self.config.database.batch_size
            for i in range(0, len(entity_queries), batch_size):
                batch = entity_queries[i:i + batch_size]
                if not self.db_service.execute_batch(batch):
                    logger.error(f"Failed to insert entity batch {i // batch_size + 1}")
            for i in range(0, len(relationship_queries), batch_size):
                batch = relationship_queries[i:i + batch_size]
                if not self.db_service.execute_batch(batch):
                    logger.error(f"Failed to insert relationship batch {i // batch_size + 1}")
            logger.info(f"Stored {len(entity_queries)} entities and {len(relationship_queries)} relationships")
        except Exception as e:
            logger.error(f"Failed to store results in database: {e}")
    def _create_entity_insert_query(self, entity: ParsedEntity) -> Optional[str]:
        try:
            def escape_string(value):
                if isinstance(value, str):
                    escaped = value.replace('"', '\\"')
                    return f'"{escaped}"'
                return str(value)
            properties = [
                f'id: {escape_string(entity.id)}',
                f'name: {escape_string(entity.name)}'
            ]
            if entity.type == 'File':
                properties.append(f'path: {escape_string(entity.file_path)}')
            else:
                file_id = entity.metadata.get('file_id', '')
                if file_id:
                    properties.append(f'file_id: {escape_string(file_id)}')
            if entity.type in ['Module', 'Class', 'Function']:
                if hasattr(entity, 'line_start') and entity.line_start is not None:
                    properties.append(f'line_start: {entity.line_start}')
                if hasattr(entity, 'line_end') and entity.line_end is not None:
                    properties.append(f'line_end: {entity.line_end}')
            elif entity.type in ['Import', 'Variable']:
                if hasattr(entity, 'line_start') and entity.line_start is not None:
                    properties.append(f'line_number: {entity.line_start}')
            schema_properties = {
                'File': ['extension', 'size', 'modified_time', 'hash', 'lines_of_code'],
                'Module': ['docstring'],
                'Class': ['module_id', 'docstring', 'is_abstract'],
                'Function': ['module_id', 'class_id', 'docstring', 'signature', 'return_type',
                           'is_async', 'is_generator', 'is_property', 'is_staticmethod', 'is_classmethod', 'complexity'],
                'Variable': ['scope_id', 'type_annotation', 'is_global', 'is_constant'],
                'Import': ['module_name', 'alias', 'is_from_import']
            }
            allowed_props = schema_properties.get(entity.type, [])
            for key, value in entity.metadata.items():
                if value is not None and key in allowed_props:
                    if isinstance(value, bool):
                        properties.append(f'{key}: {str(value).lower()}')
                    elif isinstance(value, (int, float)):
                        properties.append(f'{key}: {value}')
                    else:
                        properties.append(f'{key}: {escape_string(str(value))}')
            props_str = ', '.join(properties)
            query = f"CREATE (:{entity.type} {{{props_str}}})"
            return query
        except Exception as e:
            logger.error(f"Failed to create entity query for {entity.name}: {e}")
            return None
    def _create_relationship_insert_query(self, relationship: ParsedRelationship) -> Optional[str]:
        try:
            if (relationship.from_id.startswith("unresolved:") or
                relationship.to_id.startswith("unresolved:")):
                return None
            def escape_string(value):
                if isinstance(value, str):
                    escaped = value.replace('"', '\\"')
                    return f'"{escaped}"'
                return str(value)
            properties = []
            for key, value in relationship.metadata.items():
                if value is not None:
                    if isinstance(value, bool):
                        properties.append(f'{key}: {str(value).lower()}')
                    elif isinstance(value, (int, float)):
                        properties.append(f'{key}: {value}')
                    else:
                        properties.append(f'{key}: {escape_string(str(value))}')
            props_str = '{' + ', '.join(properties) + '}' if properties else ''
            query = f"""
            MATCH (from_node {{id: {escape_string(relationship.from_id)}}}),
                  (to_node {{id: {escape_string(relationship.to_id)}}})
            CREATE (from_node)-[:{relationship.relationship_type} {props_str}]->(to_node)
            """
            return query.strip()
        except Exception as e:
            logger.error(f"Failed to create relationship query: {e}")
            return None
</file>

<file path="src/codebased/parsers/incremental.py">
import os
import time
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
from .extractor import EntityExtractor
from .base import ParseResult
from .registry import get_parser
from ..database.service import DatabaseService
from ..config import CodeBasedConfig
logger = logging.getLogger(__name__)
class IncrementalUpdater:
    def __init__(self, config: CodeBasedConfig, db_service: DatabaseService):
        self.config = config
        self.db_service = db_service
        self.extractor = EntityExtractor(config, db_service)
        self.file_hashes: Dict[str, str] = {}
        self._load_file_hashes()
    def update_graph(self, directory_path: str = None) -> Dict[str, Any]:
        if directory_path is None:
            directory_path = self.config.project_root
        logger.info(f"Starting incremental update for {directory_path}")
        start_time = time.time()
        results = {
            'total_files': 0,
            'files_added': 0,
            'files_modified': 0,
            'files_removed': 0,
            'files_unchanged': 0,
            'entities_added': 0,
            'entities_updated': 0,
            'entities_removed': 0,
            'relationships_added': 0,
            'relationships_updated': 0,
            'relationships_removed': 0,
            'errors': [],
            'update_time': 0.0
        }
        try:
            changes = self._detect_changes(directory_path)
            results.update(changes)
            if changes['added'] or changes['modified'] or changes['removed']:
                self._process_changes(changes, results)
            else:
                logger.info("No changes detected - skipping update")
            self._save_file_hashes()
            results['update_time'] = time.time() - start_time
            logger.info(f"Incremental update completed in {results['update_time']:.2f}s")
        except Exception as e:
            logger.error(f"Incremental update failed: {e}")
            results['errors'].append(str(e))
        return results
    def force_full_update(self, directory_path: str = None) -> Dict[str, Any]:
        if directory_path is None:
            directory_path = self.config.project_root
        logger.info(f"Starting full update for {directory_path}")
        start_time = time.time()
        try:
            logger.info("Clearing existing graph data...")
            if not self.db_service.clear_graph():
                logger.error("Failed to clear graph data")
                return {'errors': ['Failed to clear graph data'], 'update_time': 0.0}
            self.file_hashes.clear()
            extraction_results = self.extractor.extract_from_directory(directory_path)
            for result in extraction_results['parse_results']:
                if result.file_hash:
                    self.file_hashes[result.file_path] = result.file_hash
            self._save_file_hashes()
            results = {
                'total_files': extraction_results['files_processed'] + extraction_results['files_failed'],
                'files_added': extraction_results['files_processed'],
                'files_modified': 0,
                'files_removed': 0,
                'files_unchanged': 0,
                'entities_added': extraction_results['entities_extracted'],
                'entities_updated': 0,
                'entities_removed': 0,
                'relationships_added': extraction_results['relationships_extracted'],
                'relationships_updated': 0,
                'relationships_removed': 0,
                'errors': extraction_results['errors'],
                'update_time': time.time() - start_time
            }
            logger.info(f"Full update completed in {results['update_time']:.2f}s")
            return results
        except Exception as e:
            logger.error(f"Full update failed: {e}")
            return {
                'errors': [str(e)],
                'update_time': time.time() - start_time
            }
    def _detect_changes(self, directory_path: str) -> Dict[str, Any]:
        logger.info("Detecting file changes...")
        changes = {
            'added': [],
            'modified': [],
            'removed': [],
            'unchanged': [],
            'total_files': 0
        }
        directory_path = Path(directory_path)
        current_files = set()
        for parser in self.extractor.parsers.values():
            for file_path in parser._find_parseable_files(directory_path):
                file_path_str = str(file_path)
                current_files.add(file_path_str)
                current_hash = self._calculate_file_hash(file_path_str)
                if file_path_str not in self.file_hashes:
                    changes['added'].append(file_path_str)
                elif self.file_hashes[file_path_str] != current_hash:
                    changes['modified'].append(file_path_str)
                else:
                    changes['unchanged'].append(file_path_str)
        for stored_file in self.file_hashes:
            if stored_file not in current_files:
                changes['removed'].append(stored_file)
        changes['total_files'] = len(current_files)
        logger.info(f"Change detection completed: "
                   f"{len(changes['added'])} added, "
                   f"{len(changes['modified'])} modified, "
                   f"{len(changes['removed'])} removed, "
                   f"{len(changes['unchanged'])} unchanged")
        return changes
    def _process_changes(self, changes: Dict[str, Any], results: Dict[str, Any]) -> None:
        if changes['removed']:
            logger.info(f"Processing {len(changes['removed'])} removed files...")
            self._remove_files_from_graph(changes['removed'], results)
        files_to_parse = changes['added'] + changes['modified']
        if files_to_parse:
            logger.info(f"Processing {len(files_to_parse)} added/modified files...")
            if changes['modified']:
                self._remove_files_from_graph(changes['modified'], results)
            parse_results = []
            for file_path in files_to_parse:
                parser = self.extractor._get_parser_for_file(file_path)
                if parser:
                    try:
                        result = parser.parse_file(file_path)
                        parse_results.append(result)
                        if result.file_hash:
                            self.file_hashes[file_path] = result.file_hash
                    except Exception as e:
                        logger.error(f"Failed to parse {file_path}: {e}")
                        results['errors'].append(f"Parse error in {file_path}: {e}")
            if parse_results:
                resolved_results = self.extractor._resolve_relationships(parse_results)
                self.extractor._store_results(resolved_results)
                for result in resolved_results:
                    if result.file_path in changes['added']:
                        results['entities_added'] += len(result.entities)
                        results['relationships_added'] += len(result.relationships)
                    else:
                        results['entities_updated'] += len(result.entities)
                        results['relationships_updated'] += len(result.relationships)
        results['files_added'] = len(changes['added'])
        results['files_modified'] = len(changes['modified'])
        results['files_removed'] = len(changes['removed'])
        results['files_unchanged'] = len(changes['unchanged'])
        results['total_files'] = changes['total_files']
    def _remove_files_from_graph(self, file_paths: List[str], results: Dict[str, Any]) -> None:
        for file_path in file_paths:
            try:
                entities_query = f"""
                MATCH (n {{file_path: "{file_path}"}})
                RETURN n.id AS id, 'Entity' AS type
                """
                entities_result = self.db_service.execute_query(entities_query)
                if entities_result:
                    entity_ids = [entity['id'] for entity in entities_result]
                    if entity_ids:
                        for entity_id in entity_ids:
                            rel_query = f"""
                            MATCH (n {{id: "{entity_id}"}})-[r]-()
                            DELETE r
                            """
                            self.db_service.execute_query(rel_query)
                            results['relationships_removed'] += 1
                    for entity_id in entity_ids:
                        entity_query = f"""
                        MATCH (n {{id: "{entity_id}"}})
                        DELETE n
                        """
                        self.db_service.execute_query(entity_query)
                        results['entities_removed'] += 1
                if file_path in self.file_hashes:
                    del self.file_hashes[file_path]
            except Exception as e:
                logger.error(f"Failed to remove {file_path} from graph: {e}")
                results['errors'].append(f"Removal error for {file_path}: {e}")
    def _calculate_file_hash(self, file_path: str) -> str:
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            return hashlib.sha256(content).hexdigest()
        except Exception as e:
            logger.error(f"Failed to calculate hash for {file_path}: {e}")
            return ""
    def _load_file_hashes(self) -> None:
        try:
            query = "MATCH (f:File) RETURN f.path AS path, f.hash AS hash"
            result = self.db_service.execute_query(query)
            if result:
                self.file_hashes = {}
                for row in result:
                    if isinstance(row, dict):
                        path = row.get('path')
                        hash_val = row.get('hash')
                    else:
                        try:
                            path = row[0] if len(row) > 0 else None
                            hash_val = row[1] if len(row) > 1 else None
                        except (IndexError, TypeError):
                            continue
                    if path and hash_val:
                        self.file_hashes[path] = hash_val
                logger.debug(f"Loaded {len(self.file_hashes)} file hashes from database")
            else:
                logger.debug("No file hashes found in database")
        except Exception as e:
            logger.error(f"Failed to load file hashes: {e}")
            self.file_hashes = {}
    def _save_file_hashes(self) -> None:
        try:
            logger.debug(f"File hashes tracked for {len(self.file_hashes)} files")
        except Exception as e:
            logger.error(f"Failed to save file hashes: {e}")
    def get_update_status(self) -> Dict[str, Any]:
        try:
            stats = self.db_service.get_stats()
            status = {
                'last_update': None,
                'tracked_files': len(self.file_hashes),
                'database_stats': stats,
                'health': self.db_service.health_check()
            }
            return status
        except Exception as e:
            logger.error(f"Failed to get update status: {e}")
            return {
                'error': str(e),
                'tracked_files': len(self.file_hashes)
            }
    def cleanup_orphaned_entities(self) -> Dict[str, int]:
        logger.info("Starting cleanup of orphaned entities...")
        cleanup_stats = {'entities_removed': 0, 'relationships_removed': 0}
        try:
            query = """
            MATCH (n)
            WHERE exists(n.file_path) AND n.file_path IS NOT NULL
            RETURN DISTINCT n.file_path AS file_path
            """
            result = self.db_service.execute_query(query)
            if result:
                existing_files = set()
                for parser in self.extractor.parsers.values():
                    directory_path = Path(self.config.project_root)
                    for file_path in parser._find_parseable_files(directory_path):
                        existing_files.add(str(file_path))
                orphaned_files = []
                for row in result:
                    file_path = row['file_path']
                    if file_path not in existing_files and not Path(file_path).exists():
                        orphaned_files.append(file_path)
                if orphaned_files:
                    logger.info(f"Found {len(orphaned_files)} orphaned files")
                    for file_path in orphaned_files:
                        remove_query = f"""
                        MATCH (n {{file_path: "{file_path}"}})
                        DETACH DELETE n
                        """
                        entities_before = self.db_service.get_stats()['nodes']
                        self.db_service.execute_query(remove_query)
                        entities_after = self.db_service.get_stats()['nodes']
                        cleanup_stats['entities_removed'] += entities_before - entities_after
                        if file_path in self.file_hashes:
                            del self.file_hashes[file_path]
                logger.info(f"Cleanup completed: removed {cleanup_stats['entities_removed']} orphaned entities")
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")
            cleanup_stats['error'] = str(e)
        return cleanup_stats
</file>

<file path="src/codebased/parsers/python.py">
import ast
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Union
from .base import BaseParser, ParsedEntity, ParsedRelationship, ParseResult
logger = logging.getLogger(__name__)
class PythonASTParser(BaseParser):
    SUPPORTED_FILE_TYPES = {"python"}
    def parse_file(self, file_path: str) -> ParseResult:
        start_time = time.time()
        entities = []
        relationships = []
        errors = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            file_hash = self._calculate_file_hash(file_path)
            try:
                tree = ast.parse(content, filename=file_path)
            except SyntaxError as e:
                error_msg = f"Syntax error in {file_path}:{e.lineno}: {e.msg}"
                logger.error(error_msg)
                errors.append(error_msg)
                return ParseResult([], [], file_hash, file_path, errors, time.time() - start_time)
            visitor = PythonASTVisitor(file_path, content.splitlines())
            visitor.visit(tree)
            entities = visitor.entities
            relationships = visitor.relationships
            errors = visitor.errors
        except Exception as e:
            error_msg = f"Failed to parse {file_path}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
            file_hash = ""
        parse_time = time.time() - start_time
        return ParseResult(entities, relationships, file_hash, file_path, errors, parse_time)
class PythonASTVisitor(ast.NodeVisitor):
    def __init__(self, file_path: str, lines: List[str]):
        self.file_path = file_path
        self.lines = lines
        self.entities: List[ParsedEntity] = []
        self.relationships: List[ParsedRelationship] = []
        self.errors: List[str] = []
        self.current_class: Optional[str] = None
        self.current_function: Optional[str] = None
        self.current_module: Optional[str] = None
        self.entity_ids: Dict[str, str] = {}
        self.imports: Dict[str, str] = {}
        self.class_name_stack: List[str] = []
        self.function_name_stack: List[str] = []
        self.module_name: str = Path(self.file_path).stem
        self._create_file_entity()
    def _create_file_entity(self):
        file_path_obj = Path(self.file_path)
        file_id = self._generate_id("file", self.file_path, 1)
        file_entity = ParsedEntity(
            id=file_id,
            name=file_path_obj.name,
            type="File",
            file_path=self.file_path,
            line_start=1,
            line_end=len(self.lines),
            metadata={
                "path": self.file_path,
                "extension": file_path_obj.suffix,
                "size": len('\n'.join(self.lines)),
                "lines_of_code": len([line for line in self.lines if line.strip() and not line.strip().startswith('#')])
            }
        )
        self.entities.append(file_entity)
        self.entity_ids['file'] = file_id
    def visit_Module(self, node: ast.Module) -> None:
        file_path_obj = Path(self.file_path)
        module_name = file_path_obj.stem
        module_id = self._generate_id("module", module_name, 1)
        docstring = self._get_docstring(node)
        module_entity = ParsedEntity(
            id=module_id,
            name=module_name,
            type="Module",
            file_path=self.file_path,
            line_start=1,
            line_end=len(self.lines),
            metadata={
                "docstring": docstring,
                "file_id": self.entity_ids['file']
            }
        )
        self.entities.append(module_entity)
        self.entity_ids['module'] = module_id
        self.current_module = module_id
        self._create_relationship(
            self.entity_ids['file'],
            module_id,
            "FILE_CONTAINS_MODULE"
        )
        self.generic_visit(node)
    def visit_ClassDef(self, node: ast.ClassDef) -> None:
        self.class_name_stack.append(node.name)
        class_id = self._generate_id("class", node.name, node.lineno)
        parent_class = self.current_class
        self.current_class = class_id
        docstring = self._get_docstring(node)
        is_abstract = self._is_abstract_class(node)
        class_entity = ParsedEntity(
            id=class_id,
            name=node.name,
            type="Class",
            file_path=self.file_path,
            line_start=node.lineno,
            line_end=self._get_end_line(node),
            metadata={
                "docstring": docstring,
                "is_abstract": is_abstract,
                "file_id": self.entity_ids['file'],
                "module_id": self.current_module,
                "parent_class": parent_class
            }
        )
        self.entities.append(class_entity)
        self.entity_ids[f"class:{node.name}"] = class_id
        if parent_class:
            self._create_relationship(parent_class, class_id, "CLASS_CONTAINS_CLASS")
        elif self.current_module:
            self._create_relationship(self.current_module, class_id, "MODULE_CONTAINS_CLASS")
            self._create_relationship(self.entity_ids['file'], class_id, "FILE_CONTAINS_CLASS")
        else:
            self._create_relationship(self.entity_ids['file'], class_id, "FILE_CONTAINS_CLASS")
        for base in node.bases:
            base_name = self._get_name_from_node(base)
            if base_name:
                self._create_relationship(
                    class_id,
                    f"unresolved:{base_name}",
                    "INHERITS"
                )
        self.generic_visit(node)
        self.current_class = parent_class
        self.class_name_stack.pop()
    def visit_FunctionDef(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> None:
        self.function_name_stack.append(node.name)
        function_id = self._generate_id("function", node.name, node.lineno)
        parent_function = self.current_function
        self.current_function = function_id
        docstring = self._get_docstring(node)
        signature = self._get_function_signature(node)
        return_type = self._get_return_type(node)
        is_async = isinstance(node, ast.AsyncFunctionDef)
        is_generator = self._is_generator(node)
        is_property = self._has_decorator(node, "property")
        is_staticmethod = self._has_decorator(node, "staticmethod")
        is_classmethod = self._has_decorator(node, "classmethod")
        complexity = self._calculate_complexity(node)
        function_entity = ParsedEntity(
            id=function_id,
            name=node.name,
            type="Function",
            file_path=self.file_path,
            line_start=node.lineno,
            line_end=self._get_end_line(node),
            metadata={
                "docstring": docstring,
                "signature": signature,
                "return_type": return_type,
                "is_async": is_async,
                "is_generator": is_generator,
                "is_property": is_property,
                "is_staticmethod": is_staticmethod,
                "is_classmethod": is_classmethod,
                "complexity": complexity,
                "file_id": self.entity_ids['file'],
                "module_id": self.current_module,
                "class_id": self.current_class,
                "parent_function": parent_function
            }
        )
        self.entities.append(function_entity)
        self.entity_ids[f"function:{node.name}"] = function_id
        if self.current_class:
            self._create_relationship(self.current_class, function_id, "CLASS_CONTAINS_FUNCTION")
        elif parent_function:
            self._create_relationship(parent_function, function_id, "FUNCTION_CONTAINS_FUNCTION")
        elif self.current_module:
            self._create_relationship(self.current_module, function_id, "MODULE_CONTAINS_FUNCTION")
            self._create_relationship(self.entity_ids['file'], function_id, "FILE_CONTAINS_FUNCTION")
        else:
            self._create_relationship(self.entity_ids['file'], function_id, "FILE_CONTAINS_FUNCTION")
        for decorator in node.decorator_list:
            decorator_name = self._get_name_from_node(decorator)
            if decorator_name:
                self._create_relationship(
                    f"unresolved:{decorator_name}",
                    function_id,
                    "DECORATES",
                    {"decorator_name": decorator_name, "line_number": node.lineno}
                )
        self.generic_visit(node)
        self.current_function = parent_function
        self.function_name_stack.pop()
    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> None:
        self.visit_FunctionDef(node)
    def visit_Import(self, node: ast.Import) -> None:
        for alias in node.names:
            import_id = self._generate_id("import", alias.name, node.lineno)
            import_entity = ParsedEntity(
                id=import_id,
                name=alias.asname if alias.asname else alias.name,
                type="Import",
                file_path=self.file_path,
                line_start=node.lineno,
                line_end=node.lineno,
                metadata={
                    "module_name": alias.name,
                    "alias": alias.asname,
                    "is_from_import": False,
                    "file_id": self.entity_ids['file']
                }
            )
            self.entities.append(import_entity)
            import_name = alias.asname if alias.asname else alias.name
            self.imports[import_name] = alias.name
            self._create_relationship(
                self.entity_ids['file'],
                import_id,
                "FILE_CONTAINS_IMPORT"
            )
        self.generic_visit(node)
    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        module_name = node.module or ""
        for alias in node.names:
            import_id = self._generate_id("import", f"{module_name}.{alias.name}", node.lineno)
            import_entity = ParsedEntity(
                id=import_id,
                name=alias.name,
                type="Import",
                file_path=self.file_path,
                line_start=node.lineno,
                line_end=node.lineno,
                metadata={
                    "module_name": module_name,
                    "alias": alias.asname,
                    "is_from_import": True,
                    "file_id": self.entity_ids['file']
                }
            )
            self.entities.append(import_entity)
            import_name = alias.asname if alias.asname else alias.name
            self.imports[import_name] = f"{module_name}.{alias.name}"
            self._create_relationship(
                self.entity_ids['file'],
                import_id,
                "FILE_CONTAINS_IMPORT"
            )
        self.generic_visit(node)
    def visit_Call(self, node: ast.Call) -> None:
        if self.current_function:
            func_name = self._get_name_from_node(node.func)
            if func_name:
                self._create_relationship(
                    self.current_function,
                    f"unresolved:{func_name}",
                    "CALLS",
                    {
                        "call_type": "function_call",
                        "line_number": node.lineno
                    }
                )
        self.generic_visit(node)
    def visit_Assign(self, node: ast.Assign) -> None:
        for target in node.targets:
            var_names = self._extract_variable_names(target)
            for var_name in var_names:
                if var_name and not var_name.startswith('_'):
                    var_id = self._generate_id("variable", var_name, node.lineno)
                    type_annotation = self._infer_type_from_value(node.value)
                    variable_entity = ParsedEntity(
                        id=var_id,
                        name=var_name,
                        type="Variable",
                        file_path=self.file_path,
                        line_start=node.lineno,
                        line_end=node.lineno,
                        metadata={
                            "type_annotation": type_annotation,
                            "is_global": self.current_function is None and self.current_class is None,
                            "is_constant": var_name.isupper(),
                            "file_id": self.entity_ids['file'],
                            "scope_id": self.current_function or self.current_class or self.current_module
                        }
                    )
                    self.entities.append(variable_entity)
                    if self.current_function:
                        self._create_relationship(self.current_function, var_id, "FUNCTION_CONTAINS_VARIABLE")
                    elif self.current_class:
                        self._create_relationship(self.current_class, var_id, "CLASS_CONTAINS_VARIABLE")
                        self._create_relationship(self.entity_ids['file'], var_id, "FILE_CONTAINS_VARIABLE")
                    elif self.current_module:
                        self._create_relationship(self.current_module, var_id, "MODULE_CONTAINS_VARIABLE")
                        self._create_relationship(self.entity_ids['file'], var_id, "FILE_CONTAINS_VARIABLE")
                    else:
                        self._create_relationship(self.entity_ids['file'], var_id, "FILE_CONTAINS_VARIABLE")
        self.generic_visit(node)
    def _generate_id(self, entity_type: str, name: str, line: int) -> str:
        import hashlib
        context_parts = [
            self.file_path,
            self.module_name,
            ".".join(self.class_name_stack) if self.class_name_stack else "",
            ".".join(self.function_name_stack) if self.function_name_stack else "",
            entity_type,
            name,
            str(line)
        ]
        identifier = ":".join(part for part in context_parts if part)
        return hashlib.md5(identifier.encode()).hexdigest()
    def _get_docstring(self, node: ast.AST) -> str:
        if (isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module)) and
            node.body and
            isinstance(node.body[0], ast.Expr) and
            isinstance(node.body[0].value, ast.Constant) and
            isinstance(node.body[0].value.value, str)):
            return node.body[0].value.value
        return ""
    def _get_end_line(self, node: ast.AST) -> int:
        if hasattr(node, 'end_lineno') and node.end_lineno:
            return node.end_lineno
        return getattr(node, 'lineno', 1)
    def _get_function_signature(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> str:
        args = []
        for arg in node.args.args:
            arg_str = arg.arg
            if arg.annotation:
                arg_str += f": {self._get_name_from_node(arg.annotation)}"
            args.append(arg_str)
        if node.args.vararg:
            vararg = f"*{node.args.vararg.arg}"
            if node.args.vararg.annotation:
                vararg += f": {self._get_name_from_node(node.args.vararg.annotation)}"
            args.append(vararg)
        if node.args.kwarg:
            kwarg = f"**{node.args.kwarg.arg}"
            if node.args.kwarg.annotation:
                kwarg += f": {self._get_name_from_node(node.args.kwarg.annotation)}"
            args.append(kwarg)
        return f"({', '.join(args)})"
    def _get_return_type(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> str:
        if node.returns:
            return self._get_name_from_node(node.returns)
        return ""
    def _get_name_from_node(self, node: ast.AST) -> str:
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            value = self._get_name_from_node(node.value)
            return f"{value}.{node.attr}" if value else node.attr
        elif isinstance(node, ast.Constant):
            return str(node.value)
        return ""
    def _is_abstract_class(self, node: ast.ClassDef) -> bool:
        for base in node.bases:
            if self._get_name_from_node(base) in ('ABC', 'abc.ABC'):
                return True
        for item in node.body:
            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                for decorator in item.decorator_list:
                    if self._get_name_from_node(decorator) in ('abstractmethod', 'abc.abstractmethod'):
                        return True
        return False
    def _is_generator(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> bool:
        for child in ast.walk(node):
            if isinstance(child, (ast.Yield, ast.YieldFrom)):
                return True
        return False
    def _has_decorator(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef], decorator_name: str) -> bool:
        for decorator in node.decorator_list:
            if self._get_name_from_node(decorator) == decorator_name:
                return True
        return False
    def _calculate_complexity(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> int:
        complexity = 1
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor,
                                ast.ExceptHandler, ast.With, ast.AsyncWith)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        return complexity
    def _get_call_context(self, node: ast.Call) -> str:
        try:
            if hasattr(node, 'lineno') and node.lineno <= len(self.lines):
                return self.lines[node.lineno - 1].strip()
        except:
            pass
        return ""
    def _extract_variable_names(self, node: ast.AST) -> List[str]:
        names = []
        if isinstance(node, ast.Name):
            names.append(node.id)
        elif isinstance(node, ast.Tuple):
            for elt in node.elts:
                names.extend(self._extract_variable_names(elt))
        elif isinstance(node, ast.List):
            for elt in node.elts:
                names.extend(self._extract_variable_names(elt))
        return names
    def _infer_type_from_value(self, node: ast.AST) -> str:
        if isinstance(node, ast.Constant):
            return type(node.value).__name__
        elif isinstance(node, ast.List):
            return "list"
        elif isinstance(node, ast.Dict):
            return "dict"
        elif isinstance(node, ast.Set):
            return "set"
        elif isinstance(node, ast.Tuple):
            return "tuple"
        elif isinstance(node, ast.Call):
            func_name = self._get_name_from_node(node.func)
            return func_name if func_name else "unknown"
        return "unknown"
    def _create_relationship(self, from_id: str, to_id: str, rel_type: str, metadata: Dict[str, Any] = None):
        relationship = ParsedRelationship(
            from_id=from_id,
            to_id=to_id,
            relationship_type=rel_type,
            metadata=metadata or {}
        )
        self.relationships.append(relationship)
</file>

<file path="src/codebased/parsers/registry.py">
from typing import Dict, Type
from .base import BaseParser
from .python import PythonASTParser
from .javascript import JavaScriptParser
from .html import HTMLParser
from .css import CSSParser
from .typescript import TypeScriptParser
from .angular import AngularParser
from .nodejs import NodeJSParser
PARSER_REGISTRY: Dict[str, Type[BaseParser]] = {
    "python": PythonASTParser,
    "javascript": JavaScriptParser,
    "html": HTMLParser,
    "css": CSSParser,
    "typescript": TypeScriptParser,
    "angular": AngularParser,
    "nodejs": NodeJSParser,
}
def get_parser(file_type: str) -> Type[BaseParser] | None:
    return PARSER_REGISTRY.get(file_type)
</file>

<file path="tests/test_database.py">
import unittest
import tempfile
import shutil
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))
try:
    from codebased.database.service import DatabaseService
    from codebased.database.schema import GraphSchema
    KUZU_AVAILABLE = True
except ImportError:
    KUZU_AVAILABLE = False
@unittest.skipUnless(KUZU_AVAILABLE, "Kuzu not available")
class TestDatabaseService(unittest.TestCase):
    def setUp(self):
        self.temp_dir = Path(tempfile.mkdtemp())
        self.db_path = self.temp_dir / "test_graph.kuzu"
        self.db_service = DatabaseService(str(self.db_path))
    def tearDown(self):
        if hasattr(self, 'db_service'):
            self.db_service.disconnect()
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
    def test_database_initialization(self):
        result = self.db_service.initialize()
        self.assertTrue(result)
        self.assertTrue(self.db_path.exists())
    def test_database_connection(self):
        self.db_service.initialize()
        result = self.db_service.connect()
        self.assertTrue(result)
    def test_execute_simple_query(self):
        self.db_service.initialize()
        result = self.db_service.execute_query("RETURN 1 AS test_value")
        self.assertIsNotNone(result)
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0]['test_value'], 1)
    def test_execute_query_with_parameters(self):
        self.db_service.initialize()
        result = self.db_service.execute_query(
            "RETURN $value AS test_value",
            {"value": "hello"}
        )
        self.assertIsNotNone(result)
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0]['test_value'], "hello")
    def test_execute_invalid_query(self):
        self.db_service.initialize()
        result = self.db_service.execute_query("INVALID CYPHER QUERY")
        self.assertIsNone(result)
    def test_health_check(self):
        health = self.db_service.health_check()
        self.assertIn('status', health)
        self.db_service.initialize()
        health = self.db_service.health_check()
        self.assertIn('status', health)
        self.assertIn('db_exists', health)
    def test_get_stats(self):
        self.db_service.initialize()
        stats = self.db_service.get_stats()
        self.assertIn('nodes', stats)
        self.assertIn('relationships', stats)
        self.assertIn('tables', stats)
        self.assertEqual(stats['nodes'], 0)
        self.assertEqual(stats['relationships'], 0)
@unittest.skipUnless(KUZU_AVAILABLE, "Kuzu not available")
class TestGraphSchema(unittest.TestCase):
    def setUp(self):
        self.temp_dir = Path(tempfile.mkdtemp())
        self.db_path = self.temp_dir / "test_schema.kuzu"
        self.db_service = DatabaseService(str(self.db_path))
        self.schema = GraphSchema(self.db_service)
    def tearDown(self):
        if hasattr(self, 'db_service') and self.db_service.conn:
            self.db_service.disconnect()
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
    def test_create_schema(self):
        self.db_service.initialize()
        result = self.schema.create_schema()
        self.assertTrue(result)
        tables_result = self.db_service.execute_query("CALL show_tables() RETURN *")
        self.assertIsNotNone(tables_result)
        self.assertGreater(len(tables_result), 0)
        table_names = {table['name'] for table in tables_result}
        expected_tables = {'File', 'Module', 'Class', 'Function', 'Variable', 'Import'}
        self.assertTrue(expected_tables.issubset(table_names))
    def test_validate_schema(self):
        self.db_service.initialize()
        print("Before validation")
        validation = self.schema.validate_schema()
        print(f"Validation result: {validation}")
        self.assertFalse(validation['valid'])
        self.schema.create_schema()
        validation = self.schema.validate_schema()
        self.assertTrue(validation['valid'])
        self.assertEqual(len(validation['missing_tables']), 0)
    def test_schema_info(self):
        self.db_service.initialize()
        self.schema.create_schema()
        info = self.schema.get_schema_info()
        self.assertIn('node_tables', info)
        self.assertIn('relationship_tables', info)
        self.assertIn('total_tables', info)
        self.assertIn('validation', info)
        self.assertGreater(len(info['node_tables']), 0)
        self.assertGreater(len(info['relationship_tables']), 0)
    def test_reset_schema(self):
        self.db_service.initialize()
        self.schema.create_schema()
        validation = self.schema.validate_schema()
        self.assertTrue(validation['valid'])
        result = self.schema.reset_schema()
        self.assertTrue(result)
        validation = self.schema.validate_schema()
        self.assertTrue(validation['valid'])
if __name__ == '__main__':
    if not KUZU_AVAILABLE:
        print("Kuzu not available - skipping database tests")
        print("Install dependencies with: pip install -r requirements.txt")
    unittest.main()
</file>

<file path="tests/test_parser.py">
import unittest
import tempfile
import os
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))
from codebased.parsers.python import PythonASTParser
from codebased.parsers.base import ParsedEntity, ParsedRelationship
class TestPythonASTParser(unittest.TestCase):
    def setUp(self):
        self.parser = PythonASTParser({
            'exclude_patterns': [],
            'max_file_size': 1024 * 1024,
            'include_docstrings': True
        })
    def tearDown(self):
        pass
    def test_supported_file_types(self):
        self.assertEqual(self.parser.SUPPORTED_FILE_TYPES, {"python"})
    def test_parse_simple_function(self):
        code = '''
def hello_world():
    """A simple function."""
    print("Hello, World!")
    return "Hello"
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            f.flush()
            try:
                result = self.parser.parse_file(f.name)
                self.assertEqual(len(result.errors), 0)
                entity_types = [e.type for e in result.entities]
                self.assertIn('File', entity_types)
                self.assertIn('Module', entity_types)
                self.assertIn('Function', entity_types)
                functions = [e for e in result.entities if e.type == 'Function']
                self.assertEqual(len(functions), 1)
                func = functions[0]
                self.assertEqual(func.name, 'hello_world')
                self.assertEqual(func.metadata.get('docstring'), 'A simple function.')
                self.assertGreater(len(result.relationships), 0)
            finally:
                os.unlink(f.name)
    def test_parse_class_with_methods(self):
        code = '''
class Calculator:
    """A simple calculator class."""
    def __init__(self, name):
        self.name = name
    def add(self, a, b):
        """Add two numbers."""
        return a + b
    def multiply(self, a, b):
        """Multiply two numbers."""
        result = self.add(a, 0)  # This creates a call relationship
        for i in range(b - 1):
            result = self.add(result, a)
        return result
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            f.flush()
            try:
                result = self.parser.parse_file(f.name)
                self.assertEqual(len(result.errors), 0)
                classes = [e for e in result.entities if e.type == 'Class']
                self.assertEqual(len(classes), 1)
                calc_class = classes[0]
                self.assertEqual(calc_class.name, 'Calculator')
                self.assertEqual(calc_class.metadata.get('docstring'), 'A simple calculator class.')
                functions = [e for e in result.entities if e.type == 'Function']
                func_names = {f.name for f in functions}
                expected_methods = {'__init__', 'add', 'multiply'}
                self.assertTrue(expected_methods.issubset(func_names))
                call_relationships = [r for r in result.relationships if r.relationship_type == 'CALLS']
                self.assertGreater(len(call_relationships), 0)
            finally:
                os.unlink(f.name)
    def test_parse_imports(self):
        code = '''
import os
import sys
from pathlib import Path
from collections import defaultdict, deque
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            f.flush()
            try:
                result = self.parser.parse_file(f.name)
                self.assertEqual(len(result.errors), 0)
                imports = [e for e in result.entities if e.type == 'Import']
                self.assertGreaterEqual(len(imports), 4)
                import_names = {i.name for i in imports}
                expected_imports = {'os', 'sys', 'Path', 'defaultdict', 'deque'}
                self.assertTrue(expected_imports.issubset(import_names))
            finally:
                os.unlink(f.name)
    def test_parse_inheritance(self):
        code = '''
class Animal:
    def speak(self):
        pass
class Dog(Animal):
    def speak(self):
        return "Woof!"
class Cat(Animal):
    def speak(self):
        return "Meow!"
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            f.flush()
            try:
                result = self.parser.parse_file(f.name)
                self.assertEqual(len(result.errors), 0)
                inherits_relationships = [r for r in result.relationships if r.relationship_type == 'INHERITS']
                self.assertEqual(len(inherits_relationships), 2)
            finally:
                os.unlink(f.name)
    def test_parse_variables(self):
        code = '''
# Global constants
MAX_SIZE = 100
DEFAULT_NAME = "test"
def process_data():
    # Local variables
    items = []
    count = 0
    result = {"status": "ok"}
    return result
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            f.flush()
            try:
                result = self.parser.parse_file(f.name)
                self.assertEqual(len(result.errors), 0)
                variables = [e for e in result.entities if e.type == 'Variable']
                self.assertGreater(len(variables), 0)
                var_names = {v.name for v in variables}
                expected_vars = {'MAX_SIZE', 'DEFAULT_NAME', 'items', 'count', 'result'}
                self.assertTrue(len(expected_vars.intersection(var_names)) > 0)
            finally:
                os.unlink(f.name)
    def test_parse_syntax_error(self):
        code = '''
def broken_function(
    # Missing closing parenthesis
    print("This will fail")
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            f.flush()
            try:
                result = self.parser.parse_file(f.name)
                self.assertGreater(len(result.errors), 0)
                self.assertIsNotNone(result)
            finally:
                os.unlink(f.name)
    def test_complex_code_structure(self):
        code = '''
"""Module docstring."""
import logging
from typing import Dict, List, Optional
logger = logging.getLogger(__name__)
class DataProcessor:
    """Processes data with various methods."""
    def __init__(self, config: Dict[str, str]):
        self.config = config
        self.cache = {}
    def process(self, data: List[str]) -> Optional[Dict]:
        """Main processing method."""
        if not data:
            logger.warning("No data provided")
            return None
        result = {}
        for item in data:
            processed = self._process_item(item)
            if processed:
                result[item] = processed
        return result
    def _process_item(self, item: str) -> Optional[str]:
        """Process a single item."""
        if item in self.cache:
            return self.cache[item]
        # Simulate processing
        processed = item.upper()
        self.cache[item] = processed
        return processed
def main():
    """Main function."""
    processor = DataProcessor({"mode": "test"})
    result = processor.process(["hello", "world"])
    print(result)
if __name__ == "__main__":
    main()
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            f.flush()
            try:
                result = self.parser.parse_file(f.name)
                self.assertEqual(len(result.errors), 0)
                entity_types = [e.type for e in result.entities]
                self.assertIn('File', entity_types)
                self.assertIn('Module', entity_types)
                self.assertIn('Class', entity_types)
                self.assertIn('Function', entity_types)
                self.assertIn('Import', entity_types)
                self.assertIn('Variable', entity_types)
                classes = [e for e in result.entities if e.type == 'Class']
                self.assertEqual(len(classes), 1)
                self.assertEqual(classes[0].name, 'DataProcessor')
                functions = [e for e in result.entities if e.type == 'Function']
                func_names = {f.name for f in functions}
                expected_funcs = {'__init__', 'process', '_process_item', 'main'}
                self.assertTrue(expected_funcs.issubset(func_names))
                relationships = result.relationships
                self.assertGreater(len(relationships), 0)
                rel_types = {r.relationship_type for r in relationships}
                expected_rel_types = {'CONTAINS', 'CALLS'}
                self.assertTrue(len(expected_rel_types.intersection(rel_types)) > 0)
            finally:
                os.unlink(f.name)
if __name__ == '__main__':
    unittest.main()
</file>

<file path="README.md">
# CodeBased - Code Graph Visualization Tool

CodeBased is a lightweight, self-contained code graph generator and visualization tool that helps developers and AI agents understand code relationships through knowledge graphs.

## Features

- **Zero External Dependencies**: Embedded Kuzu graph database - no servers required
- **Incremental Updates**: Fast updates using file hashing and differential parsing
- **Interactive Visualization**: D3.js-powered force-directed graph with filtering
- **AI Agent Ready**: Pre-built Cypher queries for common code analysis tasks
- **Multi-Format Export**: JSON, CSV, and visual exports
- **Command Line Interface**: Simple CLI for automation and scripting
- **Performance Optimized**: Handles large codebases with WebGL rendering
- **Dynamic Parsers**: Automatically loads available language parsers

## Quick Start

### Installation

1. **Automated Setup (Recommended)**:
   ```bash
   # Unix/Linux/macOS
   chmod +x setup.sh
   ./setup.sh
   
   # Windows (PowerShell)
   Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
   .\setup.ps1
   ```

2. **Manual Setup**:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # or venv\Scripts\activate on Windows
   pip install -r requirements.txt
   pip install -e .
   # optional: install tree-sitter grammars for JS/TS/HTML/CSS
   pip install tree_sitter tree_sitter_languages
   ```

### Basic Usage

```bash
# Initialize CodeBased in your project
codebased init

# Update the code graph
codebased update

# Start the web interface
codebased serve

# Open http://localhost:8000 in your browser
```

## Architecture

```
CodeBased/
├── src/codebased/          # Core Python package
│   ├── cli/               # Command-line interface
│   ├── api/               # FastAPI REST endpoints
│   ├── database/          # Kuzu database service
│   ├── parsers/           # Python AST parsers
│   └── config.py          # Configuration management
├── web/                   # Frontend application
│   ├── index.html         # Main HTML interface
│   ├── app.js            # Application logic
│   ├── graph.js          # D3.js visualization
│   ├── performance.js    # Performance optimizations
│   └── style.css         # Styling
├── data/                  # Database and cache files
├── examples/              # Query examples and templates
├── tests/                # Test suite
└── docs/                 # Documentation
```

## Configuration

CodeBased uses `.codebased.yml` for configuration:

```yaml
project_root: "."
database:
  path: ".codebased/data/graph.kuzu"
parsing:
  include_patterns: ["**/*.py"]
  exclude_patterns: ["venv/**", "__pycache__/**"]
api:
  host: "localhost"
  port: 8000
web:
  max_nodes: 5000
  max_edges: 10000
```

## API Endpoints

- `GET /api/graph` - Get graph visualization data
- `POST /api/query` - Execute Cypher queries
- `POST /api/update` - Trigger graph update
- `GET /api/templates` - Get query templates
- `GET /api/status` - System status and statistics
- `GET /api/tree` - Project directory structure

## Query Examples

### Find Function Callers
```cypher
MATCH (caller:Function)-[:CALLS]->(target:Function {name: "my_function"})
RETURN caller.name, caller.file_path, caller.line_start
```

### Class Hierarchy
```cypher
MATCH path = (child:Class)-[:INHERITS*]->(parent:Class {name: "BaseClass"})
RETURN path
```

### Impact Analysis
```cypher
MATCH path = (f:Function {name: "critical_function"})<-[:CALLS*1..3]-(caller)
RETURN DISTINCT caller.name, caller.file_path, length(path) AS depth
ORDER BY depth
```

### Circular Dependencies
```cypher
MATCH path = (f1:File)-[:IMPORTS*2..]->(f1)
WHERE length(path) > 2
RETURN path LIMIT 10
```

See [examples/queries.md](examples/queries.md) for more examples.

## CLI Commands

```bash
# Initialize CodeBased in current directory
codebased init [--force]

# Update the graph (incremental by default)
codebased update [--full] [--path PATH]

# Start web server
codebased serve [--host HOST] [--port PORT]

# Execute Cypher query
codebased query "MATCH (n) RETURN count(n)"

# Show system status
codebased status

# Export graph data
codebased export [--format json|csv|graphml] [--output FILE]
```

## Performance

CodeBased is optimized for performance:

- **Incremental Updates**: Only re-parses changed files
- **Efficient Storage**: Kuzu's columnar storage
- **Streaming Parsers**: Memory-efficient for large codebases
- **WebGL Rendering**: Hardware acceleration for visualization
- **Level-of-Detail**: Dynamic node/edge simplification
- **Viewport Culling**: Only render visible elements

### Benchmarks

| Codebase Size | Initial Parse | Incremental Update | Memory Usage |
|---------------|---------------|-------------------|--------------|
| 1,000 LOC     | 2s           | 0.1s             | 50MB        |
| 10,000 LOC    | 15s          | 1s               | 200MB       |
| 100,000 LOC   | 2m           | 5s               | 800MB       |

## Troubleshooting

### Common Issues

**"Python not found"**
- Ensure Python 3.8+ is installed and in PATH
- Try `python3` instead of `python`

**"Permission denied" on setup scripts**
- Unix: `chmod +x setup.sh`
- Windows: Run PowerShell as Administrator

**"Database locked"**
- Stop any running `codebased serve` processes
- Remove `.codebased/data/graph.kuzu.lock` if it exists

**Large codebase performance**
- Increase `max_nodes` and `max_edges` in configuration
- Use filtering to focus on relevant code sections
- Enable WebGL rendering in browser

### Debug Mode

```bash
export CODEBASED_DEBUG=1
codebased update
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `python -m pytest tests/`
5. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Support

- Documentation: [docs/](docs/)
- Issues: Report on GitHub
- Questions: Check FAQ in [docs/FAQ.md](docs/FAQ.md)
</file>

<file path="src/codebased/parsers/base.py">
import ast
import os
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Generator, Tuple
from dataclasses import dataclass
from abc import ABC, abstractmethod
from .file_types import get_file_type
logger = logging.getLogger(__name__)
@dataclass
class ParsedEntity:
    id: str
    name: str
    type: str
    file_path: str
    line_start: int
    line_end: int
    metadata: Dict[str, Any]
@dataclass
class ParsedRelationship:
    from_id: str
    to_id: str
    relationship_type: str
    metadata: Dict[str, Any]
@dataclass
class ParseResult:
    entities: List[ParsedEntity]
    relationships: List[ParsedRelationship]
    file_hash: str
    file_path: str
    errors: List[str]
    parse_time: float
class BaseParser(ABC):
    SUPPORTED_FILE_TYPES: Set[str] = set()
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.errors: List[str] = []
    def can_parse(self, file_path: str) -> bool:
        file_type = get_file_type(file_path)
        return file_type in self.SUPPORTED_FILE_TYPES
    @abstractmethod
    def parse_file(self, file_path: str) -> ParseResult:
        pass
    def parse_directory(self, directory_path: str) -> Generator[ParseResult, None, None]:
        directory_path = Path(directory_path)
        if not directory_path.exists() or not directory_path.is_dir():
            logger.error(f"Directory not found: {directory_path}")
            return
        for file_path in self._find_parseable_files(directory_path):
            try:
                result = self.parse_file(str(file_path))
                if result:
                    yield result
            except Exception as e:
                logger.error(f"Failed to parse {file_path}: {e}")
    def _find_parseable_files(self, directory_path: Path) -> Generator[Path, None, None]:
        exclude_patterns = self.config.get('exclude_patterns', [])
        max_file_size = self.config.get('max_file_size', 1024 * 1024)
        follow_symlinks = self.config.get('follow_symlinks', False)
        for root, dirs, files in os.walk(directory_path, followlinks=follow_symlinks):
            root_path = Path(root)
            dirs[:] = [d for d in dirs if not self._should_exclude(d, exclude_patterns)]
            for file_name in files:
                file_path = root_path / file_name
                if self._should_exclude(str(file_path), exclude_patterns):
                    continue
                try:
                    if file_path.stat().st_size > max_file_size:
                        logger.debug(f"Skipping large file: {file_path}")
                        continue
                except OSError:
                    continue
                file_type = get_file_type(str(file_path))
                if file_type and file_type in self.SUPPORTED_FILE_TYPES:
                    yield file_path
    def _should_exclude(self, path: str, exclude_patterns: List[str]) -> bool:
        import fnmatch
        path_name = Path(path).name
        for pattern in exclude_patterns:
            if fnmatch.fnmatch(path_name, pattern) or fnmatch.fnmatch(path, pattern):
                return True
        return False
    def _calculate_file_hash(self, file_path: str) -> str:
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            return hashlib.sha256(content).hexdigest()
        except Exception as e:
            logger.error(f"Failed to calculate hash for {file_path}: {e}")
            return ""
    def _generate_entity_id(self, name: str, file_path: str, line_start: int) -> str:
        identifier = f"{file_path}:{name}:{line_start}"
        return hashlib.md5(identifier.encode()).hexdigest()
class FileTraversal:
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
    def get_python_files(self, root_path: str) -> List[str]:
        python_files = []
        root_path = Path(root_path)
        file_extensions = self.config.get('file_extensions', ['.py'])
        exclude_patterns = self.config.get('exclude_patterns', [])
        max_file_size = self.config.get('max_file_size', 1024 * 1024)
        follow_symlinks = self.config.get('follow_symlinks', False)
        for file_path in root_path.rglob('*'):
            if file_path.is_dir():
                continue
            if file_path.suffix not in file_extensions:
                continue
            if self._should_exclude(str(file_path), exclude_patterns):
                continue
            try:
                if file_path.stat().st_size > max_file_size:
                    logger.debug(f"Skipping large file: {file_path}")
                    continue
            except OSError:
                continue
            if not follow_symlinks and file_path.is_symlink():
                continue
            python_files.append(str(file_path))
        return sorted(python_files)
    def _should_exclude(self, path: str, exclude_patterns: List[str]) -> bool:
        import fnmatch
        path_obj = Path(path)
        for pattern in exclude_patterns:
            if fnmatch.fnmatch(path_obj.name, pattern):
                return True
            if fnmatch.fnmatch(str(path_obj), pattern):
                return True
            for parent in path_obj.parents:
                if fnmatch.fnmatch(parent.name, pattern):
                    return True
        return False
    def get_file_info(self, file_path: str) -> Dict[str, Any]:
        file_path = Path(file_path)
        try:
            stat = file_path.stat()
            info = {
                'path': str(file_path),
                'name': file_path.name,
                'stem': file_path.stem,
                'suffix': file_path.suffix,
                'size': stat.st_size,
                'modified_time': stat.st_mtime,
                'is_symlink': file_path.is_symlink(),
                'exists': file_path.exists(),
                'is_readable': os.access(file_path, os.R_OK)
            }
            if info['is_readable'] and info['size'] <= self.config.get('max_file_size', 1024 * 1024):
                try:
                    with open(file_path, 'rb') as f:
                        content = f.read()
                    info['hash'] = hashlib.sha256(content).hexdigest()
                    if file_path.suffix in ['.py', '.js', '.ts', '.jsx', '.tsx']:
                        try:
                            text_content = content.decode('utf-8', errors='ignore')
                            lines = text_content.split('\n')
                            loc = sum(1 for line in lines
                                     if line.strip() and not line.strip().startswith('#'))
                            info['lines_of_code'] = loc
                            info['total_lines'] = len(lines)
                        except:
                            info['lines_of_code'] = 0
                            info['total_lines'] = 0
                except Exception as e:
                    logger.debug(f"Failed to process file content {file_path}: {e}")
                    info['hash'] = ""
            return info
        except Exception as e:
            logger.error(f"Failed to get file info for {file_path}: {e}")
            return {
                'path': str(file_path),
                'name': file_path.name,
                'error': str(e)
            }
</file>

<file path="src/codebased/parsers/file_types.py">
from pathlib import Path
from typing import Dict, Optional
FILE_TYPE_PATTERNS: Dict[str, str] = {
    ".component.ts": "angular",
    ".module.ts": "angular",
    ".service.ts": "angular",
    ".guard.ts": "angular",
    ".pipe.ts": "angular",
    ".component.html": "angular",
    ".component.css": "angular",
}
FILE_TYPE_MAPPINGS: Dict[str, str] = {
    ".py": "python",
    ".pyw": "python",
    ".pyi": "python",
    ".js": "javascript",
    ".jsx": "javascript",
    ".mjs": "javascript",
    ".ts": "typescript",
    ".tsx": "typescript",
    ".html": "html",
    ".htm": "html",
    ".css": "css",
    ".scss": "css",
    ".sass": "css",
    ".json": "json",
    ".yml": "yaml",
    ".yaml": "yaml",
    ".md": "markdown",
    "Dockerfile": "dockerfile",
}
def get_file_type(file_path: str) -> Optional[str]:
    path = Path(file_path)
    name_lower = path.name.lower()
    for pattern, ftype in FILE_TYPE_PATTERNS.items():
        if name_lower.endswith(pattern):
            return ftype
    extension = path.suffix.lower()
    if not extension and path.name in FILE_TYPE_MAPPINGS:
        return FILE_TYPE_MAPPINGS[path.name]
    if extension in FILE_TYPE_MAPPINGS:
        return FILE_TYPE_MAPPINGS[extension]
    return None
</file>

</files>
